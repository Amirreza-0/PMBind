{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras_hub.src.layers.modeling.rotary_embedding import RotaryEmbedding\n",
    "import json\n",
    "import pyarrow.parquet as pq\n",
    "from functools import lru_cache\n",
    "import os\n",
    "import sys\n",
    "from typing import Dict, Tuple, List\n",
    "from tqdm import tqdm"
   ],
   "id": "a14f885e5cec90cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mixed_precision = True  # Enable mixed precision for significant speedup\n",
    "\n",
    "if mixed_precision:\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "    print(f'Mixed precision enabled: {policy}')"
   ],
   "id": "fdcd87b32724d831"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Constants for data processing\n",
    "ESM_DIM = 1536  # Will be updated from metadata\n",
    "\n",
    "# Constants\n",
    "BLOSUM62 = {\n",
    "    'A': [4, -1, -2, -2, 0, -1, -1, 0, -2, -1, -1, -1, -1, -2, -1, 1, 0, -3, -2, 0, -2, -1, 0],\n",
    "    'R': [-1, 5, 0, -2, -3, 1, 0, -2, 0, -3, -2, 2, -1, -3, -2, -1, -1, -3, -2, -3, -1, 0, -1],\n",
    "    'N': [-2, 0, 6, 1, -3, 0, 0, 0, 1, -3, -3, 0, -2, -3, -2, 1, 0, -4, -2, -3, 3, 0, -1],\n",
    "    'D': [-2, -2, 1, 6, -3, 0, 2, -1, -1, -3, -4, -1, -3, -3, -1, 0, -1, -4, -3, -3, 4, 1, -1],\n",
    "    'C': [0, -3, -3, -3, 9, -3, -4, -3, -3, -1, -1, -3, -1, -2, -3, -1, -1, -2, -2, -1, -3, -3, -2],\n",
    "    'Q': [-1, 1, 0, 0, -3, 5, 2, -2, 0, -3, -2, 1, 0, -3, -1, 0, -1, -2, -1, -2, 0, 3, -1],\n",
    "    'E': [-1, 0, 0, 2, -4, 2, 5, -2, 0, -3, -3, 1, -2, -3, -1, 0, -1, -3, -2, -2, 1, 4, -1],\n",
    "    'G': [0, -2, 0, -1, -3, -2, -2, 6, -2, -4, -4, -2, -3, -3, -2, 0, -2, -2, -3, -3, -1, -2, -1],\n",
    "    'H': [-2, 0, 1, -1, -3, 0, 0, -2, 8, -3, -3, -1, -2, -1, -2, -1, -2, -2, 2, -3, 0, 0, -1],\n",
    "    'I': [-1, -3, -3, -3, -1, -3, -3, -4, -3, 4, 2, -3, 1, 0, -3, -2, -1, -3, -1, 3, -3, -3, -1],\n",
    "    'L': [-1, -2, -3, -4, -1, -2, -3, -4, -3, 2, 4, -2, 2, 0, -3, -2, -1, -2, -1, 1, -4, -3, -1],\n",
    "    'K': [-1, 2, 0, -1, -3, 1, 1, -2, -1, -3, -2, 5, -1, -3, -1, 0, -1, -3, -2, -2, 0, 1, -1],\n",
    "    'M': [-1, -1, -2, -3, -1, 0, -2, -3, -2, 1, 2, -1, 5, 0, -2, -1, -1, -1, -1, 1, -3, -1, -1],\n",
    "    'F': [-2, -3, -3, -3, -2, -3, -3, -3, -1, 0, 0, -3, 0, 6, -4, -2, -2, 1, 3, -1, -3, -3, -1],\n",
    "    'P': [-1, -2, -2, -1, -3, -1, -1, -2, -2, -3, -3, -1, -2, -4, 7, -1, -1, -4, -3, -2, -2, -1, -2],\n",
    "    'S': [1, -1, 1, 0, -1, 0, 0, 0, -1, -2, -2, 0, -1, -2, -1, 4, 1, -3, -2, -2, 0, 0, 0],\n",
    "    'T': [0, -1, 0, -1, -1, -1, -1, -2, -2, -1, -1, -1, -1, -2, -1, 1, 5, -2, -2, 0, -1, -1, 0],\n",
    "    'W': [-3, -3, -4, -4, -2, -2, -3, -2, -2, -3, -2, -3, -1, 1, -4, -3, -2, 11, 2, -3, -4, -3, -2],\n",
    "    'Y': [-2, -2, -2, -3, -2, -1, -2, -3, 2, -1, -1, -2, -1, 3, -3, -2, -2, 2, 7, -1, -3, -2, -1],\n",
    "    'V': [0, -3, -3, -3, -1, -2, -2, -3, -3, 3, 1, -2, 1, -1, -2, -2, 0, -3, -1, 4, -3, -2, -1],\n",
    "    'B': [-2, -1, 3, 4, -3, 0, 1, -1, 0, -3, -4, 0, -3, -3, -2, 0, -1, -4, -3, -3, 4, 1, -1],\n",
    "    'Z': [-1, 0, 0, 1, -3, 3, 4, -2, 0, -3, -3, 1, -1, -3, -1, 0, -1, -3, -2, -2, 1, 4, -1],\n",
    "    'X': [0, -1, -1, -1, -2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -2, 0, 0, -2, -1, -1, -1, -1, -1],\n",
    "}\n",
    "\n",
    "# Normalized\n",
    "# BLOSUM62 = {\n",
    "#  \"A\": [0.533, 0.2, 0.133, 0.133, 0.267, 0.2, 0.2, 0.267, 0.133, 0.2, 0.2, 0.2, 0.2, 0.133, 0.2, 0.333, 0.267, 0.067, 0.133, 0.267, 0.133, 0.2, 0.267],\n",
    "#  \"R\": [0.2, 0.6, 0.267, 0.133, 0.067, 0.333, 0.267, 0.133, 0.267, 0.067, 0.133, 0.4, 0.2, 0.067, 0.133, 0.2, 0.2, 0.067, 0.133, 0.067, 0.2, 0.267, 0.2],\n",
    "#  \"N\": [0.133, 0.267, 0.667, 0.333, 0.067, 0.267, 0.267, 0.267, 0.333, 0.067, 0.067, 0.267, 0.133, 0.067, 0.133, 0.333, 0.267, 0.0, 0.133, 0.067, 0.467, 0.267, 0.2],\n",
    "#  \"D\": [0.133, 0.133, 0.333, 0.667, 0.067, 0.267, 0.4, 0.2, 0.2, 0.067, 0.0, 0.2, 0.067, 0.067, 0.2, 0.267, 0.2, 0.0, 0.067, 0.067, 0.533, 0.333, 0.2],\n",
    "#  \"C\": [0.267, 0.067, 0.067, 0.067, 0.867, 0.067, 0.0, 0.067, 0.067, 0.2, 0.2, 0.067, 0.2, 0.133, 0.067, 0.2, 0.2, 0.133, 0.133, 0.2, 0.067, 0.067, 0.133],\n",
    "#  \"Q\": [0.2, 0.333, 0.267, 0.267, 0.067, 0.6, 0.4, 0.133, 0.267, 0.067, 0.133, 0.333, 0.267, 0.067, 0.2, 0.267, 0.2, 0.133, 0.2, 0.133, 0.267, 0.467, 0.2],\n",
    "#  \"E\": [0.2, 0.267, 0.267, 0.4, 0.0, 0.4, 0.6, 0.133, 0.267, 0.067, 0.067, 0.333, 0.133, 0.067, 0.2, 0.267, 0.2, 0.067, 0.133, 0.133, 0.333, 0.533, 0.2],\n",
    "#  \"G\": [0.267, 0.133, 0.267, 0.2, 0.067, 0.133, 0.133, 0.667, 0.133, 0.0, 0.0, 0.133, 0.067, 0.067, 0.133, 0.267, 0.133, 0.133, 0.067, 0.067, 0.2, 0.133, 0.2],\n",
    "#  \"H\": [0.133, 0.267, 0.333, 0.2, 0.067, 0.267, 0.267, 0.133, 0.8, 0.067, 0.067, 0.2, 0.133, 0.2, 0.133, 0.2, 0.133, 0.133, 0.4, 0.067, 0.267, 0.267, 0.2],\n",
    "#  \"I\": [0.2, 0.067, 0.067, 0.067, 0.2, 0.067, 0.067, 0.0, 0.067, 0.533, 0.4, 0.067, 0.333, 0.267, 0.067, 0.133, 0.2, 0.067, 0.2, 0.467, 0.067, 0.067, 0.2],\n",
    "#  \"L\": [0.2, 0.133, 0.067, 0.0, 0.2, 0.133, 0.067, 0.0, 0.067, 0.4, 0.533, 0.133, 0.4, 0.267, 0.067, 0.133, 0.2, 0.133, 0.2, 0.333, 0.0, 0.067, 0.2],\n",
    "#  \"K\": [0.2, 0.4, 0.267, 0.2, 0.067, 0.333, 0.333, 0.133, 0.2, 0.067, 0.133, 0.6, 0.2, 0.067, 0.2, 0.267, 0.2, 0.067, 0.133, 0.133, 0.267, 0.333, 0.2],\n",
    "#  \"M\": [0.2, 0.2, 0.133, 0.067, 0.2, 0.267, 0.133, 0.067, 0.133, 0.333, 0.4, 0.2, 0.6, 0.267, 0.133, 0.2, 0.2, 0.2, 0.2, 0.333, 0.067, 0.2, 0.2],\n",
    "#  \"F\": [0.133, 0.067, 0.067, 0.067, 0.133, 0.067, 0.067, 0.067, 0.2, 0.267, 0.267, 0.067, 0.267, 0.667, 0.0, 0.133, 0.133, 0.333, 0.467, 0.2, 0.067, 0.067, 0.2],\n",
    "#  \"P\": [0.2, 0.133, 0.133, 0.2, 0.067, 0.2, 0.2, 0.133, 0.133, 0.067, 0.067, 0.2, 0.133, 0.0, 0.733, 0.2, 0.2, 0.0, 0.067, 0.133, 0.133, 0.2, 0.133],\n",
    "#  \"S\": [0.333, 0.2, 0.333, 0.267, 0.2, 0.267, 0.267, 0.267, 0.2, 0.133, 0.133, 0.267, 0.2, 0.133, 0.2, 0.533, 0.333, 0.067, 0.133, 0.133, 0.267, 0.267, 0.267],\n",
    "#  \"T\": [0.267, 0.2, 0.267, 0.2, 0.2, 0.2, 0.2, 0.133, 0.133, 0.2, 0.2, 0.2, 0.2, 0.133, 0.2, 0.333, 0.6, 0.133, 0.133, 0.267, 0.2, 0.2, 0.267],\n",
    "#  \"W\": [0.067, 0.067, 0.0, 0.0, 0.133, 0.133, 0.067, 0.133, 0.133, 0.067, 0.133, 0.067, 0.2, 0.333, 0.0, 0.067, 0.133, 1.0, 0.4, 0.067, 0.0, 0.067, 0.133],\n",
    "#  \"Y\": [0.133, 0.133, 0.133, 0.067, 0.133, 0.2, 0.133, 0.067, 0.4, 0.2, 0.2, 0.133, 0.2, 0.467, 0.067, 0.133, 0.133, 0.4, 0.733, 0.2, 0.067, 0.133, 0.2],\n",
    "#  \"V\": [0.267, 0.067, 0.067, 0.067, 0.2, 0.133, 0.133, 0.067, 0.067, 0.467, 0.333, 0.133, 0.333, 0.2, 0.133, 0.133, 0.267, 0.067, 0.2, 0.533, 0.067, 0.133, 0.2],\n",
    "#  \"B\": [0.133, 0.2, 0.467, 0.533, 0.067, 0.267, 0.333, 0.2, 0.267, 0.067, 0.0, 0.267, 0.067, 0.067, 0.133, 0.267, 0.2, 0.0, 0.067, 0.067, 0.533, 0.333, 0.2],\n",
    "#  \"Z\": [0.2, 0.267, 0.267, 0.333, 0.067, 0.467, 0.533, 0.133, 0.267, 0.067, 0.067, 0.333, 0.2, 0.067, 0.2, 0.267, 0.2, 0.067, 0.133, 0.133, 0.333, 0.533, 0.2],\n",
    "#  \"X\": [0.267, 0.2, 0.2, 0.2, 0.133, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.133, 0.267, 0.267, 0.133, 0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "#\n",
    "# }\n",
    "\n",
    "# Create a reverse mapping from a BLOSUM62 vector to an amino acid character.\n",
    "# The score lists are converted to tuples so they can be used as dictionary keys.\n",
    "VECTOR_TO_AA = {tuple(v): k for k, v in BLOSUM62.items()}\n",
    "\n",
    "AA = \"ACDEFGHIKLMNPQRSTVWY-\"\n",
    "AA_TO_INT = {a: i for i, a in enumerate(AA)}\n",
    "UNK_IDX = PAD_INDEX_OHE = 20  # Index for \"unknown\"\n",
    "\n",
    "MASK_TOKEN = -1.0\n",
    "NORM_TOKEN = 1.0\n",
    "PAD_TOKEN = -2.0\n",
    "PAD_VALUE = 0.0\n",
    "MASK_VALUE = 0.0\n",
    "\n",
    "AMINO_ACID_VOCAB = \"\".join(list(BLOSUM62.keys()))\n",
    "AMINO_ACID_MAP = {k: i for i, k in enumerate(AMINO_ACID_VOCAB)}\n",
    "UNK_IDX_23 = PAD_INDEX_62 = len(AMINO_ACID_VOCAB)  # Index for padding in BLOSUM62 (after all amino acids)\n",
    "\n",
    "\n",
    "def seq_to_onehot(sequence: str, max_seq_len: int) -> np.ndarray:\n",
    "    \"\"\"Convert peptide sequence to one-hot encoding\"\"\"\n",
    "    arr = np.full((max_seq_len, 21), PAD_VALUE, dtype=np.float32)  # initialize padding with 0\n",
    "    for j, aa in enumerate(sequence.upper()[:max_seq_len]):\n",
    "        arr[j, AA_TO_INT.get(aa, UNK_IDX)] = 1.0\n",
    "        # print number of UNKs in the sequence\n",
    "    # num_unks = np.sum(arr[:, UNK_IDX])\n",
    "    # zero out gaps\n",
    "    arr[:, AA_TO_INT['-']] = PAD_VALUE  # Set gaps to PAD_VALUE\n",
    "    # if num_unks > 0:\n",
    "    #     print(f\"Warning: {num_unks} unknown amino acids in sequence '{sequence}'\")\n",
    "    return arr\n",
    "\n",
    "\n",
    "def seq_to_blossom62(sequence: str, max_seq_len: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converts a peptide sequence into a matrix using BLOSUM62 substitution scores.\n",
    "\n",
    "    This function maps each amino acid in the input sequence to its corresponding\n",
    "    vector of substitution scores from the BLOSUM62 matrix. The resulting matrix\n",
    "    is padded or truncated to a specified maximum length.\n",
    "\n",
    "    Args:\n",
    "        sequence (str): The input peptide sequence.\n",
    "        max_seq_len (int): The target length for the output matrix. Sequences\n",
    "                           shorter than this will be padded with zeros, and\n",
    "                           longer ones will be truncated.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A NumPy array of shape (max_seq_len, 23) where each row\n",
    "                    corresponds to an amino acid's BLOSUM62 scores.\n",
    "    \"\"\"\n",
    "    # The BLOSUM62 matrix has 23 columns corresponding to the score list length.\n",
    "    num_features = 23\n",
    "\n",
    "    # Initialize the output array with the padding value (0.0).\n",
    "    arr = np.full((max_seq_len, num_features), PAD_VALUE, dtype=np.float32)\n",
    "\n",
    "    # Use the vector for 'X' (unknown) as the default for any character\n",
    "    # not found in the BLOSUM62 dictionary, including gaps ('-').\n",
    "    default_vector = BLOSUM62['X']\n",
    "\n",
    "    # Iterate over the sequence up to the maximum length.\n",
    "    for i, aa in enumerate(sequence.upper()[:max_seq_len]):\n",
    "        # Retrieve the BLOSUM62 vector for the current amino acid.\n",
    "        # If the amino acid is not a key in the dictionary, use the default vector.\n",
    "        blosum_vector = BLOSUM62.get(aa, default_vector)\n",
    "        arr[i, :] = blosum_vector\n",
    "    return arr\n",
    "\n",
    "\n",
    "def get_embed_key(key: str, emb_dict: Dict[str, str]) -> str:  # why ndarray?\n",
    "    \"\"\"\n",
    "    Get the embedding key for a given allele key.\n",
    "    If the key is not found in the embedding dictionary, return None.\n",
    "    # find the matching emb key in the emb_dict.\n",
    "    Sometimes the emb key is longer than the allele key, so we need to check if the key is a substring of the emb key.\n",
    "    \"\"\"\n",
    "    # Use a generator expression for efficient lookup\n",
    "    return next((emb_key for emb_key in emb_dict if emb_key.upper().startswith(key.upper())), None)\n",
    "\n",
    "\n",
    "def get_seq(key: str, seq_dict: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Get the sequence for a given allele key.\n",
    "    If the key is not found in the sequence dictionary, return None.\n",
    "    \"\"\"\n",
    "    return next((seq for seq_key, seq in seq_dict.items() if seq_key.upper().startswith(key.upper())), None)\n",
    "\n",
    "\n",
    "def clean_key(allele_key: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean allele keys by removing special characters and converting to uppercase.\n",
    "    This is useful for matching keys in embedding dictionaries.\n",
    "    \"\"\"\n",
    "    if allele_key is None:\n",
    "        return \"None\"\n",
    "    mapping = str.maketrans({'*': '', ':': '', ' ': '', '/': '_'})\n",
    "    return allele_key.translate(mapping).upper()\n",
    "\n",
    "\n",
    "def get_mhc_seq_class2(key, embed_map, seq_map):\n",
    "    # print(f\"Processing key: {key}\")  # Debugging line\n",
    "    if key is None: return ''\n",
    "    key_parts = key.split('_')\n",
    "    # print(f\"Key parts: {key_parts}\")  # Debugging line\n",
    "    if len(key_parts) >= 2:\n",
    "        key1 = get_embed_key(key_parts[0], embed_map)\n",
    "        key2 = get_embed_key(key_parts[1], embed_map)\n",
    "        if embed_map.get(key1, None) is None or embed_map.get(key2, None) is None:\n",
    "            print(\n",
    "                f\"Warning: Embedding not found for embd_key 1: '{key1}' 2: '{key2}' in input:'{key_parts[0]}', '{key_parts[1]}'\")\n",
    "        # print(f\"Key1: {key1}, Key2: {key2}\")  # Debugging line\n",
    "        seq1 = get_seq(key_parts[0], seq_map) if key1 else ''\n",
    "        seq2 = get_seq(key_parts[1], seq_map) if key2 else ''\n",
    "        # print(f\"Seq1: {seq1}, Seq2: {seq2}\")  # Debugging line\n",
    "        return seq1 + seq2\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected MHC class II key format: '{key}'\")\n",
    "\n",
    "\n",
    "def get_embed_key_class2(key, embed_map):\n",
    "    if key is None: return None\n",
    "    key_parts = key.split('_')\n",
    "    if len(key_parts) >= 2:\n",
    "        key1 = get_embed_key(key_parts[0], embed_map)\n",
    "        key2 = get_embed_key(key_parts[1], embed_map)\n",
    "        if embed_map.get(key1, None) is None or embed_map.get(key2, None) is None:\n",
    "            print(\n",
    "                f\"Warning: Embedding not found for embd_key 1: '{key1}' 2: '{key2}' in input:'{key_parts[0]}', '{key_parts[1]}'\")\n",
    "        return \"_\".join(filter(None, [key1, key2]))\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected MHC class II key format: '{key}'\")\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def _neg_inf(dtype: tf.dtypes.DType) -> tf.Tensor:\n",
    "    \"\"\"Return a large negative constant suited for masking in given dtype.\"\"\"\n",
    "    if dtype == tf.float16 or dtype == tf.bfloat16:\n",
    "        return tf.constant(-1e4, dtype=dtype)\n",
    "    return tf.constant(-1e9, dtype=dtype)\n",
    "\n",
    "\n",
    "# Custom layers from your original code\n",
    "@tf.keras.utils.register_keras_serializable(package='Custom', name='AsymmetricPenaltyBinaryCrossentropy')\n",
    "class AsymmetricPenaltyBinaryCrossentropy(tf.keras.losses.Loss):\n",
    "    \"\"\"Asymmetric Penalty Binary Cross-Entropy Loss\"\"\"\n",
    "\n",
    "    def __init__(self, label_smoothing=0.1, asymmetry_strength_pos=5.0, asymmetry_strength_neg=0.5,\n",
    "                 name='asymmetric_penalty_binary_crossentropy', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.asymmetry_strength_pos = asymmetry_strength_pos\n",
    "        self.asymmetry_strength_neg = asymmetry_strength_neg\n",
    "\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Asymmetric Penalty Binary Cross-Entropy Loss\n",
    "\n",
    "        Features:\n",
    "        - Minimum at smoothed label: p = 1-ε (true=1), p = ε (true=0)\n",
    "        - Steeper penalty toward opposing class\n",
    "        - Gentler penalty toward actual class\n",
    "\n",
    "        Args:\n",
    "            y_true: Ground truth binary labels\n",
    "            y_pred: Predicted probabilities\n",
    "            label_smoothing: Smoothing parameter ε (0.05-0.15 recommended)\n",
    "            asymmetry_strength: Controls penalty asymmetry (0.3-0.8 recommended)\n",
    "        \"\"\"\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1 - epsilon)\n",
    "\n",
    "        # Base with label smoothing\n",
    "        y_smooth = y_true * (1 - self.label_smoothing) + (1 - y_true) * self.label_smoothing\n",
    "        base_loss = -(y_smooth * tf.math.log(y_pred) + (1 - y_smooth) * tf.math.log(1 - y_pred))\n",
    "\n",
    "        # Optimal prediction points\n",
    "        optimal = y_true * (1 - self.label_smoothing) + (1 - y_true) * self.label_smoothing\n",
    "        base_loss_optimal = -(y_smooth * tf.math.log(optimal) + (1 - y_smooth) * tf.math.log(1 - optimal))\n",
    "\n",
    "        # Normalize so min = 0\n",
    "        base_loss = base_loss - base_loss_optimal\n",
    "\n",
    "        # Distance from optimal points\n",
    "        optimal_true1 = 1 - self.label_smoothing\n",
    "        optimal_true0 = self.label_smoothing\n",
    "        dist_from_optimal_true1 = tf.abs(y_pred - optimal_true1)\n",
    "        dist_from_optimal_true0 = tf.abs(y_pred - optimal_true0)\n",
    "\n",
    "        # Asymmetric penalties with separate strengths\n",
    "        penalty_true1 = y_true * tf.where(\n",
    "            y_pred < optimal_true1,\n",
    "            self.asymmetry_strength_pos * dist_from_optimal_true1**2,\n",
    "            self.asymmetry_strength_pos * 0.3 * dist_from_optimal_true1**2\n",
    "        )\n",
    "\n",
    "        penalty_true0 = (1 - y_true) * tf.where(\n",
    "            y_pred > optimal_true0,\n",
    "            self.asymmetry_strength_neg * dist_from_optimal_true0**2,\n",
    "            self.asymmetry_strength_neg * 0.3 * dist_from_optimal_true0**2\n",
    "        )\n",
    "\n",
    "        total_loss = tf.reduce_mean(base_loss + penalty_true1 + penalty_true0)\n",
    "        return tf.maximum(0.0, total_loss)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'label_smoothing': self.label_smoothing,\n",
    "            'asymmetry_strength_pos': self.asymmetry_strength_pos,\n",
    "            'asymmetry_strength_neg': self.asymmetry_strength_neg\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package='custom_layers', name='SelfAttentionWith2DMask')\n",
    "class SelfAttentionWith2DMask(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom self-attention layer that supports 2D masks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, query_dim, context_dim, output_dim, heads=4,\n",
    "                 return_att_weights=False, name='SelfAttentionWith2DMask',\n",
    "                 epsilon=1e-6, mask_token=-1., pad_token=-2., self_attn_mhc=True, apply_rope=True):\n",
    "        super().__init__(name=name)\n",
    "        self.query_dim = query_dim\n",
    "        self.context_dim = context_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.heads = heads\n",
    "        self.return_att_weights = return_att_weights\n",
    "        self.epsilon = epsilon\n",
    "        self.mask_token = mask_token\n",
    "        self.pad_token = pad_token\n",
    "        self.att_dim = output_dim // heads  # Attention dimension per head\n",
    "        self.self_attn_mhc = self_attn_mhc\n",
    "        self.apply_rope = apply_rope  # flag for rotary positional embedding\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Validate input dim matches provided dims (since a single x is used for q/k/v)\n",
    "        in_dim = int(input_shape[-1])\n",
    "        if in_dim != self.query_dim or in_dim != self.context_dim:\n",
    "            raise ValueError(\n",
    "                f\"Input dim {in_dim} must match query_dim {self.query_dim} and context_dim {self.context_dim}.\")\n",
    "\n",
    "        # Projection weights\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=self.epsilon, name=f'ln1_{self.name}')\n",
    "        self.q_proj = self.add_weight(shape=(self.heads, self.query_dim, self.att_dim),\n",
    "                                      initializer='random_normal', trainable=True, name=f'q_proj_{self.name}')\n",
    "        self.k_proj = self.add_weight(shape=(self.heads, self.context_dim, self.att_dim),\n",
    "                                      initializer='random_normal', trainable=True, name=f'k_proj_{self.name}')\n",
    "        self.v_proj = self.add_weight(shape=(self.heads, self.context_dim, self.att_dim),\n",
    "                                      initializer='random_normal', trainable=True, name=f'v_proj_{self.name}')\n",
    "        self.g_proj = self.add_weight(shape=(self.heads, self.query_dim, self.att_dim),\n",
    "                                      initializer='random_uniform', trainable=True, name=f'g_proj_{self.name}')\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=self.epsilon, name=f'ln2_{self.name}')\n",
    "\n",
    "        self.out_w = self.add_weight(shape=(self.heads * self.att_dim, self.output_dim),\n",
    "                                     initializer='random_normal', trainable=True, name=f'outw_{self.name}')\n",
    "        self.out_b = self.add_weight(shape=(self.output_dim,), initializer='zeros',\n",
    "                                     trainable=True, name=f'outb_{self.name}')\n",
    "        self.scale = 1.0 / tf.math.sqrt(tf.cast(self.att_dim, tf.keras.mixed_precision.global_policy().compute_dtype))\n",
    "\n",
    "        if self.apply_rope:\n",
    "            if (self.att_dim % 2) != 0:\n",
    "                raise ValueError(f\"RotaryEmbedding requires even att_dim, got {self.att_dim}.\")\n",
    "            # q/k have shape (B, H, S, D): sequence_axis=2, feature_axis=-1\n",
    "            self.rope = RotaryEmbedding(sequence_axis=2, feature_axis=-1, name=f'rope_{self.name}')\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def call(self, x_pmhc, p_mask, m_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (B, N+M, query_dim) for query.\n",
    "            p_mask: Tensor of shape (B, N) for peptide mask.\n",
    "            m_mask: Tensor of shape (B, M) for mhc mask.\n",
    "        Returns:\n",
    "            Tensor of shape (B, N+M, output_dim)\n",
    "        \"\"\"\n",
    "        x_pmhc = self.norm1(x_pmhc)  # Normalize input\n",
    "        p_mask = tf.cast(p_mask, self.compute_dtype)\n",
    "        m_mask = tf.cast(m_mask, self.compute_dtype)\n",
    "        p_mask = tf.where(p_mask == self.pad_token, x=0., y=1.)  # (B, N)\n",
    "        m_mask = tf.where(m_mask == self.pad_token, x=0., y=1.)  # (B, M)\n",
    "\n",
    "        q = tf.einsum('bxd,hde->bhxe', tf.cast(x_pmhc , self.compute_dtype), tf.cast(self.q_proj, self.compute_dtype))  # (B, N+M, E) * (H, E, D) -> (B, H, N+M, D)\n",
    "        k = tf.einsum('bxd,hde->bhxe', tf.cast(x_pmhc, self.compute_dtype), tf.cast(self.k_proj, self.compute_dtype))  # (B, N+M, E) * (H, E, D) -> (B, H, N+M, D)\n",
    "        v = tf.einsum('bxd,hde->bhxe', tf.cast(x_pmhc, self.compute_dtype), tf.cast(self.v_proj, self.compute_dtype))  # (B, N+M, E) * (H, E, D) -> (B, H, N+M, D)\n",
    "        g = tf.einsum('bxd,hde->bhxe', tf.cast(x_pmhc, self.compute_dtype), tf.cast(self.g_proj, self.compute_dtype))  # (B, N+M, E) * (H, E, D) -> (B, H, N+M, D)\n",
    "\n",
    "        if self.apply_rope:\n",
    "            q = self.rope(q)\n",
    "            k = self.rope(k)\n",
    "\n",
    "        att = tf.einsum('bhxe,bhye->bhxy', q, k) * tf.cast(self.scale, self.compute_dtype)  # (B, H, N+M, D) * (B, H, D, N+M) -> (B, H, N+M, N+M)\n",
    "        # Create 2D mask\n",
    "        mask_2d = self.mask_2d(p_mask, m_mask)\n",
    "        mask_2d = tf.cast(mask_2d, self.compute_dtype)  # (B, N+M, N+M)\n",
    "        mask_2d_neg = (1.0 - mask_2d) * _neg_inf(att.dtype)  # Apply mask to attention scores\n",
    "        att = tf.nn.softmax(att + tf.expand_dims(mask_2d_neg, axis=1), axis=-1)  # Apply softmax to attention scores\n",
    "        att *= tf.expand_dims(mask_2d,\n",
    "                              axis=1)  # remove the impact of row wise attention of padded tokens. since all are 1e-9\n",
    "        out = tf.matmul(att, v)  # (B, H, N+M, N+M) * (B, H, N+M, D) -> (B, H, N+M, D)\n",
    "        out *= tf.nn.sigmoid(g)  # Apply gating mechanism\n",
    "        out = tf.transpose(out, [0, 2, 1, 3])\n",
    "        out = tf.reshape(out, [tf.shape(x_pmhc)[0], tf.shape(x_pmhc)[1], self.heads * self.att_dim])\n",
    "        out = tf.matmul(out, tf.cast(self.out_w, self.compute_dtype)) + tf.cast(self.out_b, self.compute_dtype)\n",
    "        out = self.norm2(out)\n",
    "        if self.return_att_weights:\n",
    "            return out, att\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def mask_2d(self, p_mask, m_mask):\n",
    "        p_mask = tf.cast(p_mask, self.compute_dtype)\n",
    "        m_mask = tf.cast(m_mask, self.compute_dtype)\n",
    "        p_mask = tf.expand_dims(p_mask, axis=-1)\n",
    "        m_mask = tf.expand_dims(m_mask, axis=-1)  # (B, N, 1), (B, M, 1)\n",
    "        # zero square masks\n",
    "        self_peptide_mask = tf.zeros_like(p_mask, dtype=self.compute_dtype)  # (B, N, 1)\n",
    "        self_peptide_mask_2d = tf.broadcast_to(self_peptide_mask, (\n",
    "            tf.shape(p_mask)[0], tf.shape(p_mask)[1], tf.shape(p_mask)[1]))  # (B, N, N)\n",
    "        if self.self_attn_mhc:\n",
    "            self_mhc_mask = m_mask\n",
    "        else:\n",
    "            self_mhc_mask = tf.zeros_like(m_mask, dtype=self.compute_dtype)\n",
    "        self_mhc_mask_2d = tf.broadcast_to(self_mhc_mask, (\n",
    "            tf.shape(m_mask)[0], tf.shape(m_mask)[1], tf.shape(m_mask)[1]))  # (B, M, M)\n",
    "        # one and zero masks\n",
    "        pep_mhc_mask_secondpart = tf.broadcast_to(p_mask, (tf.shape(p_mask)[0], tf.shape(p_mask)[1],\n",
    "                                                           tf.shape(m_mask)[-1]))  # (B, N, M)\n",
    "        pep_mhc_mask_secondpart = pep_mhc_mask_secondpart * tf.transpose(m_mask,\n",
    "                                                                         perm=[0, 2, 1])  # (B,N,M)*(B,1,M)=(B, N, M)\n",
    "        mhc_pep_mask_secondpart = tf.broadcast_to(m_mask, (tf.shape(m_mask)[0], tf.shape(m_mask)[1],\n",
    "                                                           tf.shape(p_mask)[-1]))  # (B, M, N)\n",
    "        mhc_pep_mask_secondpart = mhc_pep_mask_secondpart * tf.transpose(p_mask,\n",
    "                                                                         perm=[0, 2, 1])  # (B,M,N)*(B,1,N)=(B, M, N)\n",
    "        # combined masks (B,N+M,N+M)\n",
    "        combined_mask_1 = tf.concat([self_peptide_mask_2d, pep_mhc_mask_secondpart], axis=2)  # (B, N, N+M)\n",
    "        combined_mask_2 = tf.concat([mhc_pep_mask_secondpart, self_mhc_mask_2d], axis=2)  # (B, M, N+M)\n",
    "        final_mask = tf.concat([combined_mask_1, combined_mask_2], axis=1)  # (B, N+M, N+M)\n",
    "        final_mask_t = tf.transpose(final_mask, perm=[0, 2, 1])  # (B,... same)\n",
    "        final_mask = tf.multiply(final_mask, final_mask_t)\n",
    "        return final_mask\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package='custom_layers', name='AddGaussianNoise')\n",
    "class AddGaussianNoise(layers.Layer):\n",
    "    def __init__(self, std=0.1, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.std = std\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(x), stddev=tf.cast(self.std, x.dtype), dtype=x.dtype)\n",
    "            return x + noise\n",
    "        return x\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package='custom_layers', name='PositionalEncoding')\n",
    "class PositionalEncoding(keras.layers.Layer):\n",
    "    \"\"\"Sinusoidal Positional Encoding layer that applies encodings only to non-masked tokens.\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, pos_range=100, mask_token=-1., pad_token=-2., name='positional_encoding', **kwargs):\n",
    "        super().__init__(name=name)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.pos_range = pos_range\n",
    "        self.mask_token = mask_token\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "    def build(self, x):\n",
    "        pos = tf.range(self.pos_range, dtype=tf.float32)[:, tf.newaxis]\n",
    "        i = tf.range(self.embed_dim, dtype=tf.float32)[tf.newaxis, :]\n",
    "        angle_rates = tf.pow(300.0, -(2.0 * tf.floor(i // 2)) / tf.cast(self.embed_dim, tf.float32))\n",
    "        angle_rads = pos * angle_rates\n",
    "\n",
    "        sines = tf.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        self.pos_encoding = tf.cast(pos_encoding, dtype=self.compute_dtype)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        pe = self.pos_encoding[:, :seq_len, :]\n",
    "        mask = tf.cast(mask[:, :, tf.newaxis], x.dtype)\n",
    "        mask = tf.where(mask == self.pad_token, tf.cast(0.0, x.dtype), tf.cast(1.0, x.dtype))\n",
    "        pe = tf.cast(pe, x.dtype) * mask\n",
    "        return x + pe\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package='Custom', name='GlobalMeanPooling1D')\n",
    "class GlobalMeanPooling1D(layers.Layer):\n",
    "    \"\"\"Global mean pooling layer.\"\"\"\n",
    "\n",
    "    def __init__(self, axis=-1, name=\"global_mean_pooling_\", **kwargs):\n",
    "        super(GlobalMeanPooling1D, self).__init__(name=name)\n",
    "        self.axis = axis\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        return tf.math.reduce_mean(input_tensor, axis=self.axis, keepdims=False)\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package='Custom', name='GlobalSTDPooling1D')\n",
    "class GlobalSTDPooling1D(layers.Layer):\n",
    "    \"\"\"Global Standard Deviation Pooling layer.\"\"\"\n",
    "\n",
    "    def __init__(self, axis=1, name='global_std_pooling', **kwargs):\n",
    "        super(GlobalSTDPooling1D, self).__init__(name=name)\n",
    "        self.axis = axis\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        pooled_std = tf.math.reduce_std(input_tensor, axis=self.axis, keepdims=False, name=None)\n",
    "        return pooled_std + 1e-9\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package='Custom', name='GlobalMaxPooling1D')\n",
    "class GlobalMaxPooling1D(layers.Layer):\n",
    "    \"\"\"Global max pooling layer.\"\"\"\n",
    "\n",
    "    def __init__(self, axis=-1, name=\"global_max_pooling_\", **kwargs):\n",
    "        super(GlobalMaxPooling1D, self).__init__(name=name)\n",
    "        self.axis = axis\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        return tf.math.reduce_max(input_tensor, axis=self.axis, keepdims=False)\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package='custom_layers', name='MaskedEmbedding')\n",
    "class MaskedEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, mask_token=-1., pad_token=-2., name='masked_embedding', **kwargs):\n",
    "        super().__init__(name=name)\n",
    "        self.mask_token = mask_token\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        mask = tf.where((mask == self.pad_token) | (mask == self.mask_token), 0., 1.)\n",
    "        mask = tf.cast(mask, x.dtype)\n",
    "        return x * mask[:, :, tf.newaxis]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'mask_token': self.mask_token,\n",
    "            'pad_token': self.pad_token,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "@tf.function(reduce_retracing=True)\n",
    "def masked_categorical_crossentropy(y_true_and_pred, mask, pad_token=-2.0, sample_weight=None, type='cce'):\n",
    "    \"\"\"Compute masked categorical cross-entropy loss.\"\"\"\n",
    "    y_true, y_pred = tf.split(y_true_and_pred, num_or_size_splits=2, axis=-1)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.0)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(mask, pad_token), tf.float32)\n",
    "    if mask.shape.rank is not None and mask.shape.rank > 2 and mask.shape[-1] == 1:\n",
    "        mask = tf.squeeze(mask, axis=-1)\n",
    "\n",
    "    if type == 'cce':\n",
    "        loss = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=-1)\n",
    "    elif type == 'mse':\n",
    "        loss = tf.reduce_sum(tf.square(y_true - y_pred), axis=-1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported loss type: {type}\")\n",
    "\n",
    "    mask = tf.cast(tf.broadcast_to(mask, tf.shape(loss)), tf.float32)\n",
    "    masked_loss = loss * mask\n",
    "\n",
    "    if sample_weight is not None:\n",
    "        sample_weight = tf.cast(sample_weight, tf.float32)\n",
    "        if sample_weight.shape.rank == 2 and sample_weight.shape[1] == 1:\n",
    "            sample_weight = tf.squeeze(sample_weight, axis=-1)\n",
    "        masked_loss *= sample_weight[:, tf.newaxis]\n",
    "        mask *= sample_weight[:, tf.newaxis]\n",
    "\n",
    "    total_loss = tf.reduce_sum(masked_loss)\n",
    "    total_weight = tf.reduce_sum(mask)\n",
    "    ce_loss = tf.math.divide_no_nan(total_loss, total_weight)\n",
    "    return tf.cast(ce_loss, tf.float32)\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package='Custom', name='BinaryMCC')\n",
    "class BinaryMCC(tf.keras.metrics.Metric):\n",
    "    \"\"\"\n",
    "    Matthews Correlation Coefficient for binary classification.\n",
    "\n",
    "    Simple implementation using the direct formula:\n",
    "    MCC = (TP*TN - FP*FN) / sqrt((TP+FP)(TP+FN)(TN+FP)(TN+FN))\n",
    "\n",
    "    Range: -1 (worst) to +1 (perfect), 0 = random\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name='binary_mcc', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.tn = self.add_weight(name='tn', initializer='zeros')\n",
    "        self.fp = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.fn = self.add_weight(name='fn', initializer='zeros')\n",
    "\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Convert to binary\n",
    "        y_pred_binary = tf.cast(y_pred >= self.threshold, tf.float32)\n",
    "        y_true_binary = tf.cast(y_true, tf.float32)\n",
    "\n",
    "        # Calculate TP, TN, FP, FN\n",
    "        tp = tf.reduce_sum(y_true_binary * y_pred_binary)\n",
    "        tn = tf.reduce_sum((1 - y_true_binary) * (1 - y_pred_binary))\n",
    "        fp = tf.reduce_sum((1 - y_true_binary) * y_pred_binary)\n",
    "        fn = tf.reduce_sum(y_true_binary * (1 - y_pred_binary))\n",
    "\n",
    "        # Update running totals\n",
    "        self.tp.assign_add(tp)\n",
    "        self.tn.assign_add(tn)\n",
    "        self.fp.assign_add(fp)\n",
    "        self.fn.assign_add(fn)\n",
    "\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def result(self):\n",
    "        # MCC = (TP*TN - FP*FN) / sqrt((TP+FP)(TP+FN)(TN+FP)(TN+FN))\n",
    "        numerator = self.tp * self.tn - self.fp * self.fn\n",
    "        denominator = tf.sqrt(\n",
    "            (self.tp + self.fp) * (self.tp + self.fn) *\n",
    "            (self.tn + self.fp) * (self.tn + self.fn)\n",
    "        )\n",
    "\n",
    "        # Handle division by zero\n",
    "        return tf.where(\n",
    "            tf.equal(denominator, 0.0),\n",
    "            tf.constant(0.0, dtype=self.dtype),\n",
    "            numerator / denominator\n",
    "        )\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.tp.assign(0.0)\n",
    "        self.tn.assign(0.0)\n",
    "        self.fp.assign(0.0)\n",
    "        self.fn.assign(0.0)"
   ],
   "id": "e7194ade1edf92e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# original\n",
    "\n",
    "def pmbind_multitask_modified(max_pep_len: int,\n",
    "                              max_mhc_len: int,\n",
    "                              emb_dim: int = 32,\n",
    "                              heads: int = 2,\n",
    "                              transformer_layers: int = 2,\n",
    "                              mask_token: float = MASK_TOKEN,\n",
    "                              pad_token: float = PAD_TOKEN,\n",
    "                              noise_std: float = 0.1,\n",
    "                              latent_dim: int = 128,\n",
    "                              drop_out_rate: float = 0.2,\n",
    "                              l2_reg: float = 0.01,\n",
    "                              ESM_dim: int = 1536):\n",
    "    \"\"\"\n",
    "    A multi-task model for semi-supervised pMHC analysis, updated to use\n",
    "    the custom SelfAttentionWith2DMask layer.\n",
    "    \"\"\"\n",
    "    # -------------------------------------------------------------------\n",
    "    # INPUTS\n",
    "    # -------------------------------------------------------------------\n",
    "    pep_blossom62_in = layers.Input((max_pep_len, 23), name=\"pep_blossom62\")\n",
    "    pep_mask_in = layers.Input((max_pep_len,), name=\"pep_mask\")\n",
    "    pep_ohe_target_in = layers.Input((max_pep_len, 21), name=\"pep_ohe_target\")\n",
    "\n",
    "    mhc_emb_in = layers.Input((max_mhc_len, ESM_dim), name=\"mhc_emb\")\n",
    "    mhc_mask_in = layers.Input((max_mhc_len,), name=\"mhc_mask\")\n",
    "    mhc_ohe_target_in = layers.Input((max_mhc_len, 21), name=\"mhc_ohe_target\")\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # SHARED ENCODER\n",
    "    # -------------------------------------------------------------------\n",
    "    # 1. Embed Peptide and MHC to the same dimension\n",
    "    pep = MaskedEmbedding(mask_token, pad_token, name=\"pep_mask_embed\")(pep_blossom62_in, pep_mask_in)\n",
    "    # pep = layers.LayerNormalization(name=\"pep_norm_input1\")(pep)\n",
    "    pep = PositionalEncoding(23, int(max_pep_len * 3), name=\"pep_pos_enc\")(pep, pep_mask_in)\n",
    "    pep = layers.GaussianNoise(noise_std, name=\"pep_gaussian_noise\")(pep)\n",
    "    pep = layers.SpatialDropout1D(drop_out_rate, name=\"pep_dropout\")(\n",
    "        pep)  # SpatialDropout1D for sequence data to drop entire feature maps\n",
    "    pep = layers.Dense(emb_dim, name=\"pep_dense_embed\")(pep)\n",
    "    pep = layers.Dropout(drop_out_rate)(pep)\n",
    "\n",
    "    mhc = MaskedEmbedding(mask_token, pad_token, name=\"mhc_mask_embed\")(mhc_emb_in, mhc_mask_in)\n",
    "    # mhc = layers.LayerNormalization(name=\"mhc_norm_layer_inp\")(mhc)\n",
    "    mhc = layers.GaussianNoise(noise_std, name=\"mhc_gaussian_noise\")(mhc)\n",
    "    mhc = PositionalEncoding(ESM_dim, int(max_mhc_len * 3), name=\"mhc_pos_enc\")(mhc, mhc_mask_in)\n",
    "    mhc = layers.SpatialDropout1D(drop_out_rate, name=\"mhc_dropout\")(\n",
    "        mhc)  # SpatialDropout1D for sequence data to drop entire feature maps\n",
    "    mhc = layers.Dense(emb_dim, name=\"mhc_dense_embed\")(mhc)\n",
    "    mhc = layers.Dropout(drop_out_rate)(mhc)\n",
    "    #mhc = layers.LayerNormalization(name=\"mhc_layer_norm\")(mhc)\n",
    "\n",
    "    pmhc_concat = layers.Concatenate(axis=1, name=\"pmhc_concat\")([pep, mhc])  # (B, P+M, D)\n",
    "\n",
    "    # pmhc self-attention with 2D mask\n",
    "    pmhc_interaction, pmhc_attn_weights = SelfAttentionWith2DMask(\n",
    "        query_dim=emb_dim,\n",
    "        context_dim=emb_dim,\n",
    "        output_dim=emb_dim,\n",
    "        heads=heads,\n",
    "        return_att_weights=True,\n",
    "        self_attn_mhc=False,  # Prevent both peptide and MHC self-attention in this layer\n",
    "        apply_rope=True,\n",
    "        name=\"pmhc_2d_masked_attention\"\n",
    "    )(pmhc_concat, pep_mask_in, mhc_mask_in)\n",
    "\n",
    "    # The final output of the encoder is our shared latent sequence\n",
    "    latent_sequence = pmhc_interaction  # Shape: (B, P+M, D)\n",
    "    #latent_sequence = layers.BatchNormalization(name=\"latent_sequence_norm\")(latent_sequence)\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # TASK 1: BINDING PREDICTION HEAD # TODO this part directly affects clustering\n",
    "    # -------------------------------------------------------------------\n",
    "    # pooled_latent = GlobalMaxPooling1D(name=\"latent_vector_pool-2\", axis=-2)(latent_sequence) # (B, D) # Tells how strong the overall signal is across the sequence (using this forces the model to pu one feature as the distance/binder information)\n",
    "    pooled_std2 = GlobalSTDPooling1D(name=\"latent_vector_std-2\", axis=-2)(\n",
    "       latent_sequence)  # (B, D) # Tells how much variation there is in the signal across the sequence\n",
    "    pooled_mean1 = GlobalMeanPooling1D(name=\"latent_vector_pool-1\", axis=-1)(\n",
    "       latent_sequence)  # (B, P+M) # Tells how strong the overall signal is across the features\n",
    "    # pooled_std1 = GlobalSTDPooling1D(name=\"latent_vector_std-1\", axis=-1)(latent_sequence)  # (B, P+M) # Tells how much variation there is in the signal across the features\n",
    "\n",
    "    # concatenate mean and std pooled vectors\n",
    "    pooled_latent = layers.Concatenate(name=\"pooled_latent_concat\", axis=-1)(\n",
    "       [pooled_mean1, pooled_std2])  # (B, 1*(D+P+M))\n",
    "\n",
    "    binding_head = layers.Dropout(drop_out_rate * 1.5, name='dropout_pooled_latent')(pooled_latent)\n",
    "    binding_head = layers.Dense(emb_dim // 2, activation=\"gelu\", name=\"binding_dense1\",\n",
    "                                kernel_regularizer=keras.regularizers.l2(l2_reg),\n",
    "                                bias_regularizer=keras.regularizers.l2(l2_reg))(binding_head)\n",
    "    binding_head = layers.Dropout(drop_out_rate, name=\"binding_dropout2\")(binding_head)\n",
    "    binding_pred = layers.Dense(1, activation=\"sigmoid\", name=\"binding_pred\", dtype=\"float32\")(binding_head)  # (B, 1)\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # TASK 2: RECONSTRUCTION HEAD\n",
    "    # -------------------------------------------------------------------\n",
    "    pep_latent_seq = latent_sequence[:, :max_pep_len, :]  # (B, P, D)\n",
    "    mhc_latent_seq = latent_sequence[:, max_pep_len:, :]  # (B, M, D)\n",
    "\n",
    "    pep_recon_head = layers.Dropout(drop_out_rate, name='pep_recon_dropout_1')(pep_latent_seq)\n",
    "    pep_recon_head = layers.Dense(emb_dim * 2, activation='relu')(pep_recon_head)\n",
    "    pep_recon_head = layers.Dropout(drop_out_rate, name='pep_recon_dropout_2')(pep_recon_head)\n",
    "    pep_recon_pred = layers.Dense(21, activation='softmax', name='pep_reconstruction_pred')(pep_recon_head)\n",
    "\n",
    "    mhc_recon_head = layers.Dropout(drop_out_rate, name='mhc_recon_dropout_1')(mhc_latent_seq)\n",
    "    mhc_recon_head = layers.Dense(emb_dim * 2, activation='relu')(mhc_recon_head)\n",
    "    mhc_recon_head = layers.Dropout(drop_out_rate, name='mhc_recon_dropout_2')(mhc_recon_head)\n",
    "    mhc_recon_pred = layers.Dense(21, activation='softmax', name='mhc_reconstruction_pred')(mhc_recon_head)\n",
    "\n",
    "    pep_out = layers.Concatenate(name='pep_ytrue_ypred')([pep_ohe_target_in, pep_recon_pred])\n",
    "    mhc_out = layers.Concatenate(name='mhc_ytrue_ypred')([mhc_ohe_target_in, mhc_recon_pred])\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # MODEL DEFINITION\n",
    "    # -------------------------------------------------------------------\n",
    "    pMHC_multitask_model = keras.Model(\n",
    "        inputs=[pep_blossom62_in, pep_mask_in, mhc_emb_in, mhc_mask_in, pep_ohe_target_in, mhc_ohe_target_in],\n",
    "        outputs={\n",
    "            \"pep_ytrue_ypred\": pep_out,\n",
    "            \"mhc_ytrue_ypred\": mhc_out,\n",
    "            \"cls_ypred\": binding_pred,\n",
    "            \"attn_weights\": pmhc_attn_weights,  # (B, heads, P+M, P+M)\n",
    "            \"latent_vector\": pooled_latent,  # (B, P+M+D)\n",
    "            \"latent_seq\": latent_sequence  # (B, P+M, D)\n",
    "        },\n",
    "        name=\"pMHC_Multitask_Transformer_Custom_Attention\"\n",
    "    )\n",
    "\n",
    "    return pMHC_multitask_model\n"
   ],
   "id": "87af00db23215cd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# PARQUET DATA LOADING WITH DATAGENERATOR AND DYNAMIC MASKING\n",
    "# ============================================================================\n",
    "\n",
    "# Global variables for DataGenerator\n",
    "EMB_DB_: np.lib.npyio.NpzFile | None = None\n",
    "MHC_CLASS = 1\n",
    "\n",
    "\n",
    "def load_embedding_db(npz_path: str):\n",
    "    \"\"\"Load embedding database with memory mapping.\"\"\"\n",
    "    try:\n",
    "        # Try loading without explicit allow_pickle (defaults to True)\n",
    "        return np.load(npz_path, mmap_mode=\"r\")\n",
    "    except ValueError as e:\n",
    "        if \"allow_pickle\" in str(e):\n",
    "            # If pickle error, try with explicit allow_pickle=True\n",
    "            print(f\"Warning: NPZ file contains pickled data, loading with allow_pickle=True\")\n",
    "            return np.load(npz_path, mmap_mode=\"r\", allow_pickle=True)\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "\n",
    "def apply_dynamic_masking(features, emd_mask_d2=True):\n",
    "    \"\"\"\n",
    "    Applies random masking for training augmentation inside the tf.data pipeline.\n",
    "    This version is corrected to match the original DataGenerator logic.\n",
    "    \"\"\"\n",
    "    # Peptide Masking\n",
    "    valid_pep_positions = tf.where(tf.equal(features[\"pep_mask\"], NORM_TOKEN))\n",
    "    num_valid_pep = tf.shape(valid_pep_positions)[0]\n",
    "    # At least 2 positions, or 15% of the valid sequence length\n",
    "    num_to_mask_pep = tf.maximum(2, tf.cast(tf.cast(num_valid_pep, tf.float32) * 0.15, tf.int32))\n",
    "    shuffled_pep_indices = tf.random.shuffle(valid_pep_positions)[:num_to_mask_pep]\n",
    "    if tf.shape(shuffled_pep_indices)[0] > 0:\n",
    "        # Update the mask to MASK_TOKEN (-1.0)\n",
    "        features[\"pep_mask\"] = tf.tensor_scatter_nd_update(features[\"pep_mask\"], shuffled_pep_indices,\n",
    "                                                           tf.repeat(MASK_TOKEN, num_to_mask_pep))\n",
    "        # Zero out the feature values for the masked positions\n",
    "        feat_dtype = features[\"pep_blossom62\"].dtype\n",
    "        mask_updates_pep = tf.fill([num_to_mask_pep, tf.shape(features[\"pep_blossom62\"])[-1]],\n",
    "                                   tf.cast(MASK_VALUE, feat_dtype))\n",
    "        features[\"pep_blossom62\"] = tf.tensor_scatter_nd_update(features[\"pep_blossom62\"], shuffled_pep_indices,\n",
    "                                                                mask_updates_pep)\n",
    "    # MHC Masking\n",
    "    valid_mhc_positions = tf.where(tf.equal(features[\"mhc_mask\"], NORM_TOKEN))\n",
    "    num_valid_mhc = tf.shape(valid_mhc_positions)[0]\n",
    "    # At least 5 positions, or 15% of the valid sequence length\n",
    "    num_to_mask_mhc = tf.maximum(10, tf.cast(tf.cast(num_valid_mhc, tf.float32) * 0.15, tf.int32))\n",
    "    shuffled_mhc_indices = tf.random.shuffle(valid_mhc_positions)[:num_to_mask_mhc]\n",
    "    if tf.shape(shuffled_mhc_indices)[0] > 0:\n",
    "        # Update the mask to MASK_TOKEN (-1.0)\n",
    "        features[\"mhc_mask\"] = tf.tensor_scatter_nd_update(features[\"mhc_mask\"], shuffled_mhc_indices,\n",
    "                                                           tf.repeat(MASK_TOKEN, num_to_mask_mhc))\n",
    "        # Zero out the feature values for the masked positions\n",
    "        mhc_dtype = features[\"mhc_emb\"].dtype\n",
    "        mask_updates_mhc = tf.fill([num_to_mask_mhc, tf.shape(features[\"mhc_emb\"])[-1]], tf.cast(MASK_VALUE, mhc_dtype))\n",
    "        features[\"mhc_emb\"] = tf.tensor_scatter_nd_update(features[\"mhc_emb\"], shuffled_mhc_indices, mask_updates_mhc)\n",
    "\n",
    "    # Dimension-level masking for MHC embeddings\n",
    "    if emd_mask_d2:\n",
    "        # Find positions that are STILL valid (not padded and not positionally masked)\n",
    "        remaining_valid_mhc = tf.where(tf.equal(features[\"mhc_mask\"], NORM_TOKEN))\n",
    "        if tf.shape(remaining_valid_mhc)[0] > 0:\n",
    "            # Get the embeddings at these remaining valid positions\n",
    "            valid_embeddings = tf.gather_nd(features[\"mhc_emb\"], remaining_valid_mhc)\n",
    "            # Create a random mask for the feature dimensions\n",
    "            dim_mask = tf.random.uniform(shape=tf.shape(valid_embeddings), dtype=features[\"mhc_emb\"].dtype) < tf.cast(\n",
    "                0.15, features[\"mhc_emb\"].dtype)\n",
    "            # Apply the mask (multiply by 0 where True, 1 where False)\n",
    "            masked_embeddings = valid_embeddings * tf.cast(~dim_mask, features[\"mhc_emb\"].dtype)\n",
    "            # Scatter the modified embeddings back into the original tensor\n",
    "            features[\"mhc_emb\"] = tf.tensor_scatter_nd_update(features[\"mhc_emb\"], remaining_valid_mhc,\n",
    "                                                              masked_embeddings)\n",
    "    return features\n",
    "\n",
    "\n",
    "def min_max_norm(emb, mhc_class=1):\n",
    "    \"\"\"min max of ESM3-open embeddings 25.09.2025\"\"\"\n",
    "    if mhc_class == 2:\n",
    "        min = -14144.0\n",
    "        max = 1456.0\n",
    "    else:\n",
    "        min = -15360.0\n",
    "        max = 1440.0\n",
    "    # normalize embedding\n",
    "    emb_norm = 2 * (emb - min) / (max- min) - 1\n",
    "    return emb_norm\n",
    "\n",
    "def log_norm_zscore(emb, eps=1e-9):\n",
    "    \"\"\"z-score normalization after log-transform\"\"\"\n",
    "    emb_shifted = emb - emb.min() + eps\n",
    "    emb_log = np.log1p(emb_shifted)\n",
    "    mean, std = emb_log.mean(), emb_log.std()\n",
    "    emb_norm = (emb_log - mean) / std\n",
    "    return emb_norm\n",
    "\n",
    "#### Parquet dataloader\n",
    "def min_max_norm(emb, mhc_class=1):\n",
    "    \"\"\"min max of ESM3-open embeddings 25.09.2025\"\"\"\n",
    "    if mhc_class == 2:\n",
    "        min = -14144.0\n",
    "        max = 1456.0\n",
    "    else:\n",
    "        min = -15360.0\n",
    "        max = 1440.0\n",
    "    # normalize embedding\n",
    "    emb_norm = 2 * (emb - min) / (max- min) - 1\n",
    "    return emb_norm\n",
    "\n",
    "def log_norm_zscore(emb, eps=1e-9):\n",
    "    \"\"\"z-score normalization after log-transform\"\"\"\n",
    "    emb_shifted = emb - emb.min() + eps\n",
    "    emb_log = np.log1p(emb_shifted)\n",
    "    mean, std = emb_log.mean(), emb_log.std()\n",
    "    emb_norm = (emb_log - mean) / std\n",
    "    return emb_norm\n",
    "\n",
    "\n",
    "class OptimizedDataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"Optimized data generator for training with dynamic masking support.\"\"\"\n",
    "\n",
    "    def __init__(self, df, seq_map, embed_map, max_pep_len, max_mhc_len, batch_size, apply_masking=True, normalization_method=\"clip_norm1000\"):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.seq_map = seq_map\n",
    "        self.embed_map = embed_map\n",
    "        self.max_pep_len = max_pep_len\n",
    "        self.max_mhc_len = max_mhc_len\n",
    "        self.batch_size = batch_size\n",
    "        self.apply_masking = apply_masking\n",
    "        self.long_mer_arr = df['long_mer'].to_numpy()\n",
    "        self.emb_key_arr = df['_emb_key'].to_numpy()\n",
    "        self.cleaned_key_arr = df['_cleaned_key'].to_numpy()\n",
    "        self.mhc_seq_arr = df['_mhc_seq'].to_numpy()\n",
    "        self.label_arr = df['assigned_label'].to_numpy()\n",
    "        self.indices = np.arange(len(df))\n",
    "        self.normalization_method = normalization_method\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.indices) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start_idx, end_idx = index * self.batch_size, min((index + 1) * self.batch_size, len(self.indices))\n",
    "        return self._generate_batch(self.indices[start_idx:end_idx])\n",
    "\n",
    "    def _get_embedding(self, emb_key, cleaned_key):\n",
    "        \"\"\"Get embedding and apply robust normalization.\"\"\"\n",
    "        if MHC_CLASS == 2:\n",
    "            parts = cleaned_key.split('_')\n",
    "            if len(parts) >= 2:\n",
    "                k1, k2 = get_embed_key(clean_key(parts[0]), self.embed_map), get_embed_key(clean_key(parts[1]),\n",
    "                                                                                           self.embed_map)\n",
    "                emb = np.concatenate([EMB_DB_[k1], EMB_DB_[k2]], axis=0)\n",
    "            else:\n",
    "                emb = EMB_DB_[emb_key]\n",
    "        else:\n",
    "            emb = EMB_DB_[emb_key]\n",
    "\n",
    "        # Apply chosen normalization method\n",
    "        return self._normalize_embedding(emb)\n",
    "\n",
    "    def _normalize_embedding(self, emb):\n",
    "        \"\"\"Apply robust normalization to handle extreme values.\"\"\"\n",
    "        if self.normalization_method == \"min_max_norm\":\n",
    "            return min_max_norm(emb)\n",
    "        elif self.normalization_method == \"log_norm_zscore\":\n",
    "            return log_norm_zscore(emb)\n",
    "        elif self.normalization_method == \"clip_norm1000\":\n",
    "            emb_norm = np.clip(emb, -1000, 1000)\n",
    "            return 20 * (emb_norm - (-1000)) / (1000 - (-1000)) - 10\n",
    "        else:\n",
    "            return emb  # No normalization\n",
    "\n",
    "    def _generate_batch(self, batch_indices):\n",
    "        n = len(batch_indices)\n",
    "        data = {\"pep_blossom62\": np.zeros((n, self.max_pep_len, 23), np.float32),\n",
    "                \"pep_mask\": np.full((n, self.max_pep_len), PAD_TOKEN, dtype=np.float32),\n",
    "                \"mhc_emb\": np.zeros((n, self.max_mhc_len, ESM_DIM), np.float32),\n",
    "                \"mhc_mask\": np.full((n, self.max_mhc_len), PAD_TOKEN, dtype=np.float32),\n",
    "                \"pep_ohe_target\": np.zeros((n, self.max_pep_len, 21), np.float32),\n",
    "                \"mhc_ohe_target\": np.zeros((n, self.max_mhc_len, 21), np.float32),\n",
    "                \"labels\": np.zeros((n, 1), np.float32)}\n",
    "\n",
    "        for i, master_idx in enumerate(batch_indices):\n",
    "            pep_seq, emb_key, cleaned_key, mhc_seq = (self.long_mer_arr[master_idx].upper(),\n",
    "                                                      self.emb_key_arr[master_idx],\n",
    "                                                      self.cleaned_key_arr[master_idx],\n",
    "                                                      self.mhc_seq_arr[master_idx])\n",
    "            pep_len = len(pep_seq)\n",
    "            data[\"pep_blossom62\"][i] = seq_to_blossom62(pep_seq, max_seq_len=self.max_pep_len)\n",
    "            #normalise pep\n",
    "            #normalize only valid peptide positions to avoid touching padding\n",
    "            # _pep_slice = data[\"pep_blossom62\"][i, :pep_len]\n",
    "            # pep_norm = 8 * (_pep_slice - (-4)) / (11 - (-4)) - 4 # MIN Max normalization of BLOSSUM62\n",
    "            # _norm = np.linalg.norm(_pep_slice, axis=-1, keepdims=True)\n",
    "            # _norm = np.maximum(_norm, 1e-12)\n",
    "            # data[\"pep_blossom62\"][i, :pep_len] = pep_norm\n",
    "            data[\"pep_ohe_target\"][i] = seq_to_onehot(pep_seq, max_seq_len=self.max_pep_len)\n",
    "            data[\"pep_mask\"][i, :pep_len] = NORM_TOKEN\n",
    "            emb = self._get_embedding(emb_key, cleaned_key)\n",
    "            L = emb.shape[0]\n",
    "            data[\"mhc_emb\"][i, :L] = emb\n",
    "            data[\"mhc_ohe_target\"][i] = seq_to_onehot(mhc_seq, max_seq_len=self.max_mhc_len)\n",
    "            # data[\"mhc_mask\"][i, ~np.all(data[\"mhc_ohe_target\"][i] == PAD_INDEX_OHE, axis=-1)] = NORM_TOKEN\n",
    "            is_padding = np.all(data[\"mhc_ohe_target\"][i, :] == 0, axis=-1)\n",
    "            data[\"mhc_mask\"][i, ~is_padding] = NORM_TOKEN\n",
    "\n",
    "            data[\"labels\"][i, 0] = float(self.label_arr[master_idx])\n",
    "\n",
    "        # Convert to tensors\n",
    "        tensor_data = {k: tf.convert_to_tensor(v) for k, v in data.items()}\n",
    "\n",
    "        # Apply dynamic masking if enabled\n",
    "        if self.apply_masking:\n",
    "            tensor_data = apply_dynamic_masking(tensor_data, emd_mask_d2=True)\n",
    "\n",
    "        return tensor_data\n",
    "\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "\n",
    "def _preprocess_df_chunk(args):\n",
    "    \"\"\"Top-level helper for multiprocessing chunk processing.\"\"\"\n",
    "    chunk, seq_map, embed_map, mhc_class = args\n",
    "    chunk = chunk.copy()\n",
    "\n",
    "    # Replicate original logic\n",
    "    chunk['_cleaned_key'] = chunk.apply(\n",
    "        lambda r: r.get('mhc_embedding_key', r['allele'].replace(' ', '').replace('*', '').replace(':', '')),\n",
    "        axis=1\n",
    "    )\n",
    "    chunk['_emb_key'] = chunk['_cleaned_key'].apply(lambda k: get_embed_key(clean_key(k), embed_map))\n",
    "\n",
    "    if mhc_class == 2:\n",
    "        def _get_mhc_seq_class2(key):\n",
    "            parts = key.split('_')\n",
    "            return seq_map.get(get_embed_key(clean_key(parts[0]), seq_map), '') + \\\n",
    "                   seq_map.get(get_embed_key(clean_key(parts[1]), seq_map), '') if len(parts) >= 2 else ''\n",
    "\n",
    "        chunk['_mhc_seq'] = chunk['_cleaned_key'].apply(_get_mhc_seq_class2)\n",
    "    else:\n",
    "        chunk['_mhc_seq'] = chunk['_emb_key'].apply(lambda k: seq_map.get(get_embed_key(clean_key(k), seq_map), ''))\n",
    "\n",
    "    return chunk\n",
    "\n",
    "\n",
    "def preprocess_df(df, seq_map, embed_map, workers=None, chunks=None):\n",
    "    \"\"\"\n",
    "    Multiprocessed preprocessing. Splits df into chunks, processes in parallel, and\n",
    "    preserves original order. Backward compatible with existing calls.\n",
    "    \"\"\"\n",
    "    if workers is None:\n",
    "        workers = max(1, os.cpu_count() // 2 or 1)\n",
    "    if chunks is None:\n",
    "        chunks = max(1, min(workers * 4, len(df)))  # more chunks than workers for load balancing\n",
    "\n",
    "    parts = np.array_split(df, chunks)\n",
    "    args = [(part, seq_map, embed_map, MHC_CLASS) for part in parts]\n",
    "\n",
    "    if workers == 1 or len(parts) == 1:\n",
    "        processed = [_preprocess_df_chunk(a) for a in args]\n",
    "    else:\n",
    "        with ProcessPoolExecutor(max_workers=workers) as ex:\n",
    "            processed = list(ex.map(_preprocess_df_chunk, args))\n",
    "\n",
    "    out = pd.concat(processed, axis=0)\n",
    "    out = out.loc[df.index]  # preserve original row order\n",
    "    return out\n",
    "\n",
    "# # Load parquet data and setup - Updated paths for local environment\n",
    "parquet_dir = 'cv_folds/'  # Local path\n",
    "allele_seq_path = '../../../../data/alleles/aligned_PMGen_class_1.csv'  # Local path\n",
    "embedding_key_path = '/media/amirreza/Crucial-500/ESM/esm3-open/PMGen_whole_seq_/mhc1_encodings.csv'  # Local path\n",
    "# embedding_npz_path = '../../../dataset/ESM/PMDb_ESM_embeddings_mhc1.npz'  # Local path\n",
    "embedding_npz_path = \"/media/amirreza/Crucial-500/ESM/esm3-open/PMGen_whole_seq_/mhc1_encodings.npz\"\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    \"MHC_CLASS\": 1,\n",
    "    \"MAX_PEP_LEN\": 16,\n",
    "    \"MAX_MHC_LEN\": 314,\n",
    "    \"EMBED_DIM\": 32,\n",
    "    \"HEADS\": 2\n",
    "}\n",
    "\n",
    "MHC_CLASS = config[\"MHC_CLASS\"]\n",
    "PEP_LEN = config[\"MAX_PEP_LEN\"]\n",
    "MHC_LEN = config[\"MAX_MHC_LEN\"]\n",
    "ESM_DIM = 1536  # ESM embedding dimension (will be updated after loading embeddings)\n",
    "\n",
    "print(f\"Dataset configuration:\")\n",
    "print(f\"Max peptide length: {PEP_LEN}\")\n",
    "print(f\"Max MHC length: {MHC_LEN}\")\n",
    "print(f\"ESM embedding dimension: {ESM_DIM} (preliminary)\")\n",
    "print(f\"MHC Class: {MHC_CLASS}\")\n",
    "\n",
    "# Load embedding database\n",
    "print(f\"\\nLoading embedding database from: {embedding_npz_path}\")\n",
    "if not os.path.exists(embedding_npz_path):\n",
    "    print(f\"Error: Embedding NPZ file not found at {embedding_npz_path}\")\n",
    "    raise FileNotFoundError(f\"Embedding NPZ file not found: {embedding_npz_path}\")\n",
    "\n",
    "EMB_DB_ = load_embedding_db(embedding_npz_path)\n",
    "print(f\"Loaded embedding database with {len(EMB_DB_.files)} embeddings\")\n",
    "print(f\"Sample embedding keys: {list(EMB_DB_.files)[:5]}\")\n",
    "\n",
    "# Check embedding dimension\n",
    "first_key = EMB_DB_.files[0]\n",
    "first_embedding = EMB_DB_[first_key]\n",
    "print(f\"First embedding shape: {first_embedding.shape}, dtype: {first_embedding.dtype}\")\n",
    "ESM_DIM = first_embedding.shape[1] if len(first_embedding.shape) > 1 else first_embedding.shape[0]\n",
    "print(f\"Updated ESM embedding dimension: {ESM_DIM}\")\n",
    "\n",
    "# Load sequence and embedding mappings\n",
    "print(\"\\nLoading sequence and embedding mappings...\")\n",
    "print(f\"Allele sequence file: {allele_seq_path}\")\n",
    "print(f\"Embedding key file: {embedding_key_path}\")\n",
    "\n",
    "# Check if files exist\n",
    "if not os.path.exists(allele_seq_path):\n",
    "    print(f\"Error: Allele sequence file not found at {allele_seq_path}\")\n",
    "    raise FileNotFoundError(f\"Allele sequence file not found: {allele_seq_path}\")\n",
    "\n",
    "if not os.path.exists(embedding_key_path):\n",
    "    print(f\"Error: Embedding key file not found at {embedding_key_path}\")\n",
    "    raise FileNotFoundError(f\"Embedding key file not found: {embedding_key_path}\")\n",
    "\n",
    "# Load with error handling\n",
    "try:\n",
    "    seq_df = pd.read_csv(allele_seq_path, index_col=\"allele\")\n",
    "    seq_map = {clean_key(k): v for k, v in seq_df[\"mhc_sequence\"].to_dict().items()}\n",
    "except Exception as e:\n",
    "    print(f\"Error loading sequence file: {e}\")\n",
    "    print(f\"Available columns in {allele_seq_path}:\")\n",
    "    temp_df = pd.read_csv(allele_seq_path)\n",
    "    print(temp_df.columns.tolist())\n",
    "    raise e\n",
    "\n",
    "try:\n",
    "    embed_df = pd.read_csv(embedding_key_path, index_col=\"key\")\n",
    "    embed_map = embed_df[\"mhc_sequence\"].to_dict()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading embedding key file: {e}\")\n",
    "    print(f\"Available columns in {embedding_key_path}:\")\n",
    "    temp_df = pd.read_csv(embedding_key_path)\n",
    "    print(temp_df.columns.tolist())\n",
    "    raise e\n",
    "\n",
    "print(f\"Loaded {len(seq_map)} sequence mappings\")\n",
    "print(f\"Loaded {len(embed_map)} embedding key mappings\")\n",
    "\n",
    "# Load training and validation data\n",
    "train_parquet_path = os.path.join(parquet_dir, 'new_df2/train_size300002_seed1.parquet')\n",
    "# val_parquet_path = os.path.join(parquet_dir, 'fold_1_val.parquet')\n",
    "val_parquet_path = os.path.join(parquet_dir, 'CapsNet_IEDB_test_reduced.parquet')\n",
    "\n",
    "print(\"\\nLoading parquet files...\")\n",
    "print(f\"Training file: {train_parquet_path}\")\n",
    "print(f\"Validation file: {val_parquet_path}\")\n",
    "\n",
    "# Check if files exist\n",
    "if not os.path.exists(train_parquet_path):\n",
    "    print(f\"Error: Training file not found at {train_parquet_path}\")\n",
    "    print(\"Available files in parquet directory:\")\n",
    "    for f in os.listdir(parquet_dir):\n",
    "        print(f\"  {f}\")\n",
    "    raise FileNotFoundError(f\"Training parquet file not found: {train_parquet_path}\")\n",
    "\n",
    "if not os.path.exists(val_parquet_path):\n",
    "    print(f\"Error: Validation file not found at {val_parquet_path}\")\n",
    "    raise FileNotFoundError(f\"Validation parquet file not found: {val_parquet_path}\")\n",
    "\n",
    "train_df = pd.read_parquet(train_parquet_path)\n",
    "# train_df = train_df.iloc[:5000, :]\n",
    "val_df = pd.read_parquet(val_parquet_path)\n",
    "\n",
    "print(f\"Training samples: {len(train_df):,}\")\n",
    "print(f\"Validation samples: {len(val_df):,}\")\n",
    "print(f\"Training columns: {list(train_df.columns)}\")\n",
    "\n",
    "# Preprocess dataframes\n",
    "print(\"\\nPreprocessing datasets...\")\n",
    "train_df = preprocess_df(train_df, seq_map, embed_map)\n",
    "val_df = preprocess_df(val_df, seq_map, embed_map)\n"
   ],
   "id": "8bc22a6c00f77ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# TFRECORDS DATA LOADING\n",
    "# ============================================================================\n",
    "\n",
    "tfrecord_dir = '/home/amirreza/Desktop/PMBind/tests/dataset/parquets/resampled_/cv_folds/new_df2/train_size300002_seed1'\n",
    "\n",
    "def load_metadata(tfrecord_dir):\n",
    "    \"\"\"Load metadata from tfrecords directory\"\"\"\n",
    "    metadata_path = os.path.join(tfrecord_dir, 'metadata.json')\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    return metadata\n",
    "\n",
    "def load_embedding_table(lookup_path, metadata):\n",
    "    \"\"\"Load MHC embedding lookup table with correct dtype.\"\"\"\n",
    "    with np.load(lookup_path) as data:\n",
    "        num_embeddings = len(data.files)\n",
    "        # FIX: Use float32 to avoid potential precision loss from float16.\n",
    "        table = np.zeros((num_embeddings, metadata['MAX_MHC_LEN'], metadata['ESM_DIM']), dtype=np.float32)\n",
    "        for i in range(num_embeddings):\n",
    "            table[i] = data[str(i)]\n",
    "    return tf.constant(table, dtype=tf.float32)\n",
    "\n",
    "def normalize_embedding_tf(emb):\n",
    "    \"\"\"\n",
    "    TensorFlow implementation of the normalization logic from the DataGenerator.\n",
    "    This is the critical fix.\n",
    "    \"\"\"\n",
    "    emb_norm = tf.clip_by_value(emb, -1000.0, 1000.0)\n",
    "    # Scale to [-10, 10]\n",
    "    return 20.0 * (emb_norm - (-1000.0)) / (1000.0 - (-1000.0)) - 10.0\n",
    "\n",
    "# Create BLOSUM62 lookup table\n",
    "_blosum_vectors = [BLOSUM62[aa] for aa in AMINO_ACID_VOCAB]\n",
    "_blosum_vectors.append([PAD_VALUE] * len(_blosum_vectors[0]))\n",
    "BLOSUM62_TABLE = tf.constant(np.array(_blosum_vectors), dtype=tf.float32)\n",
    "metadata = load_metadata(tfrecord_dir)\n",
    "\n",
    "# Extract all the important values from metadata\n",
    "MAX_PEP_LEN = metadata['MAX_PEP_LEN']\n",
    "MAX_MHC_LEN = metadata['MAX_MHC_LEN']\n",
    "ESM_DIM = metadata['ESM_DIM']\n",
    "train_samples = metadata['train_samples']\n",
    "\n",
    "def _parse_tf_example(example_proto, mhc_embedding_table):\n",
    "    \"\"\"\n",
    "    Parse TFRecord example.\n",
    "    Creates separate masks for inputs (PAD_TOKEN) and targets (0.0 for padding).\n",
    "    For MHC, zeroes out the gap column and then derives padding from rows that are all zeros.\n",
    "    \"\"\"\n",
    "    metadata = load_metadata(tfrecord_dir)\n",
    "    MAX_PEP_LEN = metadata['MAX_PEP_LEN']\n",
    "    MAX_MHC_LEN = metadata['MAX_MHC_LEN']\n",
    "\n",
    "    feature_description = {\n",
    "        'pep_indices': tf.io.FixedLenFeature([MAX_PEP_LEN], tf.int64),\n",
    "        'pep_ohe_indices': tf.io.FixedLenFeature([MAX_PEP_LEN], tf.int64),\n",
    "        'mhc_ohe_indices': tf.io.FixedLenFeature([MAX_MHC_LEN], tf.int64),\n",
    "        'embedding_id': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    parsed = tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "    pep_indices = parsed['pep_indices']\n",
    "    pep_ohe_indices = parsed['pep_ohe_indices']\n",
    "    mhc_ohe_indices = parsed['mhc_ohe_indices']\n",
    "\n",
    "    # --- FEATURE TENSORS ---\n",
    "    embedding_id = tf.cast(parsed['embedding_id'], tf.int32)\n",
    "    mhc_emb = tf.gather(mhc_embedding_table, embedding_id)\n",
    "    pep_blossom62_input = tf.gather(BLOSUM62_TABLE, tf.cast(pep_indices, tf.int32))\n",
    "\n",
    "    # --- ONE-HOT TARGETS ---\n",
    "    vocab_size_ohe = len(AA)  # 21\n",
    "    pep_ohe_target = tf.one_hot(tf.cast(pep_ohe_indices, tf.int32), depth=vocab_size_ohe, dtype=tf.float32)\n",
    "    mhc_ohe_target = tf.one_hot(tf.cast(mhc_ohe_indices, tf.int32), depth=vocab_size_ohe, dtype=tf.float32)\n",
    "\n",
    "    # Zero the GAP column to match seq_to_onehot behavior\n",
    "    gap_idx = tf.constant(AA_TO_INT['-'], dtype=tf.int32)\n",
    "    gap_zero_vec = 1.0 - tf.one_hot(gap_idx, depth=vocab_size_ohe, dtype=tf.float32)  # ones except 0 at gap_idx\n",
    "    pep_ohe_target = pep_ohe_target * gap_zero_vec\n",
    "    mhc_ohe_target = mhc_ohe_target * gap_zero_vec\n",
    "\n",
    "    # --- MASKS ---\n",
    "    # Input mask for peptides (uses PAD_TOKEN = -2.0)\n",
    "    pep_mask_input = tf.where(pep_indices == PAD_INDEX_62, PAD_TOKEN, NORM_TOKEN)\n",
    "\n",
    "    # Derive MHC padding from all-zero rows in the one-hot (after zeroing gap column)\n",
    "    mhc_is_padding = tf.reduce_all(tf.equal(mhc_ohe_target, 0.0), axis=-1)\n",
    "\n",
    "    # Input mask for MHC (PAD_TOKEN on padding rows, NORM_TOKEN otherwise)\n",
    "    mhc_mask_input = tf.where(mhc_is_padding, PAD_TOKEN, NORM_TOKEN)\n",
    "\n",
    "    # Target masks (0.0 for padding, 1.0 otherwise)\n",
    "    pep_mask_target = tf.where(pep_ohe_indices == PAD_INDEX_OHE, 0.0, 1.0)\n",
    "    mhc_mask_target = tf.where(mhc_is_padding, 0.0, 1.0)\n",
    "\n",
    "    # Apply target masks to zero out padded regions\n",
    "    pep_ohe_target = pep_ohe_target * tf.expand_dims(pep_mask_target, axis=-1)\n",
    "    mhc_ohe_target = mhc_ohe_target * tf.expand_dims(mhc_mask_target, axis=-1)\n",
    "\n",
    "    # Labels\n",
    "    labels = tf.cast(parsed['label'], tf.float32)\n",
    "    labels = tf.expand_dims(labels, axis=-1)\n",
    "\n",
    "    return {\n",
    "        \"pep_blossom62\": tf.cast(pep_blossom62_input, tf.float32),\n",
    "        \"pep_mask\": tf.cast(pep_mask_input, tf.float32),\n",
    "        \"mhc_emb\": tf.cast(mhc_emb, tf.float32),\n",
    "        \"mhc_mask\": tf.cast(mhc_mask_input, tf.float32),\n",
    "        \"pep_ohe_target\": pep_ohe_target,\n",
    "        \"mhc_ohe_target\": mhc_ohe_target,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# create_dataset_from_files also remains unchanged\n",
    "def create_dataset_from_files(file_list, batch_size, shuffle=True, apply_masking=False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(file_list)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda x: tf.data.TFRecordDataset(x, compression_type=\"GZIP\"),\n",
    "        cycle_length=len(file_list) if shuffle else 1,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        deterministic=not shuffle\n",
    "    )\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=10000)\n",
    "    dataset = dataset.map(\n",
    "        lambda x: _parse_tf_example(x, MHC_EMBEDDING_TABLE),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    if apply_masking:\n",
    "        dataset = dataset.map(apply_dynamic_masking, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset"
   ],
   "id": "a1833e9b3e695452"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def inspect_tfrecord_schema(tfrecord_path):\n",
    "    \"\"\"Inspect the actual schema of a TFRecord file\"\"\"\n",
    "    print(f\"Inspecting TFRecord: {tfrecord_path}\")\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_path, compression_type=\"GZIP\")\n",
    "\n",
    "    # Get first example\n",
    "    for raw_record in dataset.take(1):\n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(raw_record.numpy())\n",
    "\n",
    "        print(\"\\nFeatures found in TFRecord:\")\n",
    "        for feature_name, feature in example.features.feature.items():\n",
    "            if feature.HasField('int64_list'):\n",
    "                print(f\"  {feature_name}: int64_list (length: {len(feature.int64_list.value)})\")\n",
    "                if len(feature.int64_list.value) <= 10:\n",
    "                    print(f\"    Values: {list(feature.int64_list.value)}\")\n",
    "                else:\n",
    "                    print(f\"    First 10 values: {list(feature.int64_list.value[:10])}\")\n",
    "            elif feature.HasField('float_list'):\n",
    "                print(f\"  {feature_name}: float_list (length: {len(feature.float_list.value)})\")\n",
    "            elif feature.HasField('bytes_list'):\n",
    "                print(f\"  {feature_name}: bytes_list (length: {len(feature.bytes_list.value)})\")\n",
    "        break\n",
    "\n",
    "# Inspect your TFRecord files\n",
    "train_files = [os.path.join(tfrecord_dir, f) for f in os.listdir(tfrecord_dir) if f.startswith('train_') and f.endswith('.tfrecord')]\n",
    "if train_files:\n",
    "    inspect_tfrecord_schema(train_files[0])\n",
    "\n",
    "# TODO reconstruct a input pep, and target using ohe to seq functions\n",
    "def onehot_to_seq(ohe_mat, alphabet=AA, pad_char='-', stop_at_pad=True):\n",
    "    \"\"\"\n",
    "    Decode a 21-dim one-hot matrix back to a string.\n",
    "    - Rows with all zeros are treated as padding.\n",
    "    - Gap column ('-') is ignored (assumes it is zeroed as in seq_to_onehot).\n",
    "    \"\"\"\n",
    "    seq_chars = []\n",
    "    for row in ohe_mat:\n",
    "        if np.all(row == 0.0):\n",
    "            if stop_at_pad:\n",
    "                break\n",
    "            else:\n",
    "                seq_chars.append(pad_char)\n",
    "                continue\n",
    "        idx = int(np.argmax(row))\n",
    "        seq_chars.append(alphabet[idx])\n",
    "    return \"\".join(seq_chars)\n",
    "\n",
    "def indices_to_seq(indices, alphabet, pad_index=None, pad_char='-', stop_at_pad=True):\n",
    "    \"\"\"\n",
    "    Decode an index vector back to a string.\n",
    "    If pad_index is provided and encountered, stops or inserts pad_char.\n",
    "    \"\"\"\n",
    "    seq_chars = []\n",
    "    for idx in indices:\n",
    "        idx = int(idx)\n",
    "        if pad_index is not None and idx == int(pad_index):\n",
    "            if stop_at_pad:\n",
    "                break\n",
    "            else:\n",
    "                seq_chars.append(pad_char)\n",
    "                continue\n",
    "        if 0 <= idx < len(alphabet):\n",
    "            seq_chars.append(alphabet[idx])\n",
    "        else:\n",
    "            # Out-of-range index: treat as unknown\n",
    "            seq_chars.append('X')\n",
    "    return \"\".join(seq_chars)\n",
    "\n",
    "def reconstruct_from_tfrecord(tfrecord_path):\n",
    "    feature_description = {\n",
    "        'pep_indices': tf.io.FixedLenFeature([None], tf.int64),         # will infer length from example\n",
    "        'pep_ohe_indices': tf.io.FixedLenFeature([None], tf.int64),\n",
    "        'mhc_ohe_indices': tf.io.FixedLenFeature([None], tf.int64),\n",
    "        'embedding_id': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    # Read first raw example\n",
    "    raw = next(iter(tf.data.TFRecordDataset(tfrecord_path, compression_type=\"GZIP\").take(1)))\n",
    "    ex = tf.train.Example()\n",
    "    ex.ParseFromString(raw.numpy())\n",
    "\n",
    "    pep_indices = list(ex.features.feature['pep_indices'].int64_list.value)\n",
    "    pep_ohe_indices = list(ex.features.feature['pep_ohe_indices'].int64_list.value)\n",
    "    mhc_ohe_indices = list(ex.features.feature['mhc_ohe_indices'].int64_list.value)\n",
    "\n",
    "    # Decode peptide from ohe indices and from 23-index space\n",
    "    pep_seq_from_ohe_idx = indices_to_seq(pep_ohe_indices, alphabet=AA, pad_index=PAD_INDEX_OHE, pad_char='-')\n",
    "    pep_seq_from_23_idx = indices_to_seq(pep_indices, alphabet=AMINO_ACID_VOCAB, pad_index=PAD_INDEX_62, pad_char='-')\n",
    "\n",
    "    # Also demonstrate roundtrip via one-hot matrix\n",
    "    pep_ohe_mat = seq_to_onehot(pep_seq_from_ohe_idx, max_seq_len=len(pep_ohe_indices))\n",
    "    pep_seq_from_ohe_mat = onehot_to_seq(pep_ohe_mat, alphabet=AA, pad_char='-')\n",
    "\n",
    "    # Decode MHC sequence from ohe indices\n",
    "    mhc_seq_from_ohe_idx = indices_to_seq(mhc_ohe_indices, alphabet=AA, pad_index=PAD_INDEX_OHE, pad_char='-')\n",
    "    mhc_ohe_mat = seq_to_onehot(mhc_seq_from_ohe_idx, max_seq_len=len(mhc_ohe_indices))\n",
    "    mhc_seq_from_ohe_mat = onehot_to_seq(mhc_ohe_mat, alphabet=AA, pad_char='-')\n",
    "\n",
    "    print(\"Reconstructed sequences from first TFRecord example:\")\n",
    "    print(f\"  pep_seq_from_ohe_indices (AA): {pep_seq_from_ohe_idx}\")\n",
    "    print(f\"  pep_seq_from_23_indices (BLOSUM alphabet): {pep_seq_from_23_idx}\")\n",
    "    print(f\"  pep_seq_roundtrip_ohe_mat: {pep_seq_from_ohe_mat}\")\n",
    "    print(f\"  mhc_seq_from_ohe_indices (AA): {mhc_seq_from_ohe_idx}\")\n",
    "    print(f\"  mhc_seq_roundtrip_ohe_mat: {mhc_seq_from_ohe_mat}\")\n",
    "\n",
    "# Call reconstruction on the first train file (if present)\n",
    "if train_files:\n",
    "    reconstruct_from_tfrecord(train_files[0])\n"
   ],
   "id": "aee832c197199ada"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PlateauBinaryCrossentropy(tf.keras.losses.Loss):\n",
    "    def __init__(self, threshold=0.05,\n",
    "                 reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n",
    "                 name=\"plateau_binary_crossentropy\"):\n",
    "        super().__init__(reduction=reduction, name=name)\n",
    "        self.threshold = float(threshold)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        t = tf.clip_by_value(tf.cast(self.threshold, y_pred.dtype), epsilon, 1.0 - epsilon)\n",
    "        # Clamp inside the logs so the loss flattens beyond the threshold\n",
    "        p_eff = tf.where(y_true > 0.5, tf.minimum(y_pred, t), tf.maximum(y_pred, 1.0 - t))\n",
    "        # Base BCE on the clamped probability\n",
    "        base = -(y_true * tf.math.log(p_eff) + (1.0 - y_true) * tf.math.log(1.0 - p_eff))\n",
    "        # Subtract the plateau constant so the flat region is at zero\n",
    "        loss = base + tf.math.log(t)\n",
    "        # Return per-example losses; Keras will apply the configured reduction\n",
    "        return loss\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"threshold\": self.threshold})\n",
    "        return config\n"
   ],
   "id": "9031612ff0c0cf41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_optimized_training_setup():\n",
    "    \"\"\"Create optimized model and training components\"\"\"\n",
    "    model = pmbind_multitask_modified(\n",
    "        max_pep_len=pep_len,\n",
    "        max_mhc_len=mhc_len,\n",
    "        emb_dim=32,\n",
    "        heads=2,\n",
    "        noise_std=0.1,\n",
    "        drop_out_rate=0.15,\n",
    "        l2_reg=0.03,\n",
    "        ESM_dim=ESM_DIM,\n",
    "    )\n",
    "\n",
    "    decay_steps = (metadata['train_samples'] // batch_size) * 2\n",
    "    lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "        initial_learning_rate=learning_rate,\n",
    "        first_decay_steps=decay_steps,\n",
    "        alpha=0.1,\n",
    "        t_mul=1.2,\n",
    "        m_mul=0.9\n",
    "    )\n",
    "    base_optimizer = keras.optimizers.Lion(learning_rate=lr_schedule)\n",
    "    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(base_optimizer) if mixed_precision else base_optimizer\n",
    "    binary_loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "    return model, optimizer, binary_loss_fn"
   ],
   "id": "40ce94906afda0b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_step(batch_data, model, optimizer, binary_loss_fn, metrics):\n",
    "    \"\"\"Standard training step\"\"\"\n",
    "\n",
    "    x_batch_list = [batch_data['pep_blossom62'], batch_data['pep_mask'],\n",
    "                    batch_data['mhc_emb'], batch_data['mhc_mask'],\n",
    "                    batch_data['pep_ohe_target'], batch_data['mhc_ohe_target']]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x_batch_list, training=True)\n",
    "\n",
    "        # Use ANL_CE loss for reconstruction tasks\n",
    "        pep_loss = masked_categorical_crossentropy(predictions['pep_ytrue_ypred'], batch_data['pep_mask'])\n",
    "        mhc_loss = masked_categorical_crossentropy(predictions['mhc_ytrue_ypred'], batch_data['mhc_mask'])\n",
    "        class_loss = binary_loss_fn(batch_data['labels'], predictions['cls_ypred'])\n",
    "\n",
    "        # OPTIMIZED LOSS WEIGHTS: Focus more on classification, less on reconstruction\n",
    "        total_loss = 1.0 * pep_loss + 1.0 * mhc_loss + 2.0 * class_loss\n",
    "\n",
    "        # Use proper LossScaleOptimizer methods for mixed precision\n",
    "        if mixed_precision:\n",
    "            scaled_loss = optimizer.scale_loss(total_loss)\n",
    "            grads = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "            # Gradient unscaling is handled automatically by apply_gradients\n",
    "        else:\n",
    "            grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "\n",
    "        grads, _ = tf.clip_by_global_norm(grads, 1.0)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Update metrics\n",
    "    metrics['loss'].update_state(total_loss)\n",
    "    metrics['pep_recon_loss'].update_state(pep_loss)\n",
    "    metrics['mhc_recon_loss'].update_state(mhc_loss)\n",
    "    metrics['class_loss'].update_state(class_loss)\n",
    "    metrics['auc'].update_state(batch_data['labels'], predictions['cls_ypred'])\n",
    "    metrics['acc'].update_state(batch_data['labels'], predictions['cls_ypred'])\n",
    "    metrics['mcc'].update_state(batch_data['labels'], predictions['cls_ypred'])\n",
    "\n",
    "    return total_loss, pep_loss, mhc_loss, class_loss, metrics\n",
    "\n",
    "\n",
    "def val_step_optimized(batch_data, model, binary_loss_fn, metrics):\n",
    "    \"\"\"Validation step with ANL_CE loss\"\"\"\n",
    "    x_batch_list = [batch_data['pep_blossom62'], batch_data['pep_mask'],\n",
    "                    batch_data['mhc_emb'], batch_data['mhc_mask'],\n",
    "                    batch_data['pep_ohe_target'], batch_data['mhc_ohe_target']]\n",
    "\n",
    "    predictions = model(x_batch_list, training=False)\n",
    "\n",
    "    pep_loss = masked_categorical_crossentropy(predictions['pep_ytrue_ypred'], batch_data['pep_mask'])\n",
    "    mhc_loss = masked_categorical_crossentropy(predictions['mhc_ytrue_ypred'], batch_data['mhc_mask'])\n",
    "    class_loss = binary_loss_fn(batch_data['labels'], predictions['cls_ypred'])\n",
    "\n",
    "    total_loss = 1.0 * pep_loss + 1.0 * mhc_loss + 2.0 * class_loss\n",
    "\n",
    "    # Update metrics\n",
    "    metrics['loss'].update_state(total_loss)\n",
    "    metrics['pep_recon_loss'].update_state(pep_loss)\n",
    "    metrics['mhc_recon_loss'].update_state(mhc_loss)\n",
    "    metrics['class_loss'].update_state(class_loss)\n",
    "    metrics['auc'].update_state(batch_data['labels'], predictions['cls_ypred'])\n",
    "    metrics['acc'].update_state(batch_data['labels'], predictions['cls_ypred'])\n",
    "    metrics['mcc'].update_state(batch_data['labels'], predictions['cls_ypred'])\n",
    "\n",
    "    return total_loss, pep_loss, mhc_loss, class_loss, predictions"
   ],
   "id": "affe45055dbad2b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# MODEL SETUP AND TRAINING WITH DATAGENERATOR\n",
    "# ============================================================================\n",
    "epochs = 15\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "patience = 5\n",
    "\n",
    "# # Create data generators\n",
    "\n",
    "# Load metadata and setup\n",
    "metadata = load_metadata(tfrecord_dir)\n",
    "pep_len = metadata['MAX_PEP_LEN']\n",
    "mhc_len = metadata['MAX_MHC_LEN']\n",
    "ESM_DIM = metadata['ESM_DIM']\n",
    "\n",
    "print(f\"Dataset metadata:\")\n",
    "print(f\"Max peptide length: {pep_len}\")\n",
    "print(f\"Max MHC length: {mhc_len}\")\n",
    "print(f\"ESM embedding dimension: {ESM_DIM}\")\n",
    "print(f\"Training samples: {metadata['train_samples']:,}\")\n",
    "\n",
    "# Load MHC embedding table\n",
    "def normalize_embedding_tf(emb, method=\"clip_norm1000\"):\n",
    "    \"\"\"TensorFlow implementation of the normalization logic.\"\"\"\n",
    "    if method == \"clip_norm1000\":\n",
    "        emb_norm = tf.clip_by_value(emb, -1000.0, 1000.0)\n",
    "        return 20.0 * (emb_norm - (-1000.0)) / (1000.0 - (-1000.0)) - 10.0\n",
    "    # Add other methods like min_max_norm if needed\n",
    "    else:\n",
    "        return emb  # No normalization\n",
    "\n",
    "# Load MHC embedding table\n",
    "lookup_path = os.path.join(tfrecord_dir, \"train_mhc_embedding_lookup.npz\")\n",
    "MHC_EMBEDDING_TABLE_RAW = load_embedding_table(lookup_path, metadata)\n",
    "print(f\"Loaded raw MHC embedding table: {MHC_EMBEDDING_TABLE_RAW.shape}\")\n",
    "\n",
    "# Normalize the entire lookup table before starting the training.\n",
    "print(\"Normalizing the MHC embedding lookup table...\")\n",
    "MHC_EMBEDDING_TABLE = normalize_embedding_tf(MHC_EMBEDDING_TABLE_RAW, method=\"clip_norm1000\")\n",
    "print(f\"Normalized MHC embedding table created.\")\n",
    "print(f\"Min value in normalized table: {tf.reduce_min(MHC_EMBEDDING_TABLE):.2f}\")\n",
    "print(f\"Max value in normalized table: {tf.reduce_max(MHC_EMBEDDING_TABLE):.2f}\")\n",
    "\n",
    "# Create datasets\n",
    "train_files = [os.path.join(tfrecord_dir, f) for f in os.listdir(tfrecord_dir) if f.startswith('train_') and f.endswith('.tfrecord')]\n",
    "\n",
    "print(\"\\nCreating datasets...\")\n",
    "print(f\"Training files: {train_files}\")\n",
    "\n",
    "train_dataset_tf = create_dataset_from_files(train_files, batch_size, shuffle=True, apply_masking=True)\n",
    "\n",
    "train_generator_infer = OptimizedDataGenerator(\n",
    "    train_df,\n",
    "    seq_map=seq_map,\n",
    "    embed_map=embed_map,\n",
    "    max_pep_len=pep_len,\n",
    "    max_mhc_len=mhc_len,\n",
    "    batch_size=batch_size,\n",
    "    apply_masking=True  # No masking for validation\n",
    ")\n",
    "\n",
    "val_generator = OptimizedDataGenerator(\n",
    "    df=val_df,  # Sample for demo\n",
    "    seq_map=seq_map,\n",
    "    embed_map=embed_map,\n",
    "    max_pep_len=pep_len,\n",
    "    max_mhc_len=mhc_len,\n",
    "    batch_size=batch_size,\n",
    "    apply_masking=False  # No masking for validation\n",
    ")\n",
    "\n",
    "model, optimizer, binary_loss_fn, = create_optimized_training_setup()"
   ],
   "id": "2b9cc0b5a26e27cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# TODO fix better use tfrecord\n",
    "\n",
    "# # print features of first sample\n",
    "# print(\"\\nSample preprocessed training data (first row):\")\n",
    "# sample = train_df.iloc[0]\n",
    "# for col in ['long_mer', 'allele', '_cleaned_key', '_emb_key', '_mhc_seq', 'assigned_label']:\n",
    "#     print(f\"{col}: {sample[col]}\")\n",
    "#\n",
    "# # generate first sample and print features\n",
    "# gen = OptimizedDataGenerator(\n",
    "#     train_df, seq_map, embed_map, PEP_LEN, MHC_LEN,\n",
    "#     batch_size=1, apply_masking=True\n",
    "# )\n",
    "# sample_batch = gen[0]\n",
    "# print(\"\\nGenerated sample batch features:\")\n",
    "# for key, value in sample_batch.items():\n",
    "#     print(f\"{key}: {value}\")\n",
    "#\n",
    "# # plot all features of first sample in batch\n",
    "# import matplotlib.pyplot as plt\n",
    "#\n",
    "def plot_sample_features(sample_batch):\n",
    "    \"\"\"Plot features of a sample batch with improved visualizations.\"\"\"\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(15, 16))  # Changed from 3x2 to 4x2\n",
    "    # Define better colormaps\n",
    "    feature_cmap = 'RdYlBu_r'  # Red-Yellow-Blue for feature values\n",
    "    mask_cmap = 'RdGy'  # Red-Gray for masks\n",
    "    onehot_cmap = 'Blues'  # Blues for one-hot encodings\n",
    "\n",
    "    # Peptide Blossom62 (transposed)\n",
    "    pep_blossom = sample_batch['pep_blossom62'][0].numpy().T  # Transpose: (features, positions)\n",
    "    im1 = axes[0, 0].imshow(pep_blossom, aspect='auto', cmap=feature_cmap, interpolation='nearest')\n",
    "    axes[0, 0].set_title('Peptide Blossum62 Encoding', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Sequence Position')\n",
    "    axes[0, 0].set_ylabel('Feature Dimension')\n",
    "    plt.colorbar(im1, ax=axes[0, 0], shrink=0.8)\n",
    "\n",
    "    # Peptide Mask (as bar plot)\n",
    "    pep_mask = sample_batch['pep_mask'][0].numpy()\n",
    "    axes[0, 1].bar(range(len(pep_mask)), pep_mask, color=['red' if x < 0 else 'green' if x > 0 else 'gray' for x in pep_mask])\n",
    "    axes[0, 1].set_title('Peptide Mask', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Sequence Position')\n",
    "    axes[0, 1].set_ylabel('Mask Value')\n",
    "    axes[0, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # MHC Embedding (transposed, showing first 50 dimensions for clarity)\n",
    "    mhc_emb_full = sample_batch['mhc_emb'][0].numpy()\n",
    "    mhc_emb = mhc_emb_full.T[:50, :]  # Show first 50 dims, transposed\n",
    "    im2 = axes[1, 0].imshow(mhc_emb, aspect='auto', cmap=feature_cmap, interpolation='nearest')\n",
    "    axes[1, 0].set_title('MHC Embedding (First 50 Dims)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Sequence Position')\n",
    "    axes[1, 0].set_ylabel('Feature Dimension')\n",
    "    plt.colorbar(im2, ax=axes[1, 0], shrink=0.8)\n",
    "\n",
    "    # MHC Mask (as bar plot)\n",
    "    mhc_mask = sample_batch['mhc_mask'][0].numpy()\n",
    "    axes[1, 1].bar(range(len(mhc_mask)), mhc_mask, color=['red' if x < 0 else 'green' if x > 0 else 'gray' for x in mhc_mask])\n",
    "    axes[1, 1].set_title('MHC Mask', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Sequence Position')\n",
    "    axes[1, 1].set_ylabel('Mask Value')\n",
    "    axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Peptide One-Hot Target (transposed)\n",
    "    pep_ohe = sample_batch['pep_ohe_target'][0].numpy().T  # Transpose: (amino_acids, positions)\n",
    "    im3 = axes[2, 0].imshow(pep_ohe, aspect='auto', cmap=onehot_cmap, interpolation='nearest')\n",
    "    axes[2, 0].set_title('Peptide One-Hot Target', fontsize=12, fontweight='bold')\n",
    "    axes[2, 0].set_xlabel('Sequence Position')\n",
    "    axes[2, 0].set_ylabel('Amino Acid Index')\n",
    "    plt.colorbar(im3, ax=axes[2, 0], shrink=0.8)\n",
    "\n",
    "    # MHC One-Hot Target (transposed)\n",
    "    mhc_ohe = sample_batch['mhc_ohe_target'][0].numpy().T  # Transpose: (amino_acids, positions)\n",
    "    im4 = axes[2, 1].imshow(mhc_ohe, aspect='auto', cmap=onehot_cmap, interpolation='nearest')\n",
    "    axes[2, 1].set_title('MHC One-Hot Target', fontsize=12, fontweight='bold')\n",
    "    axes[2, 1].set_xlabel('Sequence Position')\n",
    "    axes[2, 1].set_ylabel('Amino Acid Index')\n",
    "    plt.colorbar(im4, ax=axes[2, 1], shrink=0.8)\n",
    "\n",
    "    # NEW: Distribution plots (excluding zeros)\n",
    "    # MHC Embedding distribution\n",
    "    mhc_emb_nonzero = mhc_emb_full[mhc_emb_full != 0]\n",
    "    mhc_emb_nonzero = np.clip(mhc_emb_nonzero, -1000, 1000)\n",
    "    axes[3, 0].hist(mhc_emb_nonzero.flatten(), bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "    axes[3, 0].set_title('MHC Embedding Value Distribution (non-zero)', fontsize=12, fontweight='bold')\n",
    "    axes[3, 0].set_xlabel('Embedding Values')\n",
    "    axes[3, 0].set_ylabel('Frequency')\n",
    "    axes[3, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Peptide Blossom62 distribution\n",
    "    pep_blossom_nonzero = pep_blossom[pep_blossom != 0]\n",
    "    axes[3, 1].hist(pep_blossom_nonzero.flatten(), bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "    axes[3, 1].set_title('Peptide Blossum62 Value Distribution (non-zero)', fontsize=12, fontweight='bold')\n",
    "    axes[3, 1].set_xlabel('Blossum62 Values')\n",
    "    axes[3, 1].set_ylabel('Frequency')\n",
    "    axes[3, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Add grid for better readability (excluding histograms which already have grid)\n",
    "    for i, ax in enumerate(axes.flat[:-2]):  # Skip the last two histogram plots\n",
    "        if hasattr(ax, 'grid'):\n",
    "            ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print some statistics\n",
    "    print(f\"\\nSample Statistics:\")\n",
    "    print(f\"Label: {sample_batch['labels'][0].numpy()[0]:.3f}\")\n",
    "    print(f\"Peptide length (non-padded): {np.sum(sample_batch['pep_mask'][0].numpy() > 0)}\")\n",
    "    print(f\"MHC length (non-padded): {np.sum(sample_batch['mhc_mask'][0].numpy() > 0)}\")\n",
    "    print(f\"Peptide Blossum62 range: [{pep_blossom.min():.3f}, {pep_blossom.max():.3f}]\")\n",
    "    print(f\"MHC embedding range: [{mhc_emb_full.min():.3f}, {mhc_emb_full.max():.3f}]\")\n",
    "\n",
    "    # TODO print the number of emb features that are below -500 and above 500\n",
    "    print(f\"MHC embedding features below -500: {np.sum(mhc_emb_full < -500)}\")\n",
    "    print(f\"MHC embedding features above 500: {np.sum(mhc_emb_full > 500)}\")\n",
    "\n",
    "    # Print the samples with embedding features below -1000\n",
    "    print(\"\\n--- MHC Embedding Features Below -1000 ---\")\n",
    "    below_indices = np.argwhere(mhc_emb_full < -1000)\n",
    "    if len(below_indices) > 0:\n",
    "        for pos, feat in below_indices:\n",
    "            value = mhc_emb_full[pos, feat]\n",
    "            print(f\"Position {pos}, Feature {feat}: {value:.3f}\")\n",
    "    else:\n",
    "        print(\"None found.\")\n",
    "#\n",
    "#\n",
    "#\n",
    "# plot_sample_features(sample_batch)\n",
    "# ============================================================================\n",
    "# DEBUGGING: TFRecord Pipeline vs. DataGenerator Pipeline\n",
    "# ============================================================================\n",
    "# The goal is to isolate why the TFRecord training behaves differently.\n",
    "# We will fetch one batch from each pipeline and visualize them side-by-side.\n",
    "print(\"Attempting to fetch batch from DataGenerator...\")\n",
    "datagen_batch = train_generator_infer[1]\n",
    "print(\"Successfully fetched batch.\")\n",
    "\n",
    "print(\"\\n--- Starting Pipeline Debugging ---\")\n",
    "\n",
    "# 1. Get one batch from the TFRecord-based dataset\n",
    "# This represents what the model ACTUALLY sees during TFRecord training\n",
    "try:\n",
    "    tfrecord_batch = next(iter(train_dataset_tf))\n",
    "    print(\"Successfully fetched one batch from the TFRecord dataset.\")\n",
    "except tf.errors.OutOfRangeError:\n",
    "    print(\"Error: The TFRecord dataset is empty. Cannot fetch a batch.\")\n",
    "    tfrecord_batch = None\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while fetching from the TFRecord dataset: {e}\")\n",
    "    tfrecord_batch = None\n",
    "\n",
    "# 2. Get one batch from the DataGenerator\n",
    "# This is our \"ground truth\" reference for what the data SHOULD look like\n",
    "try:\n",
    "    # We use train_generator_infer because it has masking enabled, just like the TFRecord pipeline.\n",
    "    datagen_batch = train_generator_infer[0]\n",
    "    print(\"Successfully fetched one batch from the DataGenerator.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while fetching from the DataGenerator: {e}\")\n",
    "    datagen_batch = None\n",
    "\n",
    "# 3. Visualize and compare the two batches\n",
    "print(\"\\n\\n--- Visualization of Batch from TFRecord Pipeline ---\")\n",
    "print(\"Inspect these plots and stats carefully. This is what your model gets from the TFRecord files.\")\n",
    "plot_sample_features(tfrecord_batch)\n",
    "\n",
    "print(\"\\n\\n--- Visualization of Batch from DataGenerator Pipeline ---\")\n",
    "print(\"This is the reference batch. Compare it to the TFRecord plots above.\")\n",
    "plot_sample_features(datagen_batch)\n",
    "\n",
    "\n"
   ],
   "id": "487b327f8f914d5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================================\n",
    "train_steps = metadata['train_samples'] // batch_size\n",
    "val_steps = len(val_df)//batch_size\n",
    "\n",
    "print(f\"\\nCreated TFRecord datasets:\")\n",
    "print(f\"Training steps per epoch: {train_steps}\")\n",
    "print(f\"Validation steps per epoch: {val_steps}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "\n",
    "# Metrics for tracking\n",
    "train_metrics = {\n",
    "    'auc': tf.keras.metrics.AUC(), 'acc': tf.metrics.BinaryAccuracy(), 'mcc': BinaryMCC(),\n",
    "    'loss': tf.keras.metrics.Mean(name='loss'), 'pep_recon_loss': tf.keras.metrics.Mean(name='pep_recon_loss'),\n",
    "    'mhc_recon_loss': tf.keras.metrics.Mean(name='mhc_recon_loss'), 'class_loss': tf.keras.metrics.Mean(name='class_loss'),\n",
    "}\n",
    "val_metrics = {\n",
    "    'auc': tf.keras.metrics.AUC(), 'acc': tf.metrics.BinaryAccuracy(), 'mcc': BinaryMCC(),\n",
    "    'loss': tf.keras.metrics.Mean(name='loss'), 'pep_recon_loss': tf.keras.metrics.Mean(name='pep_recon_loss'),\n",
    "    'mhc_recon_loss': tf.keras.metrics.Mean(name='mhc_recon_loss'), 'class_loss': tf.keras.metrics.Mean(name='class_loss'),\n",
    "}\n",
    "\n",
    "print(\"\\nModel Summary:\")\n",
    "model.summary()\n",
    "best_val_auc = -np.inf\n",
    "patience_counter = 0\n",
    "model_save_path = \"best_model_tf_300k_seed1.keras\"\n",
    "\n",
    "history = {f\"{key}\": [] for key in train_metrics.keys()}\n",
    "history.update({f\"val_{key}\": [] for key in val_metrics.keys()})\n",
    "\n",
    "print(\"\\nStarting training with TFRecord pipeline and dynamic masking...\")\n",
    "for epoch in range(epochs):\n",
    "    for metric in train_metrics.values(): metric.reset_state()\n",
    "    for metric in val_metrics.values(): metric.reset_state()\n",
    "\n",
    "    # Training\n",
    "    pbar = tqdm(train_dataset_tf, total=train_steps, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    for batch_data in pbar:\n",
    "        total_loss, pep_loss, mhc_loss, class_loss, metrics = train_step(\n",
    "            batch_data, model, optimizer, binary_loss_fn, train_metrics\n",
    "        )\n",
    "        pbar.set_postfix({\n",
    "            \"Loss\": f\"{train_metrics['loss'].result():.4f}\", \"AUC\": f\"{train_metrics['auc'].result():.4f}\",\n",
    "            \"ACC\": f\"{train_metrics['acc'].result():.4f}\", \"MCC\": f\"{train_metrics['mcc'].result():.4f}\",\n",
    "        })\n",
    "\n",
    "    # Validation\n",
    "    pbar_val = tqdm(range(val_steps), desc=f\"Epoch {epoch + 1}/{epochs} - Val\", total=val_steps)\n",
    "    for batch_idx in pbar_val:\n",
    "        batch_data = val_generator[batch_idx]\n",
    "        val_total_loss, val_pep_loss, val_mhc_loss, val_class_loss, _ = val_step_optimized(\n",
    "            batch_data, model, binary_loss_fn, val_metrics\n",
    "        )\n",
    "        pbar_val.set_postfix({\n",
    "            \"Val_Loss\": f\"{val_metrics['loss'].result():.4f}\", \"Val_AUC\": f\"{val_metrics['auc'].result():.4f}\",\n",
    "            \"Val_ACC\": f\"{val_metrics['acc'].result():.4f}\", \"Val_MCC\": f\"{val_metrics['mcc'].result():.4f}\",\n",
    "        })\n",
    "\n",
    "    train_results = {key: value.result().numpy() for key, value in train_metrics.items()}\n",
    "    val_results = {key: value.result().numpy() for key, value in val_metrics.items()}\n",
    "    for key, value in train_results.items(): history[key].append(value)\n",
    "    for key, value in val_results.items(): history[f\"val_{key}\"].append(value)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - \"\n",
    "          f\"Loss: {train_results['loss']:.4f} - AUC: {train_results['auc']:.4f} - ACC: {train_results['acc']:.4f} - MCC: {train_results['mcc']:.4f} - \"\n",
    "          f\"Val Loss: {val_results['loss']:.4f} - Val AUC: {val_results['auc']:.4f} - Val ACC: {val_results['acc']:.4f} - Val MCC: {val_results['mcc']:.4f}\")\n",
    "\n",
    "    current_val_auc = val_results['auc']\n",
    "    if current_val_auc > best_val_auc:\n",
    "        print(f\"Validation AUC improved from {best_val_auc:.4f} to {current_val_auc:.4f}. Saving model...\")\n",
    "        best_val_auc = current_val_auc\n",
    "        patience_counter = 0\n",
    "        model.save(model_save_path)\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"Validation AUC did not improve. Patience: {patience_counter}/{patience}\")\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered. Training has been halted.\")\n",
    "        break\n",
    "print(\"Training finished!\")\n",
    "\n",
    "# Plot training history\n",
    "print(\"\\nPlotting training history...\")\n",
    "history_df = pd.DataFrame(history)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Training and Validation Metrics', fontsize=20)\n",
    "\n",
    "# Plot Total Loss\n",
    "axes[0, 0].plot(history_df.index, history_df['loss'], label='Train Loss')\n",
    "axes[0, 0].plot(history_df.index, history_df['val_loss'], label='Validation Loss')\n",
    "axes[0, 0].set_title('Total Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Plot AUC\n",
    "axes[0, 1].plot(history_df.index, history_df['auc'], label='Train AUC')\n",
    "axes[0, 1].plot(history_df.index, history_df['val_auc'], label='Validation AUC')\n",
    "axes[0, 1].set_title('AUC')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('AUC')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Plot Reconstruction Losses\n",
    "axes[1, 0].plot(history_df.index, history_df['pep_recon_loss'], label='Train Peptide Recon Loss')\n",
    "axes[1, 0].plot(history_df.index, history_df['val_pep_recon_loss'], label='Val Peptide Recon Loss')\n",
    "axes[1, 0].plot(history_df.index, history_df['mhc_recon_loss'], label='Train MHC Recon Loss', linestyle='--')\n",
    "axes[1, 0].plot(history_df.index, history_df['val_mhc_recon_loss'], label='Val MHC Recon Loss', linestyle='--')\n",
    "axes[1, 0].set_title('Reconstruction Losses')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Plot Classification Loss\n",
    "axes[1, 1].plot(history_df.index, history_df['class_loss'], label='Train Classification Loss')\n",
    "axes[1, 1].plot(history_df.index, history_df['val_class_loss'], label='Validation Classification Loss')\n",
    "axes[1, 1].set_title('Classification Loss (Asymmetric)')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ],
   "id": "8faa515ad0fcd0d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_save_path = \"best_model_tf_300k_seed1.keras\"\n",
    "# load best model:\n",
    "print(f\"\\nLoading the best model from {model_save_path} for final evaluation.\")\n",
    "model = tf.keras.models.load_model(\n",
    "    model_save_path,\n",
    "    custom_objects={\n",
    "        'SelfAttentionWith2DMask': SelfAttentionWith2DMask,\n",
    "        'AsymmetricPenaltyBinaryCrossentropy': AsymmetricPenaltyBinaryCrossentropy,\n",
    "        'AddGaussianNoise': AddGaussianNoise,\n",
    "        'PositionalEncoding': PositionalEncoding,\n",
    "        'GlobalMeanPooling1D': GlobalMeanPooling1D,\n",
    "        'GlobalSTDPooling1D': GlobalSTDPooling1D,\n",
    "        'GlobalMaxPooling1D': GlobalMaxPooling1D,\n",
    "        'MaskedEmbedding': MaskedEmbedding,\n",
    "        'BinaryMCC': BinaryMCC\n",
    "    }\n",
    ")\n",
    "\n",
    "\n"
   ],
   "id": "37aecd5e3afe7774"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================================\n",
    "# Visualize attention weights\n",
    "print(\"\\nVisualizing attention weights...\")\n",
    "\n",
    "# Get attention weights from a validation batch\n",
    "batch_data = val_generator[0]\n",
    "x_batch_list = [batch_data['pep_blossom62'], batch_data['pep_mask'],\n",
    "                batch_data['mhc_emb'], batch_data['mhc_mask'],\n",
    "                batch_data['pep_ohe_target'], batch_data['mhc_ohe_target']]\n",
    "predictions = model(x_batch_list, training=False)\n",
    "attn_weights = predictions['attn_weights'].numpy()\n",
    "sample_labels = batch_data['labels'].numpy().flatten()\n",
    "\n",
    "# Also get the masks for analysis\n",
    "pep_mask = batch_data['pep_mask'].numpy()\n",
    "mhc_mask = batch_data['mhc_mask'].numpy()\n",
    "\n",
    "# Plot attention weights for first few samples\n",
    "num_samples_to_plot = 3\n",
    "fig, axes = plt.subplots(1, num_samples_to_plot, figsize=(20, 6))\n",
    "fig.suptitle('Attention Weight Visualizations (Head 0)', fontsize=16)\n",
    "\n",
    "for i in range(num_samples_to_plot):\n",
    "    # Take first attention head\n",
    "    attn_matrix = attn_weights[i, 0, :, :]  # Shape: (seq_len, seq_len)\n",
    "\n",
    "    # Use different scaling to see weak patterns\n",
    "    im = axes[i].imshow(attn_matrix, cmap='Blues', aspect='auto', vmin=0, vmax=0.1)\n",
    "    axes[i].set_title(f'Sample {i + 1} (Label: {int(sample_labels[i])})')\n",
    "    axes[i].set_xlabel('Key Position')\n",
    "    axes[i].set_ylabel('Query Position')\n",
    "\n",
    "    # Add colorbar\n",
    "    plt.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)\n",
    "\n",
    "    # Add grid to separate peptide and MHC regions\n",
    "    axes[i].axhline(y=pep_len - 0.5, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[i].axvline(x=pep_len - 0.5, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize latent space (simple 2D projection using PCA)\n",
    "print(\"\\nVisualizing latent space...\")\n",
    "\n",
    "# Collect latent vectors and labels\n",
    "latent_vectors = []\n",
    "true_labels = []\n",
    "\n",
    "for batch_idx in range(min(10, len(val_generator))):  # Limit for demo\n",
    "    batch_data = val_generator[batch_idx]\n",
    "    x_batch_list = [batch_data['pep_blossom62'], batch_data['pep_mask'],\n",
    "                    batch_data['mhc_emb'], batch_data['mhc_mask'],\n",
    "                    batch_data['pep_ohe_target'], batch_data['mhc_ohe_target']]\n",
    "    predictions = model(x_batch_list, training=False)\n",
    "    latent_vectors.append(predictions['latent_vector'].numpy())\n",
    "    true_labels.append(batch_data['labels'].numpy())\n",
    "\n",
    "latent_vectors = np.concatenate(latent_vectors, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0).flatten()"
   ],
   "id": "49a6b6fa3ecdb047"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(true_labels)",
   "id": "58b3a08143eff1e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Simple UMAP visualization\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# UMAP embedding\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "embedding = reducer.fit_transform(latent_vectors)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(embedding[:, 0], embedding[:, 1],\n",
    "                     c=true_labels, alpha=0.7, s=5)\n",
    "plt.colorbar(scatter, label='Binding Label')\n",
    "plt.title('UMAP Projection of Latent Space')\n",
    "plt.xlabel('UMAP 1')\n",
    "plt.ylabel('UMAP 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ],
   "id": "20f9ef0c923c6c61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def analyze_attention_patterns(attn_weights, pep_len, mhc_len, sample_idx=0, head_idx=0):\n",
    "    \"\"\"Analyze and visualize attention patterns in detail\"\"\"\n",
    "\n",
    "    attn_matrix = attn_weights[sample_idx, head_idx, :, :]\n",
    "    total_len = pep_len + mhc_len\n",
    "\n",
    "    # Extract different attention regions\n",
    "    pep_to_pep = attn_matrix[:pep_len, :pep_len]  # Upper-left\n",
    "    pep_to_mhc = attn_matrix[:pep_len, pep_len:]  # Upper-right\n",
    "    mhc_to_pep = attn_matrix[pep_len:, :pep_len]  # Lower-left\n",
    "    mhc_to_mhc = attn_matrix[pep_len:, pep_len:]  # Lower-right\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle(f'Detailed Attention Analysis - Sample {sample_idx}, Head {head_idx}', fontsize=16)\n",
    "\n",
    "    # Full attention matrix\n",
    "    im1 = axes[0, 0].imshow(attn_matrix, cmap='Blues', aspect='auto')\n",
    "    axes[0, 0].set_title('Full Attention Matrix')\n",
    "    axes[0, 0].set_xlabel('Key Position')\n",
    "    axes[0, 0].set_ylabel('Query Position')\n",
    "    axes[0, 0].axhline(y=pep_len - 0.5, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[0, 0].axvline(x=pep_len - 0.5, color='red', linestyle='--', alpha=0.7)\n",
    "    plt.colorbar(im1, ax=axes[0, 0])\n",
    "\n",
    "    # Peptide to Peptide (should be dim due to 0.1 scaling)\n",
    "    im2 = axes[0, 1].imshow(pep_to_pep, cmap='Blues', aspect='auto')\n",
    "    axes[0, 1].set_title('Peptide → Peptide\\n(Self-attention, scaled by 0.1)')\n",
    "    axes[0, 1].set_xlabel('Peptide Key Position')\n",
    "    axes[0, 1].set_ylabel('Peptide Query Position')\n",
    "    plt.colorbar(im2, ax=axes[0, 1])\n",
    "\n",
    "    # Peptide to MHC (should be strong)\n",
    "    im3 = axes[0, 2].imshow(pep_to_mhc, cmap='Blues', aspect='auto')\n",
    "    axes[0, 2].set_title('Peptide → MHC\\n(Cross-attention, full strength)')\n",
    "    axes[0, 2].set_xlabel('MHC Key Position')\n",
    "    axes[0, 2].set_ylabel('Peptide Query Position')\n",
    "    plt.colorbar(im3, ax=axes[0, 2])\n",
    "\n",
    "    # MHC to Peptide (should be strong)\n",
    "    im4 = axes[1, 0].imshow(mhc_to_pep, cmap='Blues', aspect='auto')\n",
    "    axes[1, 0].set_title('MHC → Peptide\\n(Cross-attention, full strength)')\n",
    "    axes[1, 0].set_xlabel('Peptide Key Position')\n",
    "    axes[1, 0].set_ylabel('MHC Query Position')\n",
    "    plt.colorbar(im4, ax=axes[1, 0])\n",
    "\n",
    "    # MHC to MHC (depends on self_attn_mhc parameter)\n",
    "    im5 = axes[1, 1].imshow(mhc_to_mhc, cmap='Blues', aspect='auto')\n",
    "    axes[1, 1].set_title('MHC → MHC\\n(Self-attention)')\n",
    "    axes[1, 1].set_xlabel('MHC Key Position')\n",
    "    axes[1, 1].set_ylabel('MHC Query Position')\n",
    "    plt.colorbar(im5, ax=axes[1, 1])\n",
    "\n",
    "    # Statistics plot\n",
    "    axes[1, 2].bar(['Pep→Pep', 'Pep→MHC', 'MHC→Pep', 'MHC→MHC'],\n",
    "                   [pep_to_pep.mean(), pep_to_mhc.mean(),\n",
    "                    mhc_to_pep.mean(), mhc_to_mhc.mean()])\n",
    "    axes[1, 2].set_title('Mean Attention Strength by Region')\n",
    "    axes[1, 2].set_ylabel('Mean Attention Weight')\n",
    "    axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"\\nAttention Statistics for Sample {sample_idx}, Head {head_idx}:\")\n",
    "    print(f\"Peptide → Peptide: mean={pep_to_pep.mean():.4f}, max={pep_to_pep.max():.4f}\")\n",
    "    print(f\"Peptide → MHC: mean={pep_to_mhc.mean():.4f}, max={pep_to_mhc.max():.4f}\")\n",
    "    print(f\"MHC → Peptide: mean={mhc_to_pep.mean():.4f}, max={mhc_to_pep.max():.4f}\")\n",
    "    print(f\"MHC → MHC: mean={mhc_to_mhc.mean():.4f}, max={mhc_to_mhc.max():.4f}\")\n",
    "\n",
    "    return attn_matrix\n",
    "\n",
    "\n",
    "def check_mask_effects(p_mask, m_mask, sample_idx=0):\n",
    "    \"\"\"Check how masking affects the attention patterns\"\"\"\n",
    "\n",
    "    pep_mask_sample = p_mask[sample_idx]\n",
    "    mhc_mask_sample = m_mask[sample_idx]\n",
    "\n",
    "    print(f\"\\nMask Analysis for Sample {sample_idx}:\")\n",
    "    print(f\"Peptide mask: {pep_mask_sample}\")\n",
    "    print(f\"MHC mask: {mhc_mask_sample}\")\n",
    "    print(f\"Valid peptide positions: {np.sum(pep_mask_sample > 0)}\")\n",
    "    print(f\"Valid MHC positions: {np.sum(mhc_mask_sample > 0)}\")\n",
    "    print(f\"Masked peptide positions: {np.sum(pep_mask_sample == MASK_TOKEN)}\")\n",
    "    print(f\"Padded peptide positions: {np.sum(pep_mask_sample == PAD_TOKEN)}\")\n",
    "    print(f\"Padded MHC positions: {np.sum(mhc_mask_sample == PAD_TOKEN)}\")\n",
    "\n",
    "\n",
    "# Analyze attention patterns and mask effects\n",
    "analyze_attention_patterns(attn_weights, pep_len, mhc_len, sample_idx=0, head_idx=0)\n",
    "check_mask_effects(pep_mask, mhc_mask, sample_idx=0)\n",
    "\n",
    "# Demonstrate dynamic masking effects\n",
    "print(\"\\nDemonstrating dynamic masking effects...\")\n",
    "masked_batch = train_generator_infer[0]  # Get a training batch with masking applied\n",
    "print(f\"Sample masks in training batch:\")\n",
    "print(f\"Peptide mask values: {np.unique(masked_batch['pep_mask'][0])}\")\n",
    "print(f\"MHC mask values: {np.unique(masked_batch['mhc_mask'][0])}\")\n",
    "print(\n",
    "    f\"Positions with MASK_TOKEN ({MASK_TOKEN}): Peptide={np.sum(masked_batch['pep_mask'][0] == MASK_TOKEN)}, MHC={np.sum(masked_batch['mhc_mask'][0] == MASK_TOKEN)}\")\n"
   ],
   "id": "19afd56f3c4fa58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Analyse predictions per allele:\n",
    "# Analyze predictions per allele across all validation batches\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from collections import defaultdict\n",
    "\n",
    "allele_counts = defaultdict(lambda: {'TP': 0, 'TN': 0, 'FP': 0, 'FN': 0})\n",
    "total_samples = 0\n",
    "\n",
    "for batch_idx in range(len(train_generator_infer)):\n",
    "    batch_data = val_generator[batch_idx]\n",
    "    val_x = [\n",
    "        batch_data['pep_blossom62'], batch_data['pep_mask'],\n",
    "        batch_data['mhc_emb'], batch_data['mhc_mask'],\n",
    "        batch_data['pep_ohe_target'], batch_data['mhc_ohe_target']\n",
    "    ]\n",
    "    preds = model(val_x, training=False)\n",
    "    pred_labels = (preds['cls_ypred'].numpy().flatten() >= 0.5).astype(int)\n",
    "    true_labels = batch_data['labels'].numpy().flatten()\n",
    "\n",
    "    n = int(true_labels.shape[0])\n",
    "    total_samples += n\n",
    "    batch_start_idx = batch_idx * val_generator.batch_size\n",
    "    val_df_batch = val_df.iloc[batch_start_idx:batch_start_idx + n, :].copy()\n",
    "    val_df_batch['pred_label'] = pred_labels\n",
    "    val_df_batch['true_label'] = true_labels\n",
    "\n",
    "    for allele, allele_df in val_df_batch.groupby('allele'):\n",
    "        if len(allele_df) == 0:\n",
    "            continue\n",
    "        tp = int(np.sum((allele_df['pred_label'] == 1) & (allele_df['true_label'] == 1)))\n",
    "        tn = int(np.sum((allele_df['pred_label'] == 0) & (allele_df['true_label'] == 0)))\n",
    "        fp = int(np.sum((allele_df['pred_label'] == 1) & (allele_df['true_label'] == 0)))\n",
    "        fn = int(np.sum((allele_df['pred_label'] == 0) & (allele_df['true_label'] == 1)))\n",
    "        allele_counts[allele]['TP'] += tp\n",
    "        allele_counts[allele]['TN'] += tn\n",
    "        allele_counts[allele]['FP'] += fp\n",
    "        allele_counts[allele]['FN'] += fn\n",
    "\n",
    "confusion_df = pd.DataFrame.from_dict(allele_counts, orient='index')\n",
    "confusion_df.index.name = 'Allele'\n",
    "confusion_df = confusion_df[['TP', 'TN', 'FP', 'FN']].astype(int)\n",
    "\n",
    "# Metrics\n",
    "eps = 1e-12\n",
    "denom = confusion_df[['TP', 'TN', 'FP', 'FN']].sum(axis=1).clip(lower=eps)\n",
    "confusion_df['Accuracy'] = (confusion_df['TP'] + confusion_df['TN']) / denom\n",
    "confusion_df['Precision'] = confusion_df['TP'] / (confusion_df['TP'] + confusion_df['FP']).clip(lower=eps)\n",
    "confusion_df['Recall'] = confusion_df['TP'] / (confusion_df['TP'] + confusion_df['FN']).clip(lower=eps)\n",
    "confusion_df = confusion_df.fillna(0.0).sort_index()\n",
    "\n",
    "# Plot confusion matrix heatmaps\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "sns.heatmap(confusion_df[['TP', 'TN', 'FP', 'FN']], annot=True, fmt='d', cmap='Blues', ax=ax1)\n",
    "ax1.set_title('Confusion Matrix Counts per Allele (All Validation Batches)')\n",
    "ax1.set_ylabel('Allele')\n",
    "ax1.set_xlabel('Counts')\n",
    "\n",
    "sns.heatmap(confusion_df[['Accuracy', 'Precision', 'Recall']], annot=True, fmt='.3f', cmap='RdYlGn', ax=ax2)\n",
    "ax2.set_title('Performance Metrics per Allele (All Validation Batches)')\n",
    "ax2.set_ylabel('Allele')\n",
    "ax2.set_xlabel('Metrics')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "allele_list = list(confusion_df.index)\n",
    "print(\"\\nValidation analysis (all batches):\")\n",
    "print(f\"Total samples: {total_samples}\")\n",
    "print(f\"Number of unique alleles: {len(allele_list)}\")\n",
    "print(f\"Alleles: {sorted(allele_list)}\")\n"
   ],
   "id": "5579556a174b3ca5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def per_allele_predictions(data_generator, threshold=0.5):\n",
    "    # Analyze predictions per allele across all validation batches\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.ticker as ticker\n",
    "    from collections import defaultdict\n",
    "\n",
    "    allele_counts = defaultdict(lambda: {'TP': 0, 'TN': 0, 'FP': 0, 'FN': 0})\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx in range(len(data_generator)):\n",
    "        batch_data = data_generator[batch_idx]\n",
    "        val_x = [\n",
    "            batch_data['pep_blossom62'], batch_data['pep_mask'],\n",
    "            batch_data['mhc_emb'], batch_data['mhc_mask'],\n",
    "            batch_data['pep_ohe_target'], batch_data['mhc_ohe_target']\n",
    "        ]\n",
    "        preds = model(val_x, training=False)\n",
    "        pred_labels = (preds['cls_ypred'].numpy().flatten() >= 0.7).astype(int)\n",
    "        true_labels = batch_data['labels'].numpy().flatten()\n",
    "\n",
    "        n = int(true_labels.shape[0])\n",
    "        total_samples += n\n",
    "        batch_start_idx = batch_idx * data_generator.batch_size\n",
    "        val_df_batch = val_df.iloc[batch_start_idx:batch_start_idx + n, :].copy()\n",
    "        val_df_batch['pred_label'] = pred_labels\n",
    "        val_df_batch['true_label'] = true_labels\n",
    "\n",
    "        for allele, allele_df in val_df_batch.groupby('allele'):\n",
    "            if len(allele_df) == 0:\n",
    "                continue\n",
    "            tp = int(np.sum((allele_df['pred_label'] == 1) & (allele_df['true_label'] == 1)))\n",
    "            tn = int(np.sum((allele_df['pred_label'] == 0) & (allele_df['true_label'] == 0)))\n",
    "            fp = int(np.sum((allele_df['pred_label'] == 1) & (allele_df['true_label'] == 0)))\n",
    "            fn = int(np.sum((allele_df['pred_label'] == 0) & (allele_df['true_label'] == 1)))\n",
    "            allele_counts[allele]['TP'] += tp\n",
    "            allele_counts[allele]['TN'] += tn\n",
    "            allele_counts[allele]['FP'] += fp\n",
    "            allele_counts[allele]['FN'] += fn\n",
    "\n",
    "    confusion_df = pd.DataFrame.from_dict(allele_counts, orient='index')\n",
    "    confusion_df.index.name = 'Allele'\n",
    "    confusion_df = confusion_df[['TP', 'TN', 'FP', 'FN']].astype(int)\n",
    "\n",
    "    # Metrics\n",
    "    eps = 1e-12\n",
    "    denom = confusion_df[['TP', 'TN', 'FP', 'FN']].sum(axis=1).clip(lower=eps)\n",
    "    confusion_df['Accuracy'] = (confusion_df['TP'] + confusion_df['TN']) / denom\n",
    "    confusion_df['Precision'] = confusion_df['TP'] / (confusion_df['TP'] + confusion_df['FP']).clip(lower=eps)\n",
    "    confusion_df['Recall'] = confusion_df['TP'] / (confusion_df['TP'] + confusion_df['FN']).clip(lower=eps)\n",
    "    confusion_df = confusion_df.fillna(0.0).sort_index()\n",
    "\n",
    "    # Plot confusion matrix heatmaps\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "    sns.heatmap(confusion_df[['TP', 'TN', 'FP', 'FN']], annot=True, fmt='d', cmap='Blues', ax=ax1)\n",
    "    ax1.set_title('Confusion Matrix Counts per Allele (All Validation Batches)')\n",
    "    ax1.set_ylabel('Allele')\n",
    "    ax1.set_xlabel('Counts')\n",
    "\n",
    "    sns.heatmap(confusion_df[['Accuracy', 'Precision', 'Recall']], annot=True, fmt='.3f', cmap='RdYlGn', ax=ax2)\n",
    "    ax2.set_title('Performance Metrics per Allele (All Validation Batches)')\n",
    "    ax2.set_ylabel('Allele')\n",
    "    ax2.set_xlabel('Metrics')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    allele_list = list(confusion_df.index)\n",
    "    print(\"\\nValidation analysis (all batches):\")\n",
    "    print(f\"Total samples: {total_samples}\")\n",
    "    print(f\"Number of unique alleles: {len(allele_list)}\")\n",
    "    print(f\"Alleles: {sorted(allele_list)}\")\n",
    "\n",
    "\n"
   ],
   "id": "25a69e3611be659a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "per_allele_predictions(val_generator, threshold=0.7)",
   "id": "95fbeff716cc16be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# run predictions on test.parquet\n",
    "# Load and preprocess test data\n",
    "print(f\"Loading train data from: {train_parquet_path}\")\n",
    "\n",
    "if not os.path.exists(train_parquet_path):\n",
    "    print(f\"Error: Test file not found at {train_parquet_path}\")\n",
    "    raise FileNotFoundError(f\"Train parquet file not found: {train_parquet_path}\")\n",
    "\n",
    "test_df = pd.read_parquet(train_parquet_path)\n",
    "print(f\"Train samples: {len(test_df):,}\")\n",
    "\n",
    "# Preprocess test dataframe\n",
    "test_df = preprocess_df(test_df, seq_map, embed_map)\n",
    "\n",
    "# Create test generator\n",
    "test_generator = OptimizedDataGenerator(\n",
    "    df=test_df,\n",
    "    seq_map=seq_map,\n",
    "    embed_map=embed_map,\n",
    "    max_pep_len=pep_len,\n",
    "    max_mhc_len=mhc_len,\n",
    "    batch_size=batch_size,\n",
    "    apply_masking=False  # No masking for test\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(test_generator)}\")\n",
    "\n",
    "# Run predictions on test set\n",
    "print(\"Running predictions on test set...\")\n",
    "test_preds = []\n",
    "test_true = []\n",
    "\n",
    "for batch_idx in range(len(test_generator)):\n",
    "    batch_data = test_generator[batch_idx]\n",
    "    x_batch_list = [batch_data['pep_blossom62'], batch_data['pep_mask'],\n",
    "                    batch_data['mhc_emb'], batch_data['mhc_mask'],\n",
    "                    batch_data['pep_ohe_target'], batch_data['mhc_ohe_target']]\n",
    "    predictions = model(x_batch_list, training=False)\n",
    "    test_preds.append(predictions['cls_ypred'].numpy())\n",
    "    test_true.append(batch_data['labels'].numpy())\n",
    "\n",
    "test_preds = np.concatenate(test_preds, axis=0).flatten()\n",
    "test_true = np.concatenate(test_true, axis=0).flatten()\n",
    "\n",
    "print(f\"Generated {len(test_preds)} predictions\")\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "roc_auc = roc_auc_score(test_true, test_preds)\n",
    "precision, recall, _ = precision_recall_curve(test_true, test_preds)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# Binary predictions for confusion matrix\n",
    "test_pred_binary = (test_preds >= 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(test_true, test_pred_binary)\n",
    "precision_score_val = precision_score(test_true, test_pred_binary)\n",
    "recall_score_val = recall_score(test_true, test_pred_binary)\n",
    "f1_score_val = f1_score(test_true, test_pred_binary)\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision_score_val:.4f}\")\n",
    "print(f\"Recall: {recall_score_val:.4f}\")\n",
    "print(f\"F1 Score: {f1_score_val:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_true, test_pred_binary)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"TN: {cm[0,0]}, FP: {cm[0,1]}\")\n",
    "print(f\"FN: {cm[1,0]}, TP: {cm[1,1]}\")\n",
    "\n",
    "# Comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Test Set Performance Analysis', fontsize=16)\n",
    "\n",
    "# 1. ROC Curve\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, _ = roc_curve(test_true, test_preds)\n",
    "axes[0, 0].plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "axes[0, 0].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "axes[0, 0].set_xlabel('False Positive Rate')\n",
    "axes[0, 0].set_ylabel('True Positive Rate')\n",
    "axes[0, 0].set_title('ROC Curve')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Precision-Recall Curve\n",
    "axes[0, 1].plot(recall, precision, label=f'PR Curve (AUC = {pr_auc:.3f})')\n",
    "axes[0, 1].axhline(y=np.mean(test_true), color='k', linestyle='--', label='Random')\n",
    "axes[0, 1].set_xlabel('Recall')\n",
    "axes[0, 1].set_ylabel('Precision')\n",
    "axes[0, 1].set_title('Precision-Recall Curve')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Confusion Matrix Heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 2],\n",
    "            xticklabels=['Non-Binder', 'Binder'],\n",
    "            yticklabels=['Non-Binder', 'Binder'])\n",
    "axes[0, 2].set_title('Confusion Matrix')\n",
    "axes[0, 2].set_xlabel('Predicted')\n",
    "axes[0, 2].set_ylabel('Actual')\n",
    "\n",
    "# 4. Prediction Distribution\n",
    "axes[1, 0].hist(test_preds[test_true == 0], bins=50, alpha=0.7, label='Non-Binders', color='red')\n",
    "axes[1, 0].hist(test_preds[test_true == 1], bins=50, alpha=0.7, label='Binders', color='blue')\n",
    "axes[1, 0].axvline(x=0.5, color='black', linestyle='--', label='Threshold')\n",
    "axes[1, 0].set_xlabel('Prediction Score')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].set_title('Prediction Score Distribution')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Calibration Plot\n",
    "from sklearn.calibration import calibration_curve\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(test_true, test_preds, n_bins=10)\n",
    "axes[1, 1].plot(mean_predicted_value, fraction_of_positives, marker='o', label='Model')\n",
    "axes[1, 1].plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "axes[1, 1].set_xlabel('Mean Predicted Probability')\n",
    "axes[1, 1].set_ylabel('Fraction of Positives')\n",
    "axes[1, 1].set_title('Calibration Plot')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Performance by Prediction Confidence\n",
    "# Bin predictions by confidence and show accuracy\n",
    "confidence_bins = np.linspace(0, 1, 11)\n",
    "bin_centers = (confidence_bins[:-1] + confidence_bins[1:]) / 2\n",
    "bin_accuracies = []\n",
    "bin_counts = []\n",
    "\n",
    "for i in range(len(confidence_bins) - 1):\n",
    "    mask = (test_preds >= confidence_bins[i]) & (test_preds < confidence_bins[i + 1])\n",
    "    if np.sum(mask) > 0:\n",
    "        bin_acc = accuracy_score(test_true[mask], test_pred_binary[mask])\n",
    "        bin_accuracies.append(bin_acc)\n",
    "        bin_counts.append(np.sum(mask))\n",
    "    else:\n",
    "        bin_accuracies.append(0)\n",
    "        bin_counts.append(0)\n",
    "\n",
    "bars = axes[1, 2].bar(bin_centers, bin_accuracies, width=0.08, alpha=0.7, color='green')\n",
    "axes[1, 2].set_xlabel('Prediction Score Bin')\n",
    "axes[1, 2].set_ylabel('Accuracy')\n",
    "axes[1, 2].set_title('Accuracy by Prediction Confidence')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar, count in zip(bars, bin_counts):\n",
    "    if count > 0:\n",
    "        axes[1, 2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                       f'n={count}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-allele analysis if allele information is available\n",
    "if '_emb_key' in test_df.columns:\n",
    "    print(\"\\nPer-allele performance analysis...\")\n",
    "\n",
    "    # Create a dataframe with predictions and alleles\n",
    "    test_results_df = test_df.copy()\n",
    "    test_results_df = test_results_df.iloc[:len(test_preds)].copy()\n",
    "    test_results_df['pred_score'] = test_preds\n",
    "    test_results_df['pred_label'] = test_pred_binary\n",
    "    test_results_df['true_label'] = test_true\n",
    "\n",
    "    # Calculate per-allele metrics\n",
    "    allele_metrics = []\n",
    "    for allele in test_results_df['_emb_key'].unique():\n",
    "        allele_data = test_results_df[test_results_df['_emb_key'] == allele]\n",
    "        if len(allele_data) > 5:  # Only analyze alleles with sufficient data\n",
    "            try:\n",
    "                allele_auc = roc_auc_score(allele_data['true_label'], allele_data['pred_score'])\n",
    "                allele_acc = accuracy_score(allele_data['true_label'], allele_data['pred_label'])\n",
    "                allele_metrics.append({\n",
    "                    'allele': allele,\n",
    "                    'count': len(allele_data),\n",
    "                    'auc': allele_auc,\n",
    "                    'accuracy': allele_acc,\n",
    "                    'positive_rate': allele_data['true_label'].mean()\n",
    "                })\n",
    "            except ValueError:\n",
    "                # Skip alleles with only one class\n",
    "                continue\n",
    "\n",
    "    if allele_metrics:\n",
    "        allele_metrics_df = pd.DataFrame(allele_metrics)\n",
    "        allele_metrics_df = allele_metrics_df.sort_values('auc', ascending=False)\n",
    "\n",
    "        print(f\"\\nTop 10 alleles by AUC:\")\n",
    "        print(allele_metrics_df.head(10).to_string(index=False))\n",
    "\n",
    "        # Visualize per-allele performance\n",
    "        plt.figure(figsize=(15, 8))\n",
    "\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.scatter(allele_metrics_df['count'], allele_metrics_df['auc'], alpha=0.6)\n",
    "        plt.xlabel('Number of Test Samples')\n",
    "        plt.ylabel('AUC')\n",
    "        plt.title('AUC vs Sample Count per Allele')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.scatter(allele_metrics_df['positive_rate'], allele_metrics_df['auc'], alpha=0.6)\n",
    "        plt.xlabel('Positive Rate')\n",
    "        plt.ylabel('AUC')\n",
    "        plt.title('AUC vs Positive Rate per Allele')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        top_alleles = allele_metrics_df.head(15)\n",
    "        plt.barh(range(len(top_alleles)), top_alleles['auc'])\n",
    "        plt.yticks(range(len(top_alleles)), top_alleles['allele'])\n",
    "        plt.xlabel('AUC')\n",
    "        plt.title('Top 15 Alleles by AUC')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.hist(allele_metrics_df['auc'], bins=20, alpha=0.7, edgecolor='black')\n",
    "        plt.axvline(x=allele_metrics_df['auc'].mean(), color='red', linestyle='--',\n",
    "                   label=f'Mean: {allele_metrics_df[\"auc\"].mean():.3f}')\n",
    "        plt.xlabel('AUC')\n",
    "        plt.ylabel('Number of Alleles')\n",
    "        plt.title('Distribution of Per-Allele AUC')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"\\nTrain evaluation complete!\")\n"
   ],
   "id": "8ead81d2134e12ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# run predictions on test.parquet\n",
    "# Load and preprocess test data\n",
    "test_parquet_path = os.path.join(parquet_dir, 'new_df2/test_set.parquet')\n",
    "print(f\"Loading test data from: {test_parquet_path}\")\n",
    "\n",
    "if not os.path.exists(test_parquet_path):\n",
    "    print(f\"Error: Test file not found at {test_parquet_path}\")\n",
    "    raise FileNotFoundError(f\"Test parquet file not found: {test_parquet_path}\")\n",
    "\n",
    "test_df = pd.read_parquet(test_parquet_path)\n",
    "print(f\"Test samples: {len(test_df):,}\")\n",
    "\n",
    "# Preprocess test dataframe\n",
    "test_df = preprocess_df(test_df, seq_map, embed_map)\n",
    "\n",
    "# Create test generator\n",
    "test_generator = OptimizedDataGenerator(\n",
    "    df=test_df,\n",
    "    seq_map=seq_map,\n",
    "    embed_map=embed_map,\n",
    "    max_pep_len=pep_len,\n",
    "    max_mhc_len=mhc_len,\n",
    "    batch_size=batch_size,\n",
    "    apply_masking=False  # No masking for test\n",
    ")\n",
    "\n",
    "print(f\"Test batches: {len(test_generator)}\")\n",
    "\n",
    "# Run predictions on test set\n",
    "print(\"Running predictions on test set...\")\n",
    "test_preds = []\n",
    "test_true = []\n",
    "\n",
    "for batch_idx in range(len(test_generator)):\n",
    "    batch_data = test_generator[batch_idx]\n",
    "    x_batch_list = [batch_data['pep_blossom62'], batch_data['pep_mask'],\n",
    "                    batch_data['mhc_emb'], batch_data['mhc_mask'],\n",
    "                    batch_data['pep_ohe_target'], batch_data['mhc_ohe_target']]\n",
    "    predictions = model(x_batch_list, training=False)\n",
    "    test_preds.append(predictions['cls_ypred'].numpy())\n",
    "    test_true.append(batch_data['labels'].numpy())\n",
    "\n",
    "test_preds = np.concatenate(test_preds, axis=0).flatten()\n",
    "test_true = np.concatenate(test_true, axis=0).flatten()\n",
    "\n",
    "print(f\"Generated {len(test_preds)} predictions\")\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "roc_auc = roc_auc_score(test_true, test_preds)\n",
    "precision, recall, _ = precision_recall_curve(test_true, test_preds)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# Binary predictions for confusion matrix\n",
    "test_pred_binary = (test_preds >= 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(test_true, test_pred_binary)\n",
    "precision_score_val = precision_score(test_true, test_pred_binary)\n",
    "recall_score_val = recall_score(test_true, test_pred_binary)\n",
    "f1_score_val = f1_score(test_true, test_pred_binary)\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision_score_val:.4f}\")\n",
    "print(f\"Recall: {recall_score_val:.4f}\")\n",
    "print(f\"F1 Score: {f1_score_val:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_true, test_pred_binary)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"TN: {cm[0,0]}, FP: {cm[0,1]}\")\n",
    "print(f\"FN: {cm[1,0]}, TP: {cm[1,1]}\")\n",
    "\n",
    "# Comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Test Set Performance Analysis', fontsize=16)\n",
    "\n",
    "# 1. ROC Curve\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, _ = roc_curve(test_true, test_preds)\n",
    "axes[0, 0].plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "axes[0, 0].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "axes[0, 0].set_xlabel('False Positive Rate')\n",
    "axes[0, 0].set_ylabel('True Positive Rate')\n",
    "axes[0, 0].set_title('ROC Curve')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Precision-Recall Curve\n",
    "axes[0, 1].plot(recall, precision, label=f'PR Curve (AUC = {pr_auc:.3f})')\n",
    "axes[0, 1].axhline(y=np.mean(test_true), color='k', linestyle='--', label='Random')\n",
    "axes[0, 1].set_xlabel('Recall')\n",
    "axes[0, 1].set_ylabel('Precision')\n",
    "axes[0, 1].set_title('Precision-Recall Curve')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Confusion Matrix Heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 2],\n",
    "            xticklabels=['Non-Binder', 'Binder'],\n",
    "            yticklabels=['Non-Binder', 'Binder'])\n",
    "axes[0, 2].set_title('Confusion Matrix')\n",
    "axes[0, 2].set_xlabel('Predicted')\n",
    "axes[0, 2].set_ylabel('Actual')\n",
    "\n",
    "# 4. Prediction Distribution\n",
    "axes[1, 0].hist(test_preds[test_true == 0], bins=50, alpha=0.7, label='Non-Binders', color='red')\n",
    "axes[1, 0].hist(test_preds[test_true == 1], bins=50, alpha=0.7, label='Binders', color='blue')\n",
    "axes[1, 0].axvline(x=0.5, color='black', linestyle='--', label='Threshold')\n",
    "axes[1, 0].set_xlabel('Prediction Score')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].set_title('Prediction Score Distribution')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Calibration Plot\n",
    "from sklearn.calibration import calibration_curve\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(test_true, test_preds, n_bins=10)\n",
    "axes[1, 1].plot(mean_predicted_value, fraction_of_positives, marker='o', label='Model')\n",
    "axes[1, 1].plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "axes[1, 1].set_xlabel('Mean Predicted Probability')\n",
    "axes[1, 1].set_ylabel('Fraction of Positives')\n",
    "axes[1, 1].set_title('Calibration Plot')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Performance by Prediction Confidence\n",
    "# Bin predictions by confidence and show accuracy\n",
    "confidence_bins = np.linspace(0, 1, 11)\n",
    "bin_centers = (confidence_bins[:-1] + confidence_bins[1:]) / 2\n",
    "bin_accuracies = []\n",
    "bin_counts = []\n",
    "\n",
    "for i in range(len(confidence_bins) - 1):\n",
    "    mask = (test_preds >= confidence_bins[i]) & (test_preds < confidence_bins[i + 1])\n",
    "    if np.sum(mask) > 0:\n",
    "        bin_acc = accuracy_score(test_true[mask], test_pred_binary[mask])\n",
    "        bin_accuracies.append(bin_acc)\n",
    "        bin_counts.append(np.sum(mask))\n",
    "    else:\n",
    "        bin_accuracies.append(0)\n",
    "        bin_counts.append(0)\n",
    "\n",
    "bars = axes[1, 2].bar(bin_centers, bin_accuracies, width=0.08, alpha=0.7, color='green')\n",
    "axes[1, 2].set_xlabel('Prediction Score Bin')\n",
    "axes[1, 2].set_ylabel('Accuracy')\n",
    "axes[1, 2].set_title('Accuracy by Prediction Confidence')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar, count in zip(bars, bin_counts):\n",
    "    if count > 0:\n",
    "        axes[1, 2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                       f'n={count}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-allele analysis if allele information is available\n",
    "if '_emb_key' in test_df.columns:\n",
    "    print(\"\\nPer-allele performance analysis...\")\n",
    "\n",
    "    # Create a dataframe with predictions and alleles\n",
    "    test_results_df = test_df.copy()\n",
    "    test_results_df = test_results_df.iloc[:len(test_preds)].copy()\n",
    "    test_results_df['pred_score'] = test_preds\n",
    "    test_results_df['pred_label'] = test_pred_binary\n",
    "    test_results_df['true_label'] = test_true\n",
    "\n",
    "    # Calculate per-allele metrics\n",
    "    allele_metrics = []\n",
    "    for allele in test_results_df['_emb_key'].unique():\n",
    "        allele_data = test_results_df[test_results_df['_emb_key'] == allele]\n",
    "        if len(allele_data) > 5:  # Only analyze alleles with sufficient data\n",
    "            try:\n",
    "                allele_auc = roc_auc_score(allele_data['true_label'], allele_data['pred_score'])\n",
    "                allele_acc = accuracy_score(allele_data['true_label'], allele_data['pred_label'])\n",
    "                allele_metrics.append({\n",
    "                    'allele': allele,\n",
    "                    'count': len(allele_data),\n",
    "                    'auc': allele_auc,\n",
    "                    'accuracy': allele_acc,\n",
    "                    'positive_rate': allele_data['true_label'].mean()\n",
    "                })\n",
    "            except ValueError:\n",
    "                # Skip alleles with only one class\n",
    "                continue\n",
    "\n",
    "    if allele_metrics:\n",
    "        allele_metrics_df = pd.DataFrame(allele_metrics)\n",
    "        allele_metrics_df = allele_metrics_df.sort_values('auc', ascending=False)\n",
    "\n",
    "        print(f\"\\nTop 10 alleles by AUC:\")\n",
    "        print(allele_metrics_df.head(10).to_string(index=False))\n",
    "\n",
    "        # Visualize per-allele performance\n",
    "        plt.figure(figsize=(15, 8))\n",
    "\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.scatter(allele_metrics_df['count'], allele_metrics_df['auc'], alpha=0.6)\n",
    "        plt.xlabel('Number of Test Samples')\n",
    "        plt.ylabel('AUC')\n",
    "        plt.title('AUC vs Sample Count per Allele')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.scatter(allele_metrics_df['positive_rate'], allele_metrics_df['auc'], alpha=0.6)\n",
    "        plt.xlabel('Positive Rate')\n",
    "        plt.ylabel('AUC')\n",
    "        plt.title('AUC vs Positive Rate per Allele')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        top_alleles = allele_metrics_df.head(15)\n",
    "        plt.barh(range(len(top_alleles)), top_alleles['auc'])\n",
    "        plt.yticks(range(len(top_alleles)), top_alleles['allele'])\n",
    "        plt.xlabel('AUC')\n",
    "        plt.title('Top 15 Alleles by AUC')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.hist(allele_metrics_df['auc'], bins=20, alpha=0.7, edgecolor='black')\n",
    "        plt.axvline(x=allele_metrics_df['auc'].mean(), color='red', linestyle='--',\n",
    "                   label=f'Mean: {allele_metrics_df[\"auc\"].mean():.3f}')\n",
    "        plt.xlabel('AUC')\n",
    "        plt.ylabel('Number of Alleles')\n",
    "        plt.title('Distribution of Per-Allele AUC')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"\\nTest evaluation complete!\")\n"
   ],
   "id": "c0909bd7af8f5b2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# test on Capsnet data\n",
    "# Load and test on CapsNet IEDB dataset\n",
    "capsnet_parquet_path = os.path.join(parquet_dir, 'CapsNet_IEDB_test_reduced.parquet')\n",
    "print(f\"Loading CapsNet IEDB test data from: {capsnet_parquet_path}\")\n",
    "\n",
    "if not os.path.exists(capsnet_parquet_path):\n",
    "    print(f\"Error: CapsNet test file not found at {capsnet_parquet_path}\")\n",
    "    raise FileNotFoundError(f\"CapsNet parquet file not found: {capsnet_parquet_path}\")\n",
    "\n",
    "capsnet_df = pd.read_parquet(capsnet_parquet_path)\n",
    "print(f\"CapsNet test samples: {len(capsnet_df):,}\")\n",
    "print(f\"CapsNet columns: {list(capsnet_df.columns)}\")\n",
    "\n",
    "# Preprocess CapsNet dataframe\n",
    "capsnet_df = preprocess_df(capsnet_df, seq_map, embed_map)\n",
    "\n",
    "# Create CapsNet test generator\n",
    "capsnet_generator = OptimizedDataGenerator(\n",
    "    df=capsnet_df,\n",
    "    seq_map=seq_map,\n",
    "    embed_map=embed_map,\n",
    "    max_pep_len=pep_len,\n",
    "    max_mhc_len=mhc_len,\n",
    "    batch_size=batch_size,\n",
    "    apply_masking=False  # No masking for test\n",
    ")\n",
    "\n",
    "print(f\"CapsNet test batches: {len(capsnet_generator)}\")\n",
    "\n",
    "# Run predictions on CapsNet dataset\n",
    "print(\"Running predictions on CapsNet IEDB dataset...\")\n",
    "capsnet_preds = []\n",
    "capsnet_true = []\n",
    "\n",
    "for batch_idx in range(len(capsnet_generator)):\n",
    "    batch_data = capsnet_generator[batch_idx]\n",
    "    x_batch_list = [batch_data['pep_blossom62'], batch_data['pep_mask'],\n",
    "                    batch_data['mhc_emb'], batch_data['mhc_mask'],\n",
    "                    batch_data['pep_ohe_target'], batch_data['mhc_ohe_target']]\n",
    "    predictions = model(x_batch_list, training=False)\n",
    "    capsnet_preds.append(predictions['cls_ypred'].numpy())\n",
    "    capsnet_true.append(batch_data['labels'].numpy())\n",
    "\n",
    "capsnet_preds = np.concatenate(capsnet_preds, axis=0).flatten()\n",
    "capsnet_true = np.concatenate(capsnet_true, axis=0).flatten()\n",
    "\n",
    "print(f\"Generated {len(capsnet_preds)} CapsNet predictions\")\n",
    "\n",
    "# Calculate CapsNet metrics\n",
    "capsnet_roc_auc = roc_auc_score(capsnet_true, capsnet_preds)\n",
    "capsnet_precision, capsnet_recall, _ = precision_recall_curve(capsnet_true, capsnet_preds)\n",
    "capsnet_pr_auc = auc(capsnet_recall, capsnet_precision)\n",
    "\n",
    "# Binary predictions for confusion matrix\n",
    "capsnet_pred_binary = (capsnet_preds >= 0.5).astype(int)\n",
    "\n",
    "capsnet_accuracy = accuracy_score(capsnet_true, capsnet_pred_binary)\n",
    "capsnet_precision_score_val = precision_score(capsnet_true, capsnet_pred_binary)\n",
    "capsnet_recall_score_val = recall_score(capsnet_true, capsnet_pred_binary)\n",
    "capsnet_f1_score_val = f1_score(capsnet_true, capsnet_pred_binary)\n",
    "\n",
    "print(f\"\\nCapsNet IEDB Test Performance:\")\n",
    "print(f\"ROC AUC: {capsnet_roc_auc:.4f}\")\n",
    "print(f\"PR AUC: {capsnet_pr_auc:.4f}\")\n",
    "print(f\"Accuracy: {capsnet_accuracy:.4f}\")\n",
    "print(f\"Precision: {capsnet_precision_score_val:.4f}\")\n",
    "print(f\"Recall: {capsnet_recall_score_val:.4f}\")\n",
    "print(f\"F1 Score: {capsnet_f1_score_val:.4f}\")\n",
    "\n",
    "# Confusion Matrix for CapsNet\n",
    "capsnet_cm = confusion_matrix(capsnet_true, capsnet_pred_binary)\n",
    "print(f\"\\nCapsNet Confusion Matrix:\")\n",
    "print(f\"TN: {capsnet_cm[0,0]}, FP: {capsnet_cm[0,1]}\")\n",
    "print(f\"FN: {capsnet_cm[1,0]}, TP: {capsnet_cm[1,1]}\")\n",
    "\n",
    "# Compare with original test set performance\n",
    "print(f\"\\nPerformance Comparison:\")\n",
    "print(f\"{'Metric':<15} {'Original Test':<15} {'CapsNet IEDB':<15} {'Difference':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'ROC AUC':<15} {roc_auc:<15.4f} {capsnet_roc_auc:<15.4f} {capsnet_roc_auc - roc_auc:+15.4f}\")\n",
    "print(f\"{'PR AUC':<15} {pr_auc:<15.4f} {capsnet_pr_auc:<15.4f} {capsnet_pr_auc - pr_auc:+15.4f}\")\n",
    "print(f\"{'Accuracy':<15} {accuracy:<15.4f} {capsnet_accuracy:<15.4f} {capsnet_accuracy - accuracy:+15.4f}\")\n",
    "print(f\"{'Precision':<15} {precision_score_val:<15.4f} {capsnet_precision_score_val:<15.4f} {capsnet_precision_score_val - precision_score_val:+15.4f}\")\n",
    "print(f\"{'Recall':<15} {recall_score_val:<15.4f} {capsnet_recall_score_val:<15.4f} {capsnet_recall_score_val - recall_score_val:+15.4f}\")\n",
    "print(f\"{'F1 Score':<15} {f1_score_val:<15.4f} {capsnet_f1_score_val:<15.4f} {capsnet_f1_score_val - f1_score_val:+15.4f}\")\n",
    "\n",
    "# Comprehensive CapsNet visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('CapsNet IEDB Test Performance vs Original Test Set', fontsize=16)\n",
    "\n",
    "# 1. ROC Curve Comparison\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr_orig, tpr_orig, _ = roc_curve(test_true, test_preds)\n",
    "fpr_caps, tpr_caps, _ = roc_curve(capsnet_true, capsnet_preds)\n",
    "\n",
    "axes[0, 0].plot(fpr_orig, tpr_orig, label=f'Original Test (AUC = {roc_auc:.3f})', linewidth=2)\n",
    "axes[0, 0].plot(fpr_caps, tpr_caps, label=f'CapsNet IEDB (AUC = {capsnet_roc_auc:.3f})', linewidth=2)\n",
    "axes[0, 0].plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random')\n",
    "axes[0, 0].set_xlabel('False Positive Rate')\n",
    "axes[0, 0].set_ylabel('True Positive Rate')\n",
    "axes[0, 0].set_title('ROC Curve Comparison')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Precision-Recall Curve Comparison\n",
    "axes[0, 1].plot(recall, precision, label=f'Original Test (AUC = {pr_auc:.3f})', linewidth=2)\n",
    "axes[0, 1].plot(capsnet_recall, capsnet_precision, label=f'CapsNet IEDB (AUC = {capsnet_pr_auc:.3f})', linewidth=2)\n",
    "axes[0, 1].axhline(y=np.mean(test_true), color='gray', linestyle='--', alpha=0.5, label=f'Random (Orig: {np.mean(test_true):.3f})')\n",
    "axes[0, 1].axhline(y=np.mean(capsnet_true), color='lightcoral', linestyle='--', alpha=0.5, label=f'Random (CapsNet: {np.mean(capsnet_true):.3f})')\n",
    "axes[0, 1].set_xlabel('Recall')\n",
    "axes[0, 1].set_ylabel('Precision')\n",
    "axes[0, 1].set_title('Precision-Recall Curve Comparison')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Side-by-side Confusion Matrices\n",
    "fig_cm, axes_cm = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes_cm[0],\n",
    "            xticklabels=['Non-Binder', 'Binder'],\n",
    "            yticklabels=['Non-Binder', 'Binder'])\n",
    "axes_cm[0].set_title('Original Test Set')\n",
    "axes_cm[0].set_xlabel('Predicted')\n",
    "axes_cm[0].set_ylabel('Actual')\n",
    "\n",
    "sns.heatmap(capsnet_cm, annot=True, fmt='d', cmap='Oranges', ax=axes_cm[1],\n",
    "            xticklabels=['Non-Binder', 'Binder'],\n",
    "            yticklabels=['Non-Binder', 'Binder'])\n",
    "axes_cm[1].set_title('CapsNet IEDB Test Set')\n",
    "axes_cm[1].set_xlabel('Predicted')\n",
    "axes_cm[1].set_ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Prediction Distribution Comparison\n",
    "axes[0, 2].hist(test_preds[test_true == 0], bins=30, alpha=0.5, label='Original Non-Binders', color='red', density=True)\n",
    "axes[0, 2].hist(test_preds[test_true == 1], bins=30, alpha=0.5, label='Original Binders', color='blue', density=True)\n",
    "axes[0, 2].hist(capsnet_preds[capsnet_true == 0], bins=30, alpha=0.5, label='CapsNet Non-Binders', color='orange', density=True)\n",
    "axes[0, 2].hist(capsnet_preds[capsnet_true == 1], bins=30, alpha=0.5, label='CapsNet Binders', color='green', density=True)\n",
    "axes[0, 2].axvline(x=0.5, color='black', linestyle='--', alpha=0.7, label='Threshold')\n",
    "axes[0, 2].set_xlabel('Prediction Score')\n",
    "axes[0, 2].set_ylabel('Density')\n",
    "axes[0, 2].set_title('Prediction Score Distribution Comparison')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Calibration Plot Comparison\n",
    "fraction_of_positives_orig, mean_predicted_value_orig = calibration_curve(test_true, test_preds, n_bins=10)\n",
    "fraction_of_positives_caps, mean_predicted_value_caps = calibration_curve(capsnet_true, capsnet_preds, n_bins=10)\n",
    "\n",
    "axes[1, 0].plot(mean_predicted_value_orig, fraction_of_positives_orig, marker='o', label='Original Test', linewidth=2)\n",
    "axes[1, 0].plot(mean_predicted_value_caps, fraction_of_positives_caps, marker='s', label='CapsNet IEDB', linewidth=2)\n",
    "axes[1, 0].plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect Calibration')\n",
    "axes[1, 0].set_xlabel('Mean Predicted Probability')\n",
    "axes[1, 0].set_ylabel('Fraction of Positives')\n",
    "axes[1, 0].set_title('Calibration Plot Comparison')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Performance Metrics Bar Chart\n",
    "metrics_names = ['ROC AUC', 'PR AUC', 'Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "orig_scores = [roc_auc, pr_auc, accuracy, precision_score_val, recall_score_val, f1_score_val]\n",
    "caps_scores = [capsnet_roc_auc, capsnet_pr_auc, capsnet_accuracy, capsnet_precision_score_val, capsnet_recall_score_val, capsnet_f1_score_val]\n",
    "\n",
    "x_pos = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 1].bar(x_pos - width/2, orig_scores, width, label='Original Test', alpha=0.8)\n",
    "axes[1, 1].bar(x_pos + width/2, caps_scores, width, label='CapsNet IEDB', alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Metrics')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_title('Performance Metrics Comparison')\n",
    "axes[1, 1].set_xticks(x_pos)\n",
    "axes[1, 1].set_xticklabels(metrics_names, rotation=45)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 7. Dataset Statistics Comparison\n",
    "axes[1, 2].bar(['Original Test', 'CapsNet IEDB'],\n",
    "               [len(test_true), len(capsnet_true)],\n",
    "               alpha=0.7, color=['blue', 'orange'])\n",
    "axes[1, 2].set_ylabel('Number of Samples')\n",
    "axes[1, 2].set_title('Dataset Size Comparison')\n",
    "axes[1, 2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add positive rate annotations\n",
    "for i, (dataset, pos_rate) in enumerate([('Original', np.mean(test_true)), ('CapsNet', np.mean(capsnet_true))]):\n",
    "    axes[1, 2].text(i, [len(test_true), len(capsnet_true)][i] * 0.5,\n",
    "                   f'Pos Rate:\\n{pos_rate:.3f}',\n",
    "                   ha='center', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-allele analysis for CapsNet if available\n",
    "if '_emb_key' in capsnet_df.columns:\n",
    "    print(\"\\nCapsNet per-allele performance analysis...\")\n",
    "\n",
    "    # Create a dataframe with CapsNet predictions and alleles\n",
    "    capsnet_results_df = capsnet_df.copy()\n",
    "    capsnet_results_df = capsnet_results_df.iloc[:len(capsnet_preds)].copy()\n",
    "    capsnet_results_df['pred_score'] = capsnet_preds\n",
    "    capsnet_results_df['pred_label'] = capsnet_pred_binary\n",
    "    capsnet_results_df['true_label'] = capsnet_true\n",
    "\n",
    "    # Calculate per-allele metrics for CapsNet\n",
    "    capsnet_allele_metrics = []\n",
    "    for allele in capsnet_results_df['_emb_key'].unique():\n",
    "        allele_data = capsnet_results_df[capsnet_results_df['_emb_key'] == allele]\n",
    "        if len(allele_data) > 5:  # Only analyze alleles with sufficient data\n",
    "            try:\n",
    "                allele_auc = roc_auc_score(allele_data['true_label'], allele_data['pred_score'])\n",
    "                allele_acc = accuracy_score(allele_data['true_label'], allele_data['pred_label'])\n",
    "                capsnet_allele_metrics.append({\n",
    "                    'allele': allele,\n",
    "                    'count': len(allele_data),\n",
    "                    'auc': allele_auc,\n",
    "                    'accuracy': allele_acc,\n",
    "                    'positive_rate': allele_data['true_label'].mean()\n",
    "                })\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    if capsnet_allele_metrics:\n",
    "        capsnet_allele_metrics_df = pd.DataFrame(capsnet_allele_metrics)\n",
    "        capsnet_allele_metrics_df = capsnet_allele_metrics_df.sort_values('auc', ascending=False)\n",
    "\n",
    "        print(f\"\\nCapsNet - Top 10 alleles by AUC:\")\n",
    "        print(capsnet_allele_metrics_df.head(10).to_string(index=False))\n",
    "\n",
    "        # Compare common alleles between datasets\n",
    "        if 'allele_metrics_df' in locals():\n",
    "            common_alleles = set(allele_metrics_df['allele']) & set(capsnet_allele_metrics_df['allele'])\n",
    "            if common_alleles:\n",
    "                print(f\"\\nFound {len(common_alleles)} common alleles between datasets\")\n",
    "\n",
    "                # Create comparison for common alleles\n",
    "                comparison_data = []\n",
    "                for allele in common_alleles:\n",
    "                    orig_metrics = allele_metrics_df[allele_metrics_df['allele'] == allele].iloc[0]\n",
    "                    caps_metrics = capsnet_allele_metrics_df[capsnet_allele_metrics_df['allele'] == allele].iloc[0]\n",
    "                    comparison_data.append({\n",
    "                        'allele': allele,\n",
    "                        'orig_auc': orig_metrics['auc'],\n",
    "                        'caps_auc': caps_metrics['auc'],\n",
    "                        'orig_count': orig_metrics['count'],\n",
    "                        'caps_count': caps_metrics['count'],\n",
    "                        'auc_diff': caps_metrics['auc'] - orig_metrics['auc']\n",
    "                    })\n",
    "\n",
    "                comparison_df = pd.DataFrame(comparison_data)\n",
    "                comparison_df = comparison_df.sort_values('auc_diff', ascending=False)\n",
    "\n",
    "                print(f\"\\nPer-allele AUC comparison (CapsNet - Original):\")\n",
    "                print(comparison_df.head(10).to_string(index=False))\n",
    "\n",
    "                # Visualize allele comparison\n",
    "                plt.figure(figsize=(15, 10))\n",
    "\n",
    "                plt.subplot(2, 2, 1)\n",
    "                plt.scatter(comparison_df['orig_auc'], comparison_df['caps_auc'], alpha=0.6)\n",
    "                plt.plot([0, 1], [0, 1], 'r--', alpha=0.5)\n",
    "                plt.xlabel('Original Test AUC')\n",
    "                plt.ylabel('CapsNet IEDB AUC')\n",
    "                plt.title('Per-Allele AUC Comparison')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "\n",
    "                plt.subplot(2, 2, 2)\n",
    "                plt.hist(comparison_df['auc_diff'], bins=15, alpha=0.7, edgecolor='black')\n",
    "                plt.axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
    "                plt.axvline(x=comparison_df['auc_diff'].mean(), color='blue', linestyle='--',\n",
    "                           label=f'Mean: {comparison_df[\"auc_diff\"].mean():.3f}')\n",
    "                plt.xlabel('AUC Difference (CapsNet - Original)')\n",
    "                plt.ylabel('Number of Alleles')\n",
    "                plt.title('Distribution of AUC Differences')\n",
    "                plt.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "\n",
    "                plt.subplot(2, 2, 3)\n",
    "                top_improved = comparison_df.head(10)\n",
    "                plt.barh(range(len(top_improved)), top_improved['auc_diff'], color='green', alpha=0.7)\n",
    "                plt.yticks(range(len(top_improved)), top_improved['allele'])\n",
    "                plt.xlabel('AUC Improvement (CapsNet vs Original)')\n",
    "                plt.title('Top 10 Most Improved Alleles')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "\n",
    "                plt.subplot(2, 2, 4)\n",
    "                bottom_improved = comparison_df.tail(10)\n",
    "                plt.barh(range(len(bottom_improved)), bottom_improved['auc_diff'], color='red', alpha=0.7)\n",
    "                plt.yticks(range(len(bottom_improved)), bottom_improved['allele'])\n",
    "                plt.xlabel('AUC Change (CapsNet vs Original)')\n",
    "                plt.title('10 Most Declined Alleles')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n"
   ],
   "id": "a853a1e239448c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# ORGANIZED VISUALIZATION FRAMEWORK\n",
    "# ============================================================================\n",
    "\n",
    "# Dataset paths configuration\n",
    "DATASET_PATHS = {\n",
    "    'train': os.path.join(parquet_dir, 'new_df2/train_size300002_seed1.parquet'),\n",
    "    'test': os.path.join(parquet_dir, 'new_df2/test_set.parquet'),\n",
    "    'val': os.path.join(parquet_dir, 'CapsNet_IEDB_test_reduced.parquet'),\n",
    "    'bench1': os.path.join(parquet_dir, 'benchmark1.parquet'),  # Add actual benchmark paths\n",
    "    'bench2': os.path.join(parquet_dir, 'benchmark2.parquet'),\n",
    "    'bench3': os.path.join(parquet_dir, 'benchmark3.parquet')\n",
    "}\n",
    "\n",
    "def plot_per_allele_metrics(predictions_df, y_true, dataset_name, save_path=None):\n",
    "    \"\"\"Plot per-allele AUC, ACC, Precision, and Recall metrics.\"\"\"\n",
    "    from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "    import pandas as pd\n",
    "\n",
    "    if 'allele' not in predictions_df.columns:\n",
    "        print(f\"Warning: No 'allele' column found in {dataset_name} predictions\")\n",
    "        return\n",
    "\n",
    "    # Calculate per-allele metrics\n",
    "    allele_metrics = []\n",
    "    for allele in predictions_df['allele'].unique():\n",
    "        mask = predictions_df['allele'] == allele\n",
    "        if mask.sum() < 2:  # Skip alleles with too few samples\n",
    "            continue\n",
    "\n",
    "        allele_y_true = y_true[mask]\n",
    "        allele_y_pred = predictions_df.loc[mask, 'prediction'].values\n",
    "        allele_y_pred_binary = (allele_y_pred > 0.5).astype(int)\n",
    "\n",
    "        if len(np.unique(allele_y_true)) > 1:  # Only calculate AUC if both classes present\n",
    "            auc = roc_auc_score(allele_y_true, allele_y_pred)\n",
    "        else:\n",
    "            auc = np.nan\n",
    "\n",
    "        acc = accuracy_score(allele_y_true, allele_y_pred_binary)\n",
    "        precision = precision_score(allele_y_true, allele_y_pred_binary, zero_division=0)\n",
    "        recall = recall_score(allele_y_true, allele_y_pred_binary, zero_division=0)\n",
    "\n",
    "        allele_metrics.append({\n",
    "            'allele': allele,\n",
    "            'auc': auc,\n",
    "            'accuracy': acc,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'sample_count': mask.sum()\n",
    "        })\n",
    "\n",
    "    metrics_df = pd.DataFrame(allele_metrics)\n",
    "    metrics_df = metrics_df.dropna(subset=['auc']).sort_values('auc', ascending=False)\n",
    "\n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(f'Per-Allele Metrics - {dataset_name}', fontsize=16)\n",
    "\n",
    "    # AUC plot\n",
    "    axes[0, 0].barh(range(len(metrics_df)), metrics_df['auc'], alpha=0.7)\n",
    "    axes[0, 0].set_yticks(range(len(metrics_df)))\n",
    "    axes[0, 0].set_yticklabels(metrics_df['allele'], fontsize=8)\n",
    "    axes[0, 0].set_xlabel('AUC')\n",
    "    axes[0, 0].set_title('AUC per Allele')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Accuracy plot\n",
    "    axes[0, 1].barh(range(len(metrics_df)), metrics_df['accuracy'], alpha=0.7, color='orange')\n",
    "    axes[0, 1].set_yticks(range(len(metrics_df)))\n",
    "    axes[0, 1].set_yticklabels(metrics_df['allele'], fontsize=8)\n",
    "    axes[0, 1].set_xlabel('Accuracy')\n",
    "    axes[0, 1].set_title('Accuracy per Allele')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Precision plot\n",
    "    axes[1, 0].barh(range(len(metrics_df)), metrics_df['precision'], alpha=0.7, color='green')\n",
    "    axes[1, 0].set_yticks(range(len(metrics_df)))\n",
    "    axes[1, 0].set_yticklabels(metrics_df['allele'], fontsize=8)\n",
    "    axes[1, 0].set_xlabel('Precision')\n",
    "    axes[1, 0].set_title('Precision per Allele')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Recall plot\n",
    "    axes[1, 1].barh(range(len(metrics_df)), metrics_df['recall'], alpha=0.7, color='red')\n",
    "    axes[1, 1].set_yticks(range(len(metrics_df)))\n",
    "    axes[1, 1].set_yticklabels(metrics_df['allele'], fontsize=8)\n",
    "    axes[1, 1].set_xlabel('Recall')\n",
    "    axes[1, 1].set_title('Recall per Allele')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}/per_allele_metrics_{dataset_name}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "def plot_attention_weights(model, sample_data, dataset_name, save_path=None):\n",
    "    \"\"\"Plot attention weights to show which positions had highest attention.\"\"\"\n",
    "    try:\n",
    "        # Get attention weights from model\n",
    "        attention_layer_outputs = []\n",
    "        for layer in model.layers:\n",
    "            if hasattr(layer, 'attention_weights') or 'attention' in layer.name.lower():\n",
    "                attention_layer_outputs.append(layer)\n",
    "\n",
    "        if not attention_layer_outputs:\n",
    "            print(f\"No attention layers found in model for {dataset_name}\")\n",
    "            return\n",
    "\n",
    "        # Extract attention weights for a sample\n",
    "        sample_input = {k: v[:1] for k, v in sample_data.items()}  # Take first sample\n",
    "\n",
    "        # Create a model that outputs attention weights\n",
    "        attention_model = keras.Model(\n",
    "            inputs=model.inputs,\n",
    "            outputs=[layer.output for layer in attention_layer_outputs] + [model.output]\n",
    "        )\n",
    "\n",
    "        outputs = attention_model(sample_input)\n",
    "        attention_weights = outputs[:-1]  # All but last (prediction)\n",
    "\n",
    "        fig, axes = plt.subplots(len(attention_weights), 1, figsize=(12, 4*len(attention_weights)))\n",
    "        if len(attention_weights) == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        fig.suptitle(f'Attention Weights - {dataset_name}', fontsize=16)\n",
    "\n",
    "        for i, att_weights in enumerate(attention_weights):\n",
    "            # Average over heads if multi-head attention\n",
    "            if len(att_weights.shape) > 3:\n",
    "                att_weights = tf.reduce_mean(att_weights, axis=1)\n",
    "\n",
    "            # Plot heatmap\n",
    "            sns.heatmap(att_weights[0].numpy(), ax=axes[i], cmap='Blues', cbar=True)\n",
    "            axes[i].set_title(f'Attention Layer {i+1}')\n",
    "            axes[i].set_xlabel('Position')\n",
    "            axes[i].set_ylabel('Head/Query')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(f\"{save_path}/attention_weights_{dataset_name}.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not extract attention weights for {dataset_name}: {e}\")\n",
    "\n",
    "def plot_latent_umap(embeddings, labels, dataset_name, save_path=None):\n",
    "    \"\"\"Create UMAP visualization of latent embeddings.\"\"\"\n",
    "    try:\n",
    "        import umap\n",
    "\n",
    "        # Fit UMAP\n",
    "        reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "        embedding_2d = reducer.fit_transform(embeddings)\n",
    "\n",
    "        # Create plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        scatter = plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1],\n",
    "                            c=labels, cmap='RdYlBu', alpha=0.6, s=50)\n",
    "        plt.colorbar(scatter, label='Binding Affinity')\n",
    "        plt.title(f'UMAP Latent Space Visualization - {dataset_name}')\n",
    "        plt.xlabel('UMAP 1')\n",
    "        plt.ylabel('UMAP 2')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(f\"{save_path}/latent_umap_{dataset_name}.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"UMAP not installed. Install with: pip install umap-learn\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating UMAP visualization for {dataset_name}: {e}\")\n",
    "\n",
    "def plot_comprehensive_metrics(y_true, y_pred, y_pred_proba, dataset_name, save_path=None):\n",
    "    \"\"\"Plot comprehensive metrics: confusion matrix, PR curve, ROC curve, etc.\"\"\"\n",
    "    from sklearn.metrics import (confusion_matrix, precision_recall_curve, roc_curve, auc, classification_report)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle(f'Comprehensive Metrics - {dataset_name}', fontsize=16)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=axes[0, 0], cmap='Blues')\n",
    "    axes[0, 0].set_title('Confusion Matrix')\n",
    "    axes[0, 0].set_xlabel('Predicted')\n",
    "    axes[0, 0].set_ylabel('Actual')\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    axes[0, 1].plot(recall, precision, label=f'PR AUC = {pr_auc:.3f}')\n",
    "    axes[0, 1].set_xlabel('Recall')\n",
    "    axes[0, 1].set_ylabel('Precision')\n",
    "    axes[0, 1].set_title('Precision-Recall Curve')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    axes[0, 2].plot(fpr, tpr, label=f'ROC AUC = {roc_auc:.3f}')\n",
    "    axes[0, 2].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    axes[0, 2].set_xlabel('False Positive Rate')\n",
    "    axes[0, 2].set_ylabel('True Positive Rate')\n",
    "    axes[0, 2].set_title('ROC Curve')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    # Prediction Score Distribution\n",
    "    axes[1, 0].hist(y_pred_proba[y_true == 0], bins=50, alpha=0.5, label='Non-binders', density=True)\n",
    "    axes[1, 0].hist(y_pred_proba[y_true == 1], bins=50, alpha=0.5, label='Binders', density=True)\n",
    "    axes[1, 0].set_xlabel('Prediction Score')\n",
    "    axes[1, 0].set_ylabel('Density')\n",
    "    axes[1, 0].set_title('Prediction Score Distribution')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Calibration Plot\n",
    "    try:\n",
    "        prob_true, prob_pred = calibration_curve(y_true, y_pred_proba, n_bins=10)\n",
    "        axes[1, 1].plot(prob_pred, prob_true, marker='o')\n",
    "        axes[1, 1].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "        axes[1, 1].set_xlabel('Mean Predicted Probability')\n",
    "        axes[1, 1].set_ylabel('Fraction of Positives')\n",
    "        axes[1, 1].set_title('Calibration Plot')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    except Exception as e:\n",
    "        axes[1, 1].text(0.5, 0.5, f'Calibration plot error: {e}',\n",
    "                       transform=axes[1, 1].transAxes, ha='center')\n",
    "\n",
    "    # Accuracy by Prediction Confidence\n",
    "    confidence = np.abs(y_pred_proba - 0.5) * 2  # Convert to 0-1 confidence\n",
    "    conf_bins = np.linspace(0, 1, 11)\n",
    "    conf_accuracies = []\n",
    "    conf_centers = []\n",
    "\n",
    "    for i in range(len(conf_bins) - 1):\n",
    "        mask = (confidence >= conf_bins[i]) & (confidence < conf_bins[i + 1])\n",
    "        if mask.sum() > 0:\n",
    "            acc = accuracy_score(y_true[mask], y_pred[mask])\n",
    "            conf_accuracies.append(acc)\n",
    "            conf_centers.append((conf_bins[i] + conf_bins[i + 1]) / 2)\n",
    "\n",
    "    axes[1, 2].plot(conf_centers, conf_accuracies, marker='o')\n",
    "    axes[1, 2].set_xlabel('Prediction Confidence')\n",
    "    axes[1, 2].set_ylabel('Accuracy')\n",
    "    axes[1, 2].set_title('Accuracy by Prediction Confidence')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}/comprehensive_metrics_{dataset_name}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_dataset(dataset_path, dataset_name, model=None, save_path=None):\n",
    "    \"\"\"Apply all visualization functions to a dataset.\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"VISUALIZING DATASET: {dataset_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"Warning: Dataset file not found: {dataset_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Load dataset\n",
    "        df = pd.read_parquet(dataset_path)\n",
    "        print(f\"Loaded {len(df)} samples from {dataset_name}\")\n",
    "\n",
    "        # Prepare data for model prediction if model is provided\n",
    "        if model is not None:\n",
    "            # Here you would need to process the dataframe into model input format\n",
    "            # This depends on your specific data preprocessing pipeline\n",
    "            print(f\"Making predictions on {dataset_name}...\")\n",
    "            # predictions = model.predict(processed_data)\n",
    "            # For now, we'll create dummy predictions for demonstration\n",
    "            predictions = np.random.random(len(df))\n",
    "            predictions_df = df.copy()\n",
    "            predictions_df['prediction'] = predictions\n",
    "\n",
    "            # Extract true labels and predictions\n",
    "            y_true = df['binding_affinity'].values if 'binding_affinity' in df.columns else np.random.randint(0, 2, len(df))\n",
    "            y_pred = (predictions > 0.5).astype(int)\n",
    "            y_pred_proba = predictions\n",
    "\n",
    "            # Apply visualization functions\n",
    "            print(\"1. Plotting per-allele metrics...\")\n",
    "            plot_per_allele_metrics(predictions_df, y_true, dataset_name, save_path)\n",
    "\n",
    "            print(\"2. Plotting attention weights...\")\n",
    "            # For attention weights, we need sample model input data\n",
    "            # This would depend on your model's input format\n",
    "            sample_data = {}  # You would need to prepare this based on your model\n",
    "            plot_attention_weights(model, sample_data, dataset_name, save_path)\n",
    "\n",
    "            print(\"3. Creating latent UMAP visualization...\")\n",
    "            # For latent embeddings, you would extract them from an intermediate layer\n",
    "            embeddings = np.random.random((len(df), 128))  # Dummy embeddings\n",
    "            plot_latent_umap(embeddings, y_true, dataset_name, save_path)\n",
    "\n",
    "            print(\"4. Plotting comprehensive metrics...\")\n",
    "            plot_comprehensive_metrics(y_true, y_pred, y_pred_proba, dataset_name, save_path)\n",
    "\n",
    "        else:\n",
    "            print(\"No model provided - skipping model-dependent visualizations\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {dataset_name}: {e}\")\n",
    "\n",
    "# Example usage - apply visualizations to all datasets\n",
    "def run_all_visualizations(model=None, save_path=\"visualization_outputs\"):\n",
    "    \"\"\"Run visualizations for all datasets.\"\"\"\n",
    "    # Create output directory\n",
    "    if save_path and not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    # Process each dataset\n",
    "    for dataset_name, dataset_path in DATASET_PATHS.items():\n",
    "        visualize_dataset(dataset_path, dataset_name, model, save_path)\n",
    "\n",
    "# Uncomment to run visualizations\n",
    "# run_all_visualizations(model=your_trained_model)\n",
    "\n",
    "print(\"\\nCapsNet IEDB evaluation complete!\")"
   ],
   "id": "fa9d6a36d38c43d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c2c05b8e52f4b537"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
