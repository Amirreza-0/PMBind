{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4TAsIvffHEA",
        "outputId": "3fa44ab1-b49a-4881-e353-85d6c3f8a004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "TESTING LIKELIHOOD FUNCTION\n",
            "======================================================================\n",
            "\n",
            "[Test 1] Basic functionality (9-mers)\n",
            "  Loss: 0.6567\n",
            "  Shapes - attn: (4, 36), bind_probs: (4, 36), cores: (4, 36, 15, 128)\n",
            "  ✓ Loss is finite: True\n",
            "  ✓ Loss >= 0: True\n",
            "\n",
            "[Test 2] Attention weights properties\n",
            "  Attention sums: [1. 1. 1. 1.]\n",
            "  ✓ Sum to 1: True\n",
            "  ✓ All in [0,1]: True\n",
            "\n",
            "[Test 3] Binding probabilities\n",
            "  Min: 0.0000, Max: 0.6471\n",
            "  ✓ All in [0,1]: True\n",
            "\n",
            "[Test 4] Variable length cores (8-11 mers)\n",
            "  Expected cores: 36, Actual: 36\n",
            "  ✓ Correct number of cores: True\n",
            "  ✓ Attention sums to 1: True\n",
            "\n",
            "[Test 5] Sample weights\n",
            "  Loss (no weight): 0.6567\n",
            "  Loss (zero weight): 0.0000\n",
            "  ✓ Zero weights give zero loss: True\n",
            "\n",
            "[Test 6] Temperature effect on attention\n",
            "  Max attention (T=0.1): [0.7064812  0.40670326 0.5280339  0.62597156]\n",
            "  Max attention (T=10.0): [0.3368348  0.33414948 0.3353532  0.336384  ]\n",
            "  ✓ Low T more peaked: True\n",
            "\n",
            "[Test 7] Gradient computation\n",
            "  Gradient shape: (4, 15, 128)\n",
            "  ✓ Gradients exist: True\n",
            "  ✓ Gradients finite: True\n",
            "\n",
            "======================================================================\n",
            "ALL TESTS PASSED ✓\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Constants\n",
        "PAD_TOKEN = -2.0\n",
        "NORM_TOKEN = 1.0\n",
        "PAD_VALUE = 0.0\n",
        "\n",
        "\n",
        "def likelihood_with_attention_vectorized(nn_model, peptide, mhc, label,\n",
        "                                         pep_mask, mhc_mask, max_pep_len,\n",
        "                                         temperature=1.0, variable_len=8,\n",
        "                                         sample_weight=None, training=True):\n",
        "    \"\"\"\n",
        "    Computes the marginal binding probability by marginalizing over latent cores.\n",
        "\n",
        "    Probabilistic Model:\n",
        "        - z_i ∈ C_i: latent variable for true binding core\n",
        "        - q_ic = P(z_i = c | S_i, MHC) = softmax(g(c, S_i, MHC) / τ)\n",
        "        - P(bind | z_i = c, MHC) = σ(f(c, MHC))\n",
        "        - P_ij = Σ_c q_ic · σ(f(c, MHC))  [marginalization]\n",
        "\n",
        "    Model outputs expected:\n",
        "        - 'core_logits': g(c, S_i, MHC) - core selection scores\n",
        "        - 'binding_logits': f(c, MHC) - binding prediction scores\n",
        "\n",
        "    Args:\n",
        "        nn_model: Keras model with two output heads\n",
        "        peptide: (B, P, D) peptide embeddings\n",
        "        mhc: (B, M, D) MHC embeddings\n",
        "        label: (B, 1) binary binding labels\n",
        "        pep_mask: (B, P) peptide mask\n",
        "        mhc_mask: (B, M) MHC mask\n",
        "        max_pep_len: int, maximum peptide length\n",
        "        temperature: float, softmax temperature for core selection\n",
        "        variable_len: int, minimum core length (cores from variable_len to max_pep_len)\n",
        "        sample_weight: (B, 1) or (B,) optional sample weights\n",
        "        training: bool, training mode flag\n",
        "\n",
        "    Returns:\n",
        "        loss: scalar, mean negative log-likelihood\n",
        "        attention_weights: (B, N) core selection probabilities q_ic\n",
        "        binding_probs_per_core: (B, N) per-core binding probabilities σ(f_c)\n",
        "        cores_stack: (B, N, P, D) padded core embeddings\n",
        "    \"\"\"\n",
        "    B = tf.shape(peptide)[0]\n",
        "    P = tf.shape(peptide)[1]\n",
        "    D = tf.shape(peptide)[2]\n",
        "    M = tf.shape(mhc)[1]\n",
        "\n",
        "    # Define core lengths: [variable_len, variable_len+1, ..., max_pep_len]\n",
        "    if variable_len is None:\n",
        "        ks = [9]\n",
        "    else:\n",
        "        ks = range(variable_len, max_pep_len + 1)\n",
        "\n",
        "    cores_list = []\n",
        "    masks_list = []\n",
        "    valid_flags_list = []\n",
        "\n",
        "    # Extract all possible cores\n",
        "    for k in ks:\n",
        "        num_windows = max_pep_len - k + 1\n",
        "        if num_windows <= 0:\n",
        "            continue\n",
        "\n",
        "        for i in range(num_windows):\n",
        "            # Extract core: (B, k, D)\n",
        "            core = peptide[:, i:i + k, :]\n",
        "\n",
        "            # Check validity: core is valid if last position is not padding\n",
        "            is_valid = tf.not_equal(pep_mask[:, i + k - 1], PAD_TOKEN)\n",
        "            valid_flags_list.append(is_valid)\n",
        "\n",
        "            # Pad core to max_pep_len: (B, max_pep_len, D)\n",
        "            paddings = [[0, 0], [0, max_pep_len - k], [0, 0]]\n",
        "            padded_core = tf.pad(core, paddings, \"CONSTANT\", constant_values=PAD_VALUE)\n",
        "            cores_list.append(padded_core)\n",
        "\n",
        "            # Create mask for this core length\n",
        "            mask_vec = tf.concat([\n",
        "                tf.fill([k], NORM_TOKEN),\n",
        "                tf.fill([max_pep_len - k], PAD_TOKEN)\n",
        "            ], axis=0)\n",
        "            mask_batch = tf.tile(tf.expand_dims(mask_vec, 0), [B, 1])\n",
        "            masks_list.append(mask_batch)\n",
        "\n",
        "    # Handle edge case: no valid cores\n",
        "    if not cores_list:\n",
        "        return (tf.constant(0.0),\n",
        "                tf.zeros((B, 0)),\n",
        "                tf.zeros((B, 0)),\n",
        "                tf.zeros((B, 0, max_pep_len, D)))\n",
        "\n",
        "    # Stack all cores: (B, N, ...)\n",
        "    cores_stack = tf.stack(cores_list, axis=1)  # (B, N, P, D)\n",
        "    masks_stack = tf.stack(masks_list, axis=1)  # (B, N, P)\n",
        "    valid_flags_stack = tf.stack(valid_flags_list, axis=1)  # (B, N)\n",
        "    valid_flags_float = tf.cast(valid_flags_stack, tf.float32)\n",
        "\n",
        "    N = tf.shape(cores_stack)[1]\n",
        "\n",
        "    # Flatten for batch processing through model\n",
        "    cores_flat = tf.reshape(cores_stack, [B * N, max_pep_len, D])\n",
        "    masks_flat = tf.reshape(masks_stack, [B * N, max_pep_len])\n",
        "\n",
        "    # Tile MHC for each core\n",
        "    mhc_tiled = tf.tile(tf.expand_dims(mhc, 1), [1, N, 1, 1])\n",
        "    mhc_flat = tf.reshape(mhc_tiled, [B * N, M, D])\n",
        "\n",
        "    mhc_mask_tiled = tf.tile(tf.expand_dims(mhc_mask, 1), [1, N, 1])\n",
        "    mhc_mask_flat = tf.reshape(mhc_mask_tiled, [B * N, M])\n",
        "\n",
        "    # Dummy target (if model requires it)\n",
        "    dummy_target = tf.zeros((B * N, max_pep_len, 21), dtype=tf.float32)\n",
        "\n",
        "    # Forward pass\n",
        "    inputs = [cores_flat, masks_flat, mhc_flat, mhc_mask_flat, dummy_target]\n",
        "    outputs = nn_model(inputs, training=training)\n",
        "\n",
        "    # Extract model outputs\n",
        "    core_logits_flat = outputs['core_logits']  # g(c, S_i, MHC): (B*N, 1)\n",
        "    binding_logits_flat = outputs['binding_logits']  # f(c, MHC): (B*N, 1)\n",
        "\n",
        "    # Reshape to (B, N)\n",
        "    core_logits = tf.reshape(core_logits_flat, [B, N])\n",
        "    binding_logits = tf.reshape(binding_logits_flat, [B, N])\n",
        "\n",
        "    # =========================================================\n",
        "    # Core selection distribution: q_ic = softmax(g_c / τ)\n",
        "    # =========================================================\n",
        "    # Mask invalid cores with -inf before softmax\n",
        "    core_logits_masked = tf.where(valid_flags_stack, core_logits, -1e9)\n",
        "    attention_weights = tf.nn.softmax(core_logits_masked / temperature, axis=1)  # (B, N)\n",
        "\n",
        "    # =========================================================\n",
        "    # Per-core binding probability: σ(f(c, MHC))\n",
        "    # =========================================================\n",
        "    binding_probs_per_core = tf.nn.sigmoid(binding_logits)  # (B, N)\n",
        "    # Mask invalid cores to 0 for clean output\n",
        "    binding_probs_per_core = binding_probs_per_core * valid_flags_float\n",
        "\n",
        "    # =========================================================\n",
        "    # Marginal binding probability: P_ij = Σ_c q_ic · σ(f_c)\n",
        "    # =========================================================\n",
        "    pred_prob = tf.reduce_sum(attention_weights * binding_probs_per_core, axis=1)  # (B,)\n",
        "\n",
        "    # =========================================================\n",
        "    # Negative log-likelihood loss\n",
        "    # =========================================================\n",
        "    label_flat = tf.cast(tf.reshape(label, [B]), dtype=pred_prob.dtype)\n",
        "\n",
        "    # Clip for numerical stability\n",
        "    pred_prob_clipped = tf.clip_by_value(pred_prob, 1e-7, 1.0 - 1e-7)\n",
        "\n",
        "    # Binary cross-entropy: -[y·log(p) + (1-y)·log(1-p)]\n",
        "    loss = -label_flat * tf.math.log(pred_prob_clipped) \\\n",
        "           - (1.0 - label_flat) * tf.math.log(1.0 - pred_prob_clipped)\n",
        "\n",
        "    # Apply sample weights if provided\n",
        "    if sample_weight is not None:\n",
        "        sample_weight = tf.cast(sample_weight, loss.dtype)\n",
        "        if len(sample_weight.shape) == 2:\n",
        "            sample_weight = tf.squeeze(sample_weight, axis=1)\n",
        "        loss = loss * sample_weight\n",
        "\n",
        "    return tf.reduce_mean(loss), attention_weights, binding_probs_per_core, cores_stack\n",
        "\n",
        "# Test suite\n",
        "def run_tests():\n",
        "    \"\"\"Run comprehensive tests on the likelihood function.\"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"TESTING LIKELIHOOD FUNCTION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Define a test model with correct output heads\n",
        "    def create_test_model(max_pep_len, max_mhc_len, embed_dim):\n",
        "        peptide_input = keras.Input(shape=(max_pep_len, embed_dim), name='peptide')\n",
        "        pep_mask_input = keras.Input(shape=(max_pep_len,), name='pep_mask')\n",
        "        mhc_input = keras.Input(shape=(max_mhc_len, embed_dim), name='mhc')\n",
        "        mhc_mask_input = keras.Input(shape=(max_mhc_len,), name='mhc_mask')\n",
        "        dummy_target = keras.Input(shape=(max_pep_len, 21), name='dummy_target')\n",
        "\n",
        "        pep_avg = layers.GlobalAveragePooling1D()(peptide_input)\n",
        "        mhc_avg = layers.GlobalAveragePooling1D()(mhc_input)\n",
        "        combined = layers.Concatenate()([pep_avg, mhc_avg])\n",
        "\n",
        "        # Use all mask/dummy inputs (multiply by 0 so they don't affect output)\n",
        "        dummy_effect = layers.Lambda(lambda x: tf.expand_dims(\n",
        "            tf.reduce_mean(x[0]) * 0.0 + tf.reduce_mean(x[1]) * 0.0 + tf.reduce_mean(x[2]) * 0.0,\n",
        "            -1\n",
        "        ))([dummy_target, pep_mask_input, mhc_mask_input])\n",
        "\n",
        "        # Two output heads as required\n",
        "        core_logits = layers.Dense(1, name='core_logits')(combined)\n",
        "        core_logits = layers.Add()([core_logits, dummy_effect])\n",
        "\n",
        "        binding_logits = layers.Dense(1, name='binding_logits')(combined)\n",
        "\n",
        "        return keras.Model(\n",
        "            inputs=[peptide_input, pep_mask_input, mhc_input, mhc_mask_input, dummy_target],\n",
        "            outputs={'core_logits': core_logits, 'binding_logits': binding_logits}\n",
        "        )\n",
        "\n",
        "    batch_size = 4\n",
        "    max_pep_len = 15\n",
        "    max_mhc_len = 50\n",
        "    embed_dim = 128\n",
        "    model = create_test_model(max_pep_len, max_mhc_len, embed_dim)\n",
        "\n",
        "    # Test 1: Basic functionality with 9-mers\n",
        "    print(\"\\n[Test 1] Basic functionality (9-mers)\")\n",
        "    peptide = tf.random.normal((batch_size, max_pep_len, embed_dim))\n",
        "    mhc = tf.random.normal((batch_size, max_mhc_len, embed_dim))\n",
        "    label = tf.constant([[1.0], [0.0], [1.0], [0.0]])\n",
        "    pep_mask = tf.concat([tf.fill([batch_size, 9], NORM_TOKEN),\n",
        "                          tf.fill([batch_size, max_pep_len - 9], PAD_TOKEN)], axis=1)\n",
        "    mhc_mask = tf.fill([batch_size, max_mhc_len], NORM_TOKEN)\n",
        "\n",
        "    loss, attn, bind_probs, cores = likelihood_with_attention_vectorized(\n",
        "        model, peptide, mhc, label, pep_mask, mhc_mask, max_pep_len, training=False)\n",
        "\n",
        "    print(f\"  Loss: {loss.numpy():.4f}\")\n",
        "    print(f\"  Shapes - attn: {attn.shape}, bind_probs: {bind_probs.shape}, cores: {cores.shape}\")\n",
        "    print(f\"  ✓ Loss is finite: {tf.math.is_finite(loss).numpy()}\")\n",
        "    print(f\"  ✓ Loss >= 0: {loss.numpy() >= 0}\")\n",
        "\n",
        "    # Test 2: Attention weights sum to 1\n",
        "    print(\"\\n[Test 2] Attention weights properties\")\n",
        "    attn_sums = tf.reduce_sum(attn, axis=1).numpy()\n",
        "    print(f\"  Attention sums: {attn_sums}\")\n",
        "    print(f\"  ✓ Sum to 1: {tf.reduce_all(tf.abs(attn_sums - 1.0) < 1e-5).numpy()}\")\n",
        "    print(f\"  ✓ All in [0,1]: {tf.reduce_all((attn >= 0) & (attn <= 1)).numpy()}\")\n",
        "\n",
        "    # Test 3: Binding probabilities in [0, 1]\n",
        "    print(\"\\n[Test 3] Binding probabilities\")\n",
        "    print(f\"  Min: {tf.reduce_min(bind_probs).numpy():.4f}, Max: {tf.reduce_max(bind_probs).numpy():.4f}\")\n",
        "    print(f\"  ✓ All in [0,1]: {tf.reduce_all((bind_probs >= 0) & (bind_probs <= 1)).numpy()}\")\n",
        "\n",
        "    # Test 4: Variable length cores (8-11 mers)\n",
        "    print(\"\\n[Test 4] Variable length cores (8-11 mers)\")\n",
        "    pep_mask_11 = tf.concat([tf.fill([batch_size, 11], NORM_TOKEN),\n",
        "                             tf.fill([batch_size, max_pep_len - 11], PAD_TOKEN)], axis=1)\n",
        "    loss_var, attn_var, _, _ = likelihood_with_attention_vectorized(\n",
        "        model, peptide, mhc, label, pep_mask_11, mhc_mask, max_pep_len,\n",
        "        variable_len=8, training=False)\n",
        "\n",
        "    expected_cores = sum(15 - k + 1 for k in range(8, 16))\n",
        "    print(f\"  Expected cores: {expected_cores}, Actual: {attn_var.shape[1]}\")\n",
        "    print(f\"  ✓ Correct number of cores: {attn_var.shape[1] == expected_cores}\")\n",
        "    print(f\"  ✓ Attention sums to 1: {tf.reduce_all(tf.abs(tf.reduce_sum(attn_var, axis=1) - 1.0) < 1e-5).numpy()}\")\n",
        "\n",
        "    # Test 5: Sample weights\n",
        "    print(\"\\n[Test 5] Sample weights\")\n",
        "    loss_no_wt, _, _, _ = likelihood_with_attention_vectorized(\n",
        "        model, peptide, mhc, label, pep_mask, mhc_mask, max_pep_len,\n",
        "        sample_weight=None, training=False)\n",
        "    loss_zero_wt, _, _, _ = likelihood_with_attention_vectorized(\n",
        "        model, peptide, mhc, label, pep_mask, mhc_mask, max_pep_len,\n",
        "        sample_weight=tf.zeros((batch_size, 1)), training=False)\n",
        "\n",
        "    print(f\"  Loss (no weight): {loss_no_wt.numpy():.4f}\")\n",
        "    print(f\"  Loss (zero weight): {loss_zero_wt.numpy():.4f}\")\n",
        "    print(f\"  ✓ Zero weights give zero loss: {abs(loss_zero_wt.numpy()) < 1e-6}\")\n",
        "\n",
        "    # Test 6: Temperature effect\n",
        "    print(\"\\n[Test 6] Temperature effect on attention\")\n",
        "    _, attn_low, _, _ = likelihood_with_attention_vectorized(\n",
        "        model, peptide, mhc, label, pep_mask, mhc_mask, max_pep_len,\n",
        "        temperature=0.1, training=False)\n",
        "    _, attn_high, _, _ = likelihood_with_attention_vectorized(\n",
        "        model, peptide, mhc, label, pep_mask, mhc_mask, max_pep_len,\n",
        "        temperature=10.0, training=False)\n",
        "\n",
        "    max_low = tf.reduce_max(attn_low, axis=1).numpy()\n",
        "    max_high = tf.reduce_max(attn_high, axis=1).numpy()\n",
        "    print(f\"  Max attention (T=0.1): {max_low}\")\n",
        "    print(f\"  Max attention (T=10.0): {max_high}\")\n",
        "    print(f\"  ✓ Low T more peaked: {tf.reduce_all(max_low > max_high).numpy()}\")\n",
        "\n",
        "    # Test 7: Gradient computation\n",
        "    print(\"\\n[Test 7] Gradient computation\")\n",
        "    peptide_var = tf.Variable(peptide)\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_grad, _, _, _ = likelihood_with_attention_vectorized(\n",
        "            model, peptide_var, mhc, label, pep_mask, mhc_mask, max_pep_len, training=True)\n",
        "    grads = tape.gradient(loss_grad, peptide_var)\n",
        "    print(f\"  Gradient shape: {grads.shape}\")\n",
        "    print(f\"  ✓ Gradients exist: {grads is not None}\")\n",
        "    print(f\"  ✓ Gradients finite: {tf.reduce_all(tf.math.is_finite(grads)).numpy()}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ALL TESTS PASSED ✓\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_tests()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YEaNR2GOfIuo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}