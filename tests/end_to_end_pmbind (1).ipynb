{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "# testing the subtract model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from focal_loss import binary_focal_loss"
   ],
   "id": "46c304db2787b9f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "class MaskedEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, mask_token=-1., pad_token=-2., name='masked_embedding'):\n",
    "        super().__init__(name=name)\n",
    "        self.mask_token = mask_token\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (B, N, D)\n",
    "            mask: Tensor of shape (B, N)\n",
    "        Returns:\n",
    "            Tensor with masked positions set to zero.\n",
    "        \"\"\"\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        mask = tf.where((mask == self.pad_token) | (mask == self.mask_token), 0., 1.)\n",
    "        return x * mask[:, :, tf.newaxis]  # Apply mask to zero out positions\n",
    "\n",
    "\n",
    "class PositionalEncoding(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Sinusoidal Positional Encoding layer that applies encodings\n",
    "    only to non-masked tokens.\n",
    "\n",
    "    Args:\n",
    "        embed_dim (int): Dimension of embeddings (must match input last dim).\n",
    "        max_len (int): Maximum sequence length expected (used to precompute encodings).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, pos_range=100, mask_token=-1., pad_token=-2., name='positional_encoding'):\n",
    "        super().__init__(name=name)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.pos_range = pos_range\n",
    "        self.mask_token = mask_token\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "    def build(self, x):\n",
    "        # Create (1, pos_range, embed_dim) encoding matrix\n",
    "        pos = tf.range(self.pos_range, dtype=tf.float32)[:, tf.newaxis]  # (pos_range, 1)\n",
    "        i = tf.range(self.embed_dim, dtype=tf.float32)[tf.newaxis, :]  # (1, embed_dim)\n",
    "        #angle_rates = 1 / tf.pow(300.0, (2 * (i // 2)) / tf.cast(self.embed_dim, tf.float32))\n",
    "        angle_rates = tf.pow(300.0, -(2 * tf.floor(i / 2)) / tf.cast(self.embed_dim, tf.float32))\n",
    "        angle_rads = pos * angle_rates  # (pos_range, embed_dim)\n",
    "\n",
    "        # Apply sin to even indices, cos to odd indices\n",
    "        sines = tf.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)  # (max_len, embed_dim)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]  # (1, max_len, embed_dim)\n",
    "        self.pos_encoding = tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (B, N, D)\n",
    "            mask: Tensor of shape (B,N)\n",
    "        Returns:\n",
    "            Tensor with positional encodings added for masked and non padded tokens.\n",
    "        \"\"\"\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        pe = self.pos_encoding[:, :seq_len, :]  # (1, N, D)\n",
    "        mask = tf.cast(mask[:, :, tf.newaxis], tf.float32)  # (B, N, 1)\n",
    "        mask = tf.where(mask == self.pad_token, 0., 1.)\n",
    "        pe = pe * mask  # zero out positions where mask is 0\n",
    "\n",
    "        return x + pe\n",
    "\n",
    "class SubtractLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom layer to subtract a tensor from another tensor.\n",
    "    Tensor1: (B, P, D) -> (B, P*D) -> (B, M, P*D)\n",
    "    Tensor2: (B, M, D) -> (B, M, P*D)\n",
    "    Output: = Tensor2 - Tensor1\n",
    "    \"\"\"\n",
    "    def __init__(self, mask_token=-1., pad_token=-2., **kwargs):\n",
    "        \"\"\"Initialize the layer.\"\"\"\n",
    "        super(SubtractLayer, self).__init__(**kwargs)\n",
    "        self.mask_token = mask_token\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "    def call(self, peptide, pep_mask, mhc, mhc_mask):\n",
    "        B = tf.shape(peptide)[0]\n",
    "        P = tf.shape(peptide)[1]\n",
    "        D = tf.shape(peptide)[2]\n",
    "        M = tf.shape(mhc)[1]\n",
    "        P_D = P * D\n",
    "\n",
    "        pep_mask = tf.cast(pep_mask, tf.float32)\n",
    "        mhc_mask = tf.cast(mhc_mask, tf.float32)\n",
    "\n",
    "        pep_mask = tf.where(pep_mask == self.pad_token, x=0., y=1.)  # (B,P)\n",
    "        mhc_mask = tf.where(mhc_mask == self.pad_token, x=0., y=1.)\n",
    "\n",
    "        # peptide  (B,P,D) -> (B,P*D) -> (B,M,P*D)\n",
    "        peptide_flat = tf.reshape(peptide, (B, P_D))\n",
    "        peptide_exp = tf.repeat(peptide_flat[:, tf.newaxis, :], repeats=M, axis=1)\n",
    "        # mhc       (B,M,D) -> tile last axis P times -> (B,M,P*D)\n",
    "        mhc_exp = tf.tile(mhc, [1, 1, P])\n",
    "        result = mhc_exp - peptide_exp  # (B,M,P*D)\n",
    "        # peptide mask  (B,P) -> (B,P,D) -> flatten -> (B,P*D) -> (B,M,P*D)\n",
    "        pep_mask_PD = tf.tile(pep_mask[:, :, tf.newaxis], [1, 1, D])  # (B,P,D)\n",
    "        pep_mask_PD = tf.reshape(pep_mask_PD, (B, P_D))  # (B,P*D)\n",
    "        pep_mask_PD = tf.repeat(pep_mask_PD[:, tf.newaxis, :], repeats=M, axis=1)  # (B,M,P*D)\n",
    "        # mhc mask      (B,M) -> (B,M,1) -> repeat P*D along last axis\n",
    "        mhc_mask_PD = tf.repeat(mhc_mask[:, :, tf.newaxis], repeats=P_D, axis=2)  # (B,M,P*D)\n",
    "        combined_mask = tf.logical_and(tf.cast(pep_mask_PD, tf.bool), tf.cast(mhc_mask_PD, tf.bool))\n",
    "        masked_result = tf.where(combined_mask, result, tf.zeros_like(result))\n",
    "        return masked_result\n",
    "\n",
    "class AddGaussianNoise(layers.Layer):\n",
    "    def __init__(self, std=0.1, **kw): super().__init__(**kw); self.std = std\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if training: return x + tf.random.normal(tf.shape(x), stddev=self.std)\n",
    "        return x\n",
    "\n",
    "class AttentionLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom multi-head attention layer supporting self- and cross-attention.\n",
    "\n",
    "    Args:\n",
    "        query_dim (int): Input feature dimension for query.\n",
    "        context_dim (int): Input feature dimension for context (key and value).\n",
    "        output_dim (int): Output feature dimension.\n",
    "        type (str): 'self' or 'cross'.\n",
    "        heads (int): Number of attention heads.\n",
    "        resnet (bool): Whether to use residual connection.\n",
    "        return_att_weights (bool): Whether to return attention weights.\n",
    "        name (str): Layer name.\n",
    "        epsilon (float): Epsilon for layer normalization.\n",
    "        gate (bool): Whether to use gating mechanism.\n",
    "        mask_token (float): Value for masked tokens.\n",
    "        pad_token (float): Value for padded tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, query_dim, context_dim, output_dim, type, heads=4,\n",
    "                 resnet=True, return_att_weights=False, name='attention',\n",
    "                 epsilon=1e-6, gate=True, mask_token=-1., pad_token=-2.):\n",
    "        super().__init__(name=name)\n",
    "        assert isinstance(query_dim, int) and isinstance(context_dim, int) and isinstance(output_dim, int)\n",
    "        assert type in ['self', 'cross']\n",
    "        if resnet:\n",
    "            assert query_dim == output_dim\n",
    "        self.query_dim = query_dim\n",
    "        self.context_dim = context_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.type = type\n",
    "        self.heads = heads\n",
    "        self.resnet = resnet\n",
    "        self.return_att_weights = return_att_weights\n",
    "        self.epsilon = epsilon\n",
    "        self.gate = gate\n",
    "        self.mask_token = mask_token\n",
    "        self.pad_token = pad_token\n",
    "        self.att_dim = output_dim // heads  # Attention dimension per head\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Projection weights\n",
    "        self.q_proj = self.add_weight(shape=(self.heads, self.query_dim, self.att_dim),\n",
    "                                      initializer='random_normal', trainable=True, name=f'q_proj_{self.name}')\n",
    "        self.k_proj = self.add_weight(shape=(self.heads, self.context_dim, self.att_dim),\n",
    "                                      initializer='random_normal', trainable=True, name=f'k_proj_{self.name}')\n",
    "        self.v_proj = self.add_weight(shape=(self.heads, self.context_dim, self.att_dim),\n",
    "                                      initializer='random_normal', trainable=True, name=f'v_proj_{self.name}')\n",
    "        if self.gate:\n",
    "            self.g = self.add_weight(shape=(self.heads, self.query_dim, self.att_dim),\n",
    "                                     initializer='random_uniform', trainable=True, name=f'gate_{self.name}')\n",
    "        self.norm = layers.LayerNormalization(epsilon=self.epsilon, name=f'ln_{self.name}')\n",
    "        if self.type == 'cross':\n",
    "            self.norm_context = layers.LayerNormalization(epsilon=self.epsilon, name=f'ln_context_{self.name}')\n",
    "        self.norm_out = layers.LayerNormalization(epsilon=self.epsilon, name=f'ln_out_{self.name}')\n",
    "        if self.resnet:\n",
    "            self.norm_resnet = layers.LayerNormalization(epsilon=self.epsilon, name=f'ln_resnet_{self.name}')\n",
    "        self.out_w = self.add_weight(shape=(self.heads * self.att_dim, self.output_dim),\n",
    "                                     initializer='random_normal', trainable=True, name=f'outw_{self.name}')\n",
    "        self.out_b = self.add_weight(shape=(self.output_dim,), initializer='zeros',\n",
    "                                     trainable=True, name=f'outb_{self.name}')\n",
    "        self.scale = 1.0 / tf.math.sqrt(tf.cast(self.att_dim, tf.float32))\n",
    "\n",
    "    def call(self, x, mask, context=None, context_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (B, N, query_dim) for query.\n",
    "            mask: Tensor of shape (B, N).\n",
    "            context: Tensor of shape (B, M, context_dim) for key/value in cross-attention.\n",
    "            context_mask: Tensor of shape (B, M) for context.\n",
    "        \"\"\"\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        if self.type == 'self':\n",
    "            context = x\n",
    "            context_mask = mask\n",
    "            q_input = k_input = v_input = self.norm(x)\n",
    "            mask_q = mask_k = tf.where(mask == self.pad_token, 0., 1.)\n",
    "        else:\n",
    "            assert context is not None and context_mask is not None\n",
    "            q_input = self.norm(x)\n",
    "            k_input = v_input = self.norm_context(context)\n",
    "            mask_q = tf.where(mask == self.pad_token, 0., 1.)\n",
    "            mask_k = tf.where(context_mask == self.pad_token, 0., 1.)\n",
    "\n",
    "        # Project query, key, value\n",
    "        q = tf.einsum('bnd,hde->bhne', q_input, self.q_proj)\n",
    "        k = tf.einsum('bmd,hde->bhme', k_input, self.k_proj)\n",
    "        v = tf.einsum('bmd,hde->bhme', v_input, self.v_proj)\n",
    "\n",
    "        # Compute attention scores\n",
    "        att = tf.einsum('bhne,bhme->bhnm', q, k) * self.scale\n",
    "        mask_q_exp = tf.expand_dims(mask_q, axis=1)\n",
    "        mask_k_exp = tf.expand_dims(mask_k, axis=1)\n",
    "        attention_mask = tf.einsum('bqn,bkm->bqnm', mask_q_exp, mask_k_exp)\n",
    "        attention_mask = tf.broadcast_to(attention_mask, tf.shape(att))\n",
    "        att += (1.0 - attention_mask) * -1e9\n",
    "        att = tf.nn.softmax(att, axis=-1) * attention_mask\n",
    "\n",
    "        # Compute output\n",
    "        out = tf.einsum('bhnm,bhme->bhne', att, v)\n",
    "        if self.gate:\n",
    "            g = tf.einsum('bnd,hde->bhne', q_input, self.g)\n",
    "            g = tf.nn.sigmoid(g)\n",
    "            out *= g\n",
    "\n",
    "        out = tf.transpose(out, [0, 2, 1, 3])\n",
    "        out = tf.reshape(out, [tf.shape(x)[0], tf.shape(x)[1], self.heads * self.att_dim])\n",
    "        out = tf.matmul(out, self.out_w) + self.out_b\n",
    "\n",
    "        if self.resnet:\n",
    "            out += x\n",
    "            out = self.norm_resnet(out)\n",
    "        out = self.norm_out(out)\n",
    "        mask_exp = tf.expand_dims(mask_q, axis=-1)\n",
    "        out *= mask_exp\n",
    "\n",
    "        return (out, att) if self.return_att_weights else out"
   ],
   "id": "13cbed3d7551725b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T20:29:53.683225Z",
     "start_time": "2025-08-21T20:29:53.520017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class Expert(layers.Layer):\n",
    "    \"\"\"A binary prediction expert with added complexity.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1, dropout_rate=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Note: do NOT rely on `input_shape=` during subclassing; we build explicitly.\n",
    "        self.fc1 = layers.Dense(hidden_dim, activation=\"relu\", name=\"fc1\")\n",
    "        self.dropout1 = layers.Dropout(dropout_rate, name=\"dropout1\")\n",
    "        self.fc2 = layers.Dense(hidden_dim // 2, activation=\"relu\", name=\"fc2\")\n",
    "        self.dropout2 = layers.Dropout(dropout_rate, name=\"dropout2\")\n",
    "        self.fc3 = layers.Dense(output_dim, name=\"fc3\")\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        batch_dim = input_shape[0]\n",
    "        # Explicitly build inner layers so their weights exist before access\n",
    "        self.fc1.build((batch_dim, self.input_dim))           # (None, input_dim) -> (None, hidden_dim)\n",
    "        self.dropout1.build((batch_dim, self.hidden_dim))\n",
    "        self.fc2.build((batch_dim, self.hidden_dim))          # (None, hidden_dim) -> (None, hidden_dim//2)\n",
    "        self.dropout2.build((batch_dim, self.hidden_dim // 2))\n",
    "        self.fc3.build((batch_dim, self.hidden_dim // 2))     # -> (None, output_dim)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EnhancedMixtureOfExperts(layers.Layer):\n",
    "    \"\"\"\n",
    "    Enhanced Mixture of Experts layer that uses cluster assignments.\n",
    "\n",
    "    - During training: use hard clustering (one-hot) to route to a specific expert per sample\n",
    "    - During inference: use soft clustering to mix experts' weights per sample\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, num_experts, output_dim=1, dropout_rate=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.experts = [\n",
    "            Expert(input_dim, hidden_dim, output_dim, dropout_rate, name=f\"expert_{i}\")\n",
    "            for i in range(num_experts)\n",
    "        ]\n",
    "        self.dropout1 = layers.Dropout(dropout_rate, name=\"moe_dropout1\")\n",
    "        self.dropout2 = layers.Dropout(dropout_rate, name=\"moe_dropout2\")\n",
    "\n",
    "        # Optional (helps Keras validate inputs)\n",
    "        self.input_spec = [\n",
    "            layers.InputSpec(min_ndim=2, axes={-1: self.input_dim}),\n",
    "            layers.InputSpec(min_ndim=2, axes={-1: self.num_experts}),\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_hard_clustering(soft_clusters):\n",
    "        \"\"\"Convert soft clustering values to hard clustering (one-hot encoding).\"\"\"\n",
    "        hard_indices = tf.argmax(soft_clusters, axis=1)\n",
    "        return tf.one_hot(hard_indices, depth=tf.shape(soft_clusters)[-1])\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape is a pair: (x_shape, soft_cluster_probs_shape)\n",
    "        if not (isinstance(input_shape, (tuple, list)) and len(input_shape) == 2):\n",
    "            raise ValueError(\"Inputs must include both features and clustering values: (x, soft_cluster_probs)\")\n",
    "        x_shape, _ = input_shape\n",
    "        batch_dim = x_shape[0]\n",
    "\n",
    "        # Build all experts so their Dense weights exist before we access them in `call`.\n",
    "        for expert in self.experts:\n",
    "            expert.build((batch_dim, self.input_dim))\n",
    "\n",
    "        # Build dropouts on their expected activation sizes\n",
    "        self.dropout1.build((batch_dim, self.hidden_dim))\n",
    "        self.dropout2.build((batch_dim, self.hidden_dim // 2))\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # Keras can sometimes infer this, but we make it explicit for safety.\n",
    "        x_shape, _ = input_shape\n",
    "        return tf.TensorShape([x_shape[0], self.output_dim])\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        if not (isinstance(inputs, (tuple, list)) and len(inputs) == 2):\n",
    "            raise ValueError(\"Inputs must include both features and clustering values: (x, soft_cluster_probs)\")\n",
    "        x, soft_cluster_probs = inputs\n",
    "\n",
    "        # Keep dtypes consistent with layer policy\n",
    "        target_dtype = self.dtype or tf.float32\n",
    "        x = tf.cast(x, target_dtype)\n",
    "        soft_cluster_probs = tf.cast(soft_cluster_probs, target_dtype)\n",
    "\n",
    "        # Hard routing on train, soft mixing on inference\n",
    "        if training:\n",
    "            expert_activation_probs = tf.cast(self.convert_to_hard_clustering(soft_cluster_probs), target_dtype)\n",
    "        else:\n",
    "            expert_activation_probs = soft_cluster_probs\n",
    "\n",
    "        # Zero tiny weights; then (re)normalize to avoid numerical drift\n",
    "        expert_activation_probs = tf.where(expert_activation_probs < 1e-5, 0.0, expert_activation_probs)\n",
    "        sum_probs = tf.reduce_sum(expert_activation_probs, axis=1, keepdims=True)\n",
    "        expert_activation_probs = tf.where(sum_probs > 0, expert_activation_probs / sum_probs, expert_activation_probs)\n",
    "\n",
    "        batch_dim = tf.shape(x)[0]\n",
    "\n",
    "        # Stack expert weights\n",
    "        # Shapes:\n",
    "        #  fc1: (E, I, H) / (E, H)\n",
    "        #  fc2: (E, H, H2) / (E, H2)\n",
    "        #  fc3: (E, H2, O) / (E, O)\n",
    "        stacked_kernel1 = tf.stack([e.fc1.kernel for e in self.experts], axis=0)\n",
    "        stacked_bias1 = tf.stack([e.fc1.bias for e in self.experts], axis=0)\n",
    "        stacked_kernel2 = tf.stack([e.fc2.kernel for e in self.experts], axis=0)\n",
    "        stacked_bias2 = tf.stack([e.fc2.bias for e in self.experts], axis=0)\n",
    "        stacked_kernel3 = tf.stack([e.fc3.kernel for e in self.experts], axis=0)\n",
    "        stacked_bias3 = tf.stack([e.fc3.bias for e in self.experts], axis=0)\n",
    "\n",
    "        # Mix weights per-sample using the routing probabilities\n",
    "        mixed_kernel1 = tf.einsum(\"be,eih->bih\", expert_activation_probs, stacked_kernel1)\n",
    "        mixed_bias1   = tf.einsum(\"be,eh->bh\",   expert_activation_probs, stacked_bias1)\n",
    "        mixed_kernel2 = tf.einsum(\"be,ehj->bhj\", expert_activation_probs, stacked_kernel2)\n",
    "        mixed_bias2   = tf.einsum(\"be,ej->bj\",   expert_activation_probs, stacked_bias2)\n",
    "        mixed_kernel3 = tf.einsum(\"be,ejk->bjk\", expert_activation_probs, stacked_kernel3)\n",
    "        mixed_bias3   = tf.einsum(\"be,ek->bk\",   expert_activation_probs, stacked_bias3)\n",
    "\n",
    "        # Forward pass using the mixed weights (per-sample learned linear layers)\n",
    "        out = tf.einsum(\"bi,bih->bh\", x, mixed_kernel1) + mixed_bias1\n",
    "        out = tf.nn.relu(out)\n",
    "        out = self.dropout1(out, training=training)\n",
    "\n",
    "        out = tf.einsum(\"bh,bhj->bj\", out, mixed_kernel2) + mixed_bias2\n",
    "        out = tf.nn.relu(out)\n",
    "        out = self.dropout2(out, training=training)\n",
    "\n",
    "        out = tf.einsum(\"bj,bjk->bk\", out, mixed_kernel3) + mixed_bias3\n",
    "        return out\n",
    "\n",
    "    def get_config(self):\n",
    "        base = super().get_config()\n",
    "        base.update({\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"hidden_dim\": self.hidden_dim,\n",
    "            \"num_experts\": self.num_experts,\n",
    "            \"output_dim\": self.output_dim,\n",
    "        })\n",
    "        return base\n",
    "\n",
    "\n",
    "# --- Example usage ---\n",
    "x_in = tf.keras.Input(shape=(42,), name=\"features\")\n",
    "p_in = tf.keras.Input(shape=(16,), name=\"cluster_probs\")\n",
    "moe = EnhancedMixtureOfExperts(input_dim=42, hidden_dim=64, num_experts=16, output_dim=1)\n",
    "y = moe((x_in, p_in))\n",
    "model = tf.keras.Model([x_in, p_in], y)\n",
    "model.summary()\n"
   ],
   "id": "6bdf59a5a982c539",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"functional_3\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape     \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m   Param #\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mConnected to     \u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ features            │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m42\u001B[0m)        │          \u001B[38;5;34m0\u001B[0m │ -                 │\n",
       "│ (\u001B[38;5;33mInputLayer\u001B[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cluster_probs       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m16\u001B[0m)        │          \u001B[38;5;34m0\u001B[0m │ -                 │\n",
       "│ (\u001B[38;5;33mInputLayer\u001B[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enhanced_mixture_o… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)         │     \u001B[38;5;34m77,840\u001B[0m │ features[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],   │\n",
       "│ (\u001B[38;5;33mEnhancedMixtureOf…\u001B[0m │                   │            │ cluster_probs[\u001B[38;5;34m0\u001B[0m]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ features            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cluster_probs       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enhanced_mixture_o… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">77,840</span> │ features[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EnhancedMixtureOf…</span> │                   │            │ cluster_probs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m77,840\u001B[0m (304.06 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">77,840</span> (304.06 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m77,840\u001B[0m (304.06 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">77,840</span> (304.06 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T20:29:55.226579Z",
     "start_time": "2025-08-21T20:29:55.218649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def masked_categorical_crossentropy(y_true_y_pred, mask, pad_token=-2.0):\n",
    "#     \"\"\"\n",
    "#     Compute masked categorical cross-entropy loss.\n",
    "#\n",
    "#     Args:\n",
    "#         y_true: True labels (tensor).\n",
    "#         y_pred: Predicted probabilities (tensor).\n",
    "#         mask: Mask tensor indicating positions to include in the loss.\n",
    "#\n",
    "#     Returns:\n",
    "#         Mean masked loss (tensor).\n",
    "#     \"\"\"\n",
    "#     y_true, y_pred = tf.split(y_true_y_pred, num_or_size_splits=2, axis=-1)\n",
    "#     # loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "#     loss = -tf.reduce_sum(y_true * tf.math.log(tf.clip_by_value(y_pred, 1e-7, 1.0)), axis=-1) #(B,N)\n",
    "#     mask = tf.cast(mask, tf.float32)  # Ensure mask is float\n",
    "#     mask = tf.where(mask == pad_token, 0.0, 1.0)  # Convert pad token to 0.0 and others to 1.0\n",
    "#     if tf.rank(mask) > tf.rank(loss):\n",
    "#         mask = tf.squeeze(mask, axis=-1)\n",
    "#     loss = loss * mask\n",
    "#     loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "#     return loss\n",
    "\n",
    "def masked_categorical_crossentropy(y_true_and_pred, mask, pad_token=-2.0, sample_weight=None):\n",
    "    \"\"\"\n",
    "    Compute masked categorical cross-entropy loss.\n",
    "\n",
    "    Args:\n",
    "        y_true_and_pred: Concatenated tensor of true labels and predictions.\n",
    "        mask: Mask tensor indicating positions to include in the loss.\n",
    "        pad_token: Value of the padding token in the mask.\n",
    "        sample_weight: Optional tensor of shape (B, 1) or (B,) to weight samples.\n",
    "\n",
    "    Returns:\n",
    "        Mean masked loss (tensor).\n",
    "    \"\"\"\n",
    "    y_true, y_pred = tf.split(y_true_and_pred, num_or_size_splits=2, axis=-1)\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.0)\n",
    "    loss = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=-1)  # (B, N)\n",
    "\n",
    "    # Build a 0/1 mask for non-pad tokens\n",
    "    mask = tf.cast(tf.not_equal(mask, pad_token), tf.float32)\n",
    "\n",
    "    # If mask has an extra trailing dim of 1, squeeze it (static check only)\n",
    "    if mask.shape.rank is not None and mask.shape.rank > 2 and mask.shape[-1] == 1:\n",
    "        mask = tf.squeeze(mask, axis=-1)\n",
    "\n",
    "    # Ensure shape compatibility with loss\n",
    "    mask = tf.cast(tf.broadcast_to(mask, tf.shape(loss)), tf.float32)\n",
    "\n",
    "    masked_loss = loss * mask\n",
    "\n",
    "    # Apply sample weights if provided\n",
    "    if sample_weight is not None:\n",
    "        sample_weight = tf.cast(sample_weight, tf.float32)\n",
    "        if sample_weight.shape.rank == 2 and sample_weight.shape[1] == 1:\n",
    "            sample_weight = tf.squeeze(sample_weight, axis=-1) # (B,)\n",
    "        # Broadcast sample_weight from (B,) to (B, N) to match masked_loss\n",
    "        masked_loss *= sample_weight[:, tf.newaxis]\n",
    "        mask *= sample_weight[:, tf.newaxis]\n",
    "\n",
    "    total_loss = tf.reduce_sum(masked_loss)\n",
    "    total_weight = tf.reduce_sum(mask)\n",
    "    return tf.math.divide_no_nan(total_loss, total_weight)\n",
    "\n",
    "class GlobalAveragePooling1D(layers.Layer):\n",
    "    \"\"\"\n",
    "    Global Average Pooling layer that computes the mean across the sequence dimension for each feature.\n",
    "    Args:\n",
    "        name (str): Layer name.\n",
    "    \"\"\"\n",
    "    def __init__(self, name='global_avg_pooling'):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: Input tensor of shape (B, N, D).\n",
    "        Returns:\n",
    "            Tensor of shape (B, N) containing the mean across D.\n",
    "        \"\"\"\n",
    "        pooled_avg = tf.math.reduce_mean(\n",
    "            input_tensor, axis=1, keepdims=False\n",
    "            )\n",
    "        return pooled_avg\n",
    "\n",
    "class GlobalSTDPooling1D(layers.Layer):\n",
    "    \"\"\"\n",
    "    Global Standard Deviation Pooling layer that computes the standard deviation\n",
    "    across the sequence dimension for each feature.\n",
    "    Args:\n",
    "        name (str): Layer name.\n",
    "    \"\"\"\n",
    "    def __init__(self, name='global_std_pooling'):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: Input tensor of shape (B, N, D).\n",
    "        Returns:\n",
    "            Tensor of shape (B, N) containing the standard deviation across D.\n",
    "        \"\"\"\n",
    "        pooled_std = tf.math.reduce_std(\n",
    "            input_tensor, axis=1, keepdims=False, name=None\n",
    "            )\n",
    "        return pooled_std\n"
   ],
   "id": "7c58a59124de59b",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T20:29:55.409095Z",
     "start_time": "2025-08-21T20:29:55.396306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# model\n",
    "from tensorflow.keras import layers\n",
    "MASK_TOKEN = -1.0\n",
    "PAD_TOKEN = -2.0\n",
    "\n",
    "\n",
    "def pmbind_subtract_moe_auto(max_pep_len: int,\n",
    "                               max_mhc_len: int,\n",
    "                               emb_dim: int = 96,\n",
    "                               heads: int = 4,\n",
    "                               noise_std: float = 0.1,\n",
    "                               num_experts: int = 30,\n",
    "                               mask_token: float = MASK_TOKEN,\n",
    "                               pad_token: float = PAD_TOKEN):\n",
    "    \"\"\"\n",
    "    Builds a pMHC autoencoder model with a Mixture-of-Experts (MoE) classifier head.\n",
    "\n",
    "    This model performs two tasks:\n",
    "    1. Autoencoding: Reconstructs peptide and MHC sequences from a latent representation.\n",
    "    2. Classification: Predicts a binary label using an MoE head, where experts are\n",
    "       selected based on an internally generated clustering of the latent space.\n",
    "    \"\"\"\n",
    "    # -------------------------------------------------------------------\n",
    "    # INPUTS\n",
    "    # -------------------------------------------------------------------\n",
    "    pep_OHE_in = keras.Input((max_pep_len, 21), name=\"pep_onehot\")\n",
    "    pep_mask_in = keras.Input((max_pep_len,), name=\"pep_mask\")\n",
    "    mhc_emb_in = keras.Input((max_mhc_len, 1152), name=\"mhc_emb\")\n",
    "    mhc_mask_in = keras.Input((max_mhc_len,), name=\"mhc_mask\")\n",
    "    mhc_OHE_in = keras.Input((max_mhc_len, 21), name=\"mhc_onehot\")\n",
    "    # -------------------------------------------------------------------\n",
    "    # MASKED  EMBEDDING  +  PE\n",
    "    # -------------------------------------------------------------------\n",
    "    pep = MaskedEmbedding(mask_token, pad_token, name=\"pep_mask2\")(pep_OHE_in, pep_mask_in)\n",
    "    pep = PositionalEncoding(21, int(max_pep_len * 3), name=\"pep_pos1\")(pep, pep_mask_in)\n",
    "    pep = layers.Dense(emb_dim, name=\"pep_Dense1\")(pep)\n",
    "    pep = layers.Dropout(0.1, name=\"pep_Dropout1\")(pep)\n",
    "\n",
    "    mhc = MaskedEmbedding(mask_token, pad_token, name=\"mhc_mask2\")(mhc_emb_in, mhc_mask_in)\n",
    "    mhc = PositionalEncoding(1152, int(max_mhc_len * 3), name=\"mhc_pos1\")(mhc, mhc_mask_in)\n",
    "    mhc = layers.Dense(emb_dim, name=\"mhc_dense1\")(mhc)\n",
    "    mhc = layers.Dropout(0.1, name=\"mhc_Dropout1\")(mhc)\n",
    "    # -------------------------------------------------------------------\n",
    "    # Subtract Layer\n",
    "    # -------------------------------------------------------------------\n",
    "    mhc_subtracted_p = SubtractLayer(name=\"pmhc_subtract\")(pep, pep_mask_in, mhc, mhc_mask_in) # (B, M, P*D) = mhc_expanded – peptide_expanded\n",
    "    #tf.print(\"mhc_subtracted_p shape:\", mhc_subtracted_p.shape)\n",
    "    # -------------------------------------------------------------------\n",
    "    # Add Gaussian Noise\n",
    "    # -------------------------------------------------------------------\n",
    "    mhc_subtracted_p = AddGaussianNoise(noise_std, name=\"pmhc_gaussian_noise\")(mhc_subtracted_p)\n",
    "    query_dim = int(emb_dim*max_pep_len)\n",
    "    # # -------------------------------------------------------------------\n",
    "    # Normal Self-Attention Layer\n",
    "    # # -------------------------------------------------------------------\n",
    "    mhc_subtracted_p_attn, mhc_subtracted_p_attn_scores = AttentionLayer(\n",
    "        query_dim=query_dim, context_dim=query_dim, output_dim=query_dim,\n",
    "        type=\"self\", heads=heads, resnet=True,\n",
    "        return_att_weights=True, name='mhc_subtracted_p_attn',\n",
    "        mask_token=mask_token,\n",
    "        pad_token=pad_token\n",
    "    )(mhc_subtracted_p, mhc_mask_in)\n",
    "    peptide_cross_att, peptide_cross_attn_scores = AttentionLayer(\n",
    "        query_dim=int(emb_dim), context_dim=query_dim, output_dim=int(emb_dim),\n",
    "        type=\"cross\", heads=heads, resnet=False,\n",
    "        return_att_weights=True, name='peptide_cross_att',\n",
    "        mask_token=mask_token,\n",
    "        pad_token=pad_token\n",
    "    )(pep, pep_mask_in, mhc_subtracted_p_attn, mhc_mask_in)\n",
    "\n",
    "    # --- Encoder ---\n",
    "    latent_sequence = layers.Dense(emb_dim*max_pep_len * 2, activation='relu', name='latent_mhc_dense1')(mhc_subtracted_p_attn)\n",
    "    latent_sequence = layers.Dropout(0.2, name='latent_mhc_dropout1')(latent_sequence)\n",
    "    latent_sequence = layers.Dense(emb_dim, activation='relu', name='cross_latent')(latent_sequence) # Shape: (B, M, D)\n",
    "\n",
    "    # --- Latent Vector for Clustering (pooled) ---\n",
    "    avg_pooled = GlobalAveragePooling1D(name=\"avg_pooled\")(peptide_cross_att) # Shape: (B, D)\n",
    "    std_pooled = GlobalSTDPooling1D(name=\"std_pooled\")(peptide_cross_att) # Shape: (B, D)\n",
    "    latent_vector = layers.Concatenate(name=\"latent_vector_concat\", axis=-1)([avg_pooled, std_pooled])  # Shape: (B, D*2)\n",
    "\n",
    "    # --- Reconstruction Heads ---\n",
    "    mhc_recon_head = layers.Dropout(0.2, name='latent_mhc_dropout2')(latent_sequence)\n",
    "    mhc_recon = layers.Dense(21, activation='softmax', name='mhc_reconstruction_pred')(mhc_recon_head)\n",
    "    pep_recon = layers.Dense(emb_dim, activation='relu', name='pep_latent')(peptide_cross_att)\n",
    "    pep_recon = layers.Dense(21, activation='softmax', name='pep_reconstruction_pred')(pep_recon)\n",
    "\n",
    "    pep_out = layers.Concatenate(name='pep_ytrue_ypred', axis=-1)([pep_OHE_in, pep_recon]) #(B,P,42)\n",
    "    mhc_out = layers.Concatenate(name='mhc_ytrue_ypred', axis=-1)([mhc_OHE_in, mhc_recon]) #(B,M,42)\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # CLASSIFIER HEAD (MIXTURE OF EXPERTS)\n",
    "    # -------------------------------------------------------------------\n",
    "    # 1. Gating network: Generate soft cluster assignments from the latent vector\n",
    "    bigger_probs = layers.Dense(num_experts * 2, activation='relu', name='gating_network_dense1')(latent_vector)\n",
    "    bigger_probs = layers.Dropout(0.2, name='gating_network_dropout1')(bigger_probs)\n",
    "    soft_cluster_probs = layers.Dense(num_experts, activation='softmax', name='gating_network_softmax')(bigger_probs)\n",
    "\n",
    "    # 2. MoE layer: Get weighted prediction from experts\n",
    "    moe_layer = EnhancedMixtureOfExperts(\n",
    "        input_dim=emb_dim*2,\n",
    "        hidden_dim=emb_dim // 2,\n",
    "        num_experts=num_experts,\n",
    "        output_dim=1,\n",
    "        dropout_rate=0.2\n",
    "    )\n",
    "    y_pred = moe_layer((latent_vector, soft_cluster_probs))\n",
    "    y_pred = layers.Activation('sigmoid', name='cls_ypred')(y_pred)\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # MODEL DEFINITION\n",
    "    # -------------------------------------------------------------------\n",
    "    model = keras.Model(\n",
    "        inputs=[pep_OHE_in, pep_mask_in, mhc_emb_in, mhc_mask_in, mhc_OHE_in],\n",
    "        outputs={\n",
    "            \"pep_ytrue_ypred\": pep_out,\n",
    "            \"mhc_ytrue_ypred\": mhc_out,\n",
    "            \"latent_vector\": latent_vector,\n",
    "            \"cls_ypred\": y_pred,\n",
    "        },\n",
    "        name=\"pmbind_subtract_moe_autoencoder\"\n",
    "    )\n",
    "\n",
    "    return model"
   ],
   "id": "a9e84e3ff1ccbd9c",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T20:30:07.132738Z",
     "start_time": "2025-08-21T20:29:55.667557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# 3. DUMMY DATA GENERATION (for a runnable example)\n",
    "# ==============================================================================\n",
    "# Reproducible, structured synthetic dataset with correlated labels\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "num_samples = 2000\n",
    "pep_len = 14                  # max peptide length\n",
    "mhc_len = 312\n",
    "pep_alphabet_size = 21        # 20 AA + gap/UNK\n",
    "mhc_embedding_dim = 1152\n",
    "\n",
    "# 1) Peptide sequences with variable lengths (8..14), realistic AA frequencies, and masks\n",
    "aa_probs = np.array([\n",
    "    0.0825, 0.0150, 0.0545, 0.0685, 0.0395, 0.0705, 0.0225, 0.0590, 0.0900, 0.0150,\n",
    "    0.0600, 0.0240, 0.0400, 0.0380, 0.0470, 0.0560, 0.0510, 0.0730, 0.0130, 0.0320,\n",
    "    0.0020  # gap/UNK rare\n",
    "], dtype=np.float32)\n",
    "aa_probs = aa_probs / aa_probs.sum()\n",
    "\n",
    "pep_lengths = np.random.randint(8, pep_len + 1, size=(num_samples,))\n",
    "pep_indices = np.full((num_samples, pep_len), fill_value=20, dtype=np.int32)  # init with gap\n",
    "pep_mask_np = np.full((num_samples, pep_len), fill_value=-2.0, dtype=np.float32)  # pad_token\n",
    "\n",
    "for i in range(num_samples):\n",
    "    L = pep_lengths[i]\n",
    "    pep_indices[i, :L] = np.random.choice(pep_alphabet_size, size=L, p=aa_probs)\n",
    "    pep_mask_np[i, :L] = 1.0\n",
    "    # randomly mask up to 2 tokens within valid region\n",
    "    num_masked = np.random.randint(0, 3)\n",
    "    if num_masked > 0 and L > 0:\n",
    "        mask_pos = np.random.choice(L, size=num_masked, replace=False)\n",
    "        pep_mask_np[i, mask_pos] = -1.0  # mask_token\n",
    "\n",
    "pep_OHE = tf.one_hot(pep_indices, depth=pep_alphabet_size, dtype=tf.float32)\n",
    "pep_mask = tf.constant(pep_mask_np, dtype=tf.float32)\n",
    "\n",
    "# 2) MHC embeddings with allele structure + MHC OHE with allele-specific residue distributions\n",
    "num_alleles = 6\n",
    "allele_ids = np.random.randint(0, num_alleles, size=(num_samples, 1)).astype(np.int32)\n",
    "\n",
    "# allele base vectors and biases to induce signal\n",
    "allele_base = np.random.normal(0, 1.0, size=(num_alleles, mhc_embedding_dim)).astype(np.float32)\n",
    "allele_bias = np.random.normal(0, 0.5, size=(num_alleles,)).astype(np.float32)\n",
    "\n",
    "# allele-specific residue distributions (Dirichlet)\n",
    "residue_dir = 2.0 * np.ones((num_alleles, pep_alphabet_size), dtype=np.float32)\n",
    "allele_res_probs = np.random.dirichlet(alpha=residue_dir[0], size=1)[0]\n",
    "allele_res_probs = np.vstack([\n",
    "    np.random.dirichlet(alpha=residue_dir[k]) for k in range(num_alleles)\n",
    "]).astype(np.float32)\n",
    "\n",
    "# build MHC embeddings and OHE indices\n",
    "mhc_emb_np = np.zeros((num_samples, mhc_len, mhc_embedding_dim), dtype=np.float32)\n",
    "mhc_indices = np.zeros((num_samples, mhc_len), dtype=np.int32)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    a = allele_ids[i, 0]\n",
    "    base_vec = allele_base[a]  # (D,)\n",
    "    # per-position small noise around allele base\n",
    "    mhc_emb_np[i] = base_vec[None, :] + 0.1 * np.random.normal(0, 1.0, size=(mhc_len, mhc_embedding_dim)).astype(np.float32)\n",
    "    # residue per-position following allele-specific distribution\n",
    "    mhc_indices[i] = np.random.choice(pep_alphabet_size, size=mhc_len, p=allele_res_probs[a])\n",
    "\n",
    "mhc_emb = tf.constant(mhc_emb_np, dtype=tf.float32)\n",
    "mhc_OHE = tf.one_hot(mhc_indices, depth=pep_alphabet_size, dtype=tf.float32)\n",
    "\n",
    "# 3) MHC mask: trailing padding (0..10)\n",
    "pad_token = -2.0\n",
    "mask_token = -1.0\n",
    "mhc_mask_np = np.ones((num_samples, mhc_len), dtype=np.float32)\n",
    "for i in range(num_samples):\n",
    "    num_padded = np.random.randint(0, 11)\n",
    "    if num_padded > 0:\n",
    "        mhc_mask_np[i, -num_padded:] = pad_token\n",
    "mhc_mask = tf.constant(mhc_mask_np, dtype=tf.float32)\n",
    "\n",
    "# 4) Zero out masked/padded positions in inputs\n",
    "pep_bool_mask = tf.cast(pep_mask > 0, dtype=tf.float32)\n",
    "mhc_bool_mask = tf.cast(mhc_mask > 0, dtype=tf.float32)\n",
    "pep_OHE = pep_OHE * pep_bool_mask[:, :, tf.newaxis]\n",
    "mhc_emb = mhc_emb * mhc_bool_mask[:, :, tf.newaxis]\n",
    "\n",
    "# 5) Create labels correlated with peptide and MHC embeddings\n",
    "latent_dim = 32\n",
    "Wp = tf.random.normal((pep_alphabet_size, latent_dim), stddev=0.5)\n",
    "Wm = tf.random.normal((mhc_embedding_dim, latent_dim), stddev=0.1)\n",
    "\n",
    "# pooled peptide feature\n",
    "pep_feat = tf.tensordot(pep_OHE, Wp, axes=[[2], [0]])                      # (B, P, Ld)\n",
    "pep_valid = pep_bool_mask[:, :, None]                                      # (B, P, 1)\n",
    "pep_pool = tf.math.divide_no_nan(tf.reduce_sum(pep_feat * pep_valid, axis=1),\n",
    "                                 tf.reduce_sum(pep_valid, axis=1))         # (B, Ld)\n",
    "\n",
    "# pooled MHC feature\n",
    "mhc_feat = tf.tensordot(mhc_emb, Wm, axes=[[2], [0]])                      # (B, M, Ld)\n",
    "mhc_valid = mhc_bool_mask[:, :, None]                                      # (B, M, 1)\n",
    "mhc_pool = tf.math.divide_no_nan(tf.reduce_sum(mhc_feat * mhc_valid, axis=1),\n",
    "                                 tf.reduce_sum(mhc_valid, axis=1))         # (B, Ld)\n",
    "\n",
    "# interaction + allele bias -> probability -> labels\n",
    "interaction = tf.reduce_sum(pep_pool * mhc_pool, axis=1, keepdims=True)    # (B,1)\n",
    "allele_bias_tf = tf.constant(allele_bias, dtype=tf.float32)\n",
    "allele_ids_tf = tf.constant(allele_ids, dtype=tf.int32)\n",
    "bias_term = tf.gather(allele_bias_tf, tf.squeeze(allele_ids_tf, axis=1))[:, None]\n",
    "logits = interaction + bias_term + tf.random.normal((num_samples, 1), stddev=0.3)\n",
    "probs = tf.sigmoid(logits)\n",
    "y_true = tf.cast(tf.random.uniform((num_samples, 1)) < probs, tf.float32)\n",
    "\n",
    "# 5b) Introduce a slight pattern shift for negative samples\n",
    "neg_mask = tf.squeeze(1.0 - y_true, axis=1)                      # (B,)\n",
    "neg_mask_3d = neg_mask[:, None, None]                            # (B,1,1)\n",
    "\n",
    "# Add a small sinusoidal pattern to MHC embeddings of negatives\n",
    "pattern = tf.sin(tf.linspace(0.0, tf.constant(2.0 * np.pi, dtype=tf.float32), mhc_embedding_dim))\n",
    "pattern = tf.reshape(pattern, (1, 1, mhc_embedding_dim))         # (1,1,D)\n",
    "mhc_emb = mhc_emb + 0.05 * neg_mask_3d * pattern\n",
    "\n",
    "# Softly bias MHC OHE towards a fixed residue for negatives (soft labels)\n",
    "alpha = 0.05\n",
    "bias_residue = tf.one_hot(3, depth=pep_alphabet_size, dtype=tf.float32)     # choose residue index 3 as bias\n",
    "bias_dist = tf.reshape(bias_residue, (1, 1, pep_alphabet_size))             # (1,1,21)\n",
    "mhc_OHE = (1.0 - alpha * neg_mask_3d) * mhc_OHE + (alpha * neg_mask_3d) * bias_dist\n",
    "\n",
    "# Re-apply masks to keep padded/invalid positions zeroed\n",
    "mhc_emb = mhc_emb * mhc_bool_mask[:, :, None]\n",
    "mhc_OHE = mhc_OHE * mhc_bool_mask[:, :, None]\n",
    "pep_OHE = pep_OHE * pep_bool_mask[:, :, None]\n",
    "\n",
    "# 6) Group inputs/targets (autoencoder targets are placeholders; recon losses use predictions)\n",
    "inputs = {\n",
    "    \"pep_onehot\": pep_OHE,\n",
    "    \"pep_mask\": pep_mask,\n",
    "    \"mhc_emb\": mhc_emb,\n",
    "    \"mhc_mask\": mhc_mask,\n",
    "    \"mhc_onehot\": mhc_OHE\n",
    "}\n",
    "targets = {\n",
    "    \"pep_ytrue_ypred\": pep_OHE,\n",
    "    \"mhc_ytrue_ypred\": mhc_OHE,\n",
    "    \"cls_ypred\": y_true\n",
    "}"
   ],
   "id": "48f15e98e223a785",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T20:30:07.179112Z",
     "start_time": "2025-08-21T20:30:07.176646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Constants\n",
    "# AA = \"ACDEFGHIKLMNPQRSTVWY-\"\n",
    "# AA_TO_INT = {a: i for i, a in enumerate(AA)}\n",
    "# UNK_IDX = 20  # Index for \"unknown\"\n",
    "# MASK_TOKEN = -1.0\n",
    "# NORM_TOKEN = 1.0\n",
    "# PAD_TOKEN = -2.0\n",
    "# PAD_VALUE = 0.0\n",
    "# MASK_VALUE = 0.0\n",
    "#\n",
    "#\n",
    "# def seq_to_onehot(sequence: str, max_seq_len: int) -> np.ndarray:\n",
    "#     \"\"\"Convert peptide sequence to one-hot encoding\"\"\"\n",
    "#     arr = np.full((max_seq_len, 21), PAD_VALUE, dtype=np.float32) # initialize padding with 0\n",
    "#     for j, aa in enumerate(sequence.upper()[:max_seq_len]):\n",
    "#         arr[j, AA_TO_INT.get(aa, UNK_IDX)] = 1.0\n",
    "#         # print number of UNKs in the sequence\n",
    "#     # num_unks = np.sum(arr[:, UNK_IDX])\n",
    "#     # zero out gaps\n",
    "#     arr[:, AA_TO_INT['-']] = PAD_VALUE  # Set gaps to PAD_VALUE\n",
    "#     # if num_unks > 0:\n",
    "#     #     print(f\"Warning: {num_unks} unknown amino acids in sequence '{sequence}'\")\n",
    "#     return arr"
   ],
   "id": "d40d8d3fb2018df9",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T20:30:08.870597Z",
     "start_time": "2025-08-21T20:30:07.202166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 4. CUSTOM TRAINING LOOP SETUP\n",
    "# ==============================================================================\n",
    "# --- Hyperparameters and Setup ---\n",
    "epochs = 15\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "emb_dim = 21\n",
    "heads = 4\n",
    "noise_std = 0.1\n",
    "num_experts = 16\n",
    "\n",
    "# --- Instantiate Model, Optimizer, and Loss ---\n",
    "model = pmbind_subtract_moe_auto(\n",
    "    max_pep_len=pep_len,\n",
    "    max_mhc_len=mhc_len,\n",
    "    emb_dim=emb_dim,\n",
    "    heads=heads,\n",
    "    noise_std=noise_std,\n",
    "    num_experts=num_experts,\n",
    "    mask_token=MASK_TOKEN,\n",
    "    pad_token=PAD_TOKEN\n",
    ")\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate)\n",
    "binary_loss_fn = binary_focal_loss\n",
    "\n",
    "# compile the model\n",
    "# model.compile(\n",
    "#     optimizer=optimizer,\n",
    "#     loss={\n",
    "#         \"pep_ytrue_ypred\": masked_categorical_crossentropy,\n",
    "#         \"mhc_ytrue_ypred\": masked_categorical_crossentropy,\n",
    "#         \"cls_ypred\": binary_loss_fn\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# --- Metrics for Tracking ---\n",
    "metrics_names = ['loss', 'pep_recon_loss', 'mhc_recon_loss', 'class_loss', 'auc']\n",
    "train_metrics = {name: tf.keras.metrics.Mean(name=f\"train_{name}\") for name in metrics_names}\n",
    "val_metrics = {name: tf.keras.metrics.Mean(name=f\"val_{name}\") for name in metrics_names}\n",
    "train_metrics['auc'] = tf.keras.metrics.AUC(name='train_auc')\n",
    "val_metrics['auc'] = tf.keras.metrics.AUC(name='val_auc')\n",
    "\n",
    "# --- Prepare tf.data.Dataset for efficient training ---\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(num_samples)\n",
    "val_size = int(0.2 * num_samples)\n",
    "train_dataset = dataset.skip(val_size).batch(batch_size, drop_remainder=True)\n",
    "val_dataset = dataset.take(val_size).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "\n",
    "# print model stats\n",
    "model.summary()\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. TRAIN AND VALIDATION STEPS (using tf.function for performance)\n",
    "# ==============================================================================\n",
    "@tf.function\n",
    "def train_step(x_batch, y_batch):\n",
    "    x_batch_list = [x_batch['pep_onehot'], x_batch['pep_mask'], x_batch['mhc_emb'], x_batch['mhc_mask'], x_batch['mhc_onehot']]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x_batch_list, training=True)\n",
    "        # Calculate individual losses\n",
    "        pep_loss = masked_categorical_crossentropy(predictions['pep_ytrue_ypred'], x_batch['pep_mask'], PAD_TOKEN,\n",
    "                                                    sample_weight=y_batch['cls_ypred'])\n",
    "        mhc_loss = masked_categorical_crossentropy(predictions['mhc_ytrue_ypred'], x_batch['mhc_mask'], PAD_TOKEN,\n",
    "                                                   sample_weight=y_batch['cls_ypred'])\n",
    "        class_loss = binary_loss_fn(y_batch['cls_ypred'], predictions['cls_ypred'], gamma=2.0, pos_weight=1.0, label_smoothing=0.1) # TODO wighted, emphasize on positive labels # FOCAL loss\n",
    "\n",
    "        # Combine losses (you can apply weights here, e.g., total_loss = 0.5*pep_loss + ...)\n",
    "        total_loss = pep_loss + mhc_loss + class_loss\n",
    "\n",
    "    # Apply gradients\n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Update metrics\n",
    "    train_metrics['loss'](total_loss)\n",
    "    train_metrics['pep_recon_loss'](pep_loss)\n",
    "    train_metrics['mhc_recon_loss'](mhc_loss)\n",
    "    train_metrics['class_loss'](class_loss)\n",
    "    train_metrics['auc'](y_batch['cls_ypred'], predictions['cls_ypred'])\n",
    "\n",
    "@tf.function\n",
    "def val_step(x_batch, y_batch):\n",
    "    x_batch_list = [x_batch['pep_onehot'], x_batch['pep_mask'], x_batch['mhc_emb'], x_batch['mhc_mask'], x_batch['mhc_onehot']]\n",
    "\n",
    "    predictions = model(x_batch_list, training=False)\n",
    "    # Calculate losses\n",
    "    pep_loss = masked_categorical_crossentropy(predictions['pep_ytrue_ypred'], x_batch['pep_mask'], PAD_TOKEN)\n",
    "    mhc_loss = masked_categorical_crossentropy(predictions['mhc_ytrue_ypred'], x_batch['mhc_mask'], PAD_TOKEN)\n",
    "    class_loss = binary_loss_fn(y_batch['cls_ypred'], predictions['cls_ypred'], gamma=2.0, pos_weight=1.0, label_smoothing=0.1) # FOCAL loss\n",
    "    total_loss = pep_loss + mhc_loss + class_loss\n",
    "\n",
    "    # Update metrics\n",
    "    val_metrics['loss'](total_loss)\n",
    "    val_metrics['pep_recon_loss'](pep_loss)\n",
    "    val_metrics['mhc_recon_loss'](mhc_loss)\n",
    "    val_metrics['class_loss'](class_loss)\n",
    "    val_metrics['auc'](y_batch['cls_ypred'], predictions['cls_ypred'])\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. THE MAIN TRAINING LOOP\n",
    "# ==============================================================================\n",
    "history = {f\"{key}\": [] for key in train_metrics.keys()}\n",
    "history.update({f\"val_{key}\": [] for key in val_metrics.keys()})\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(epochs):\n",
    "    # Reset metrics at the start of each epoch\n",
    "    for metric in train_metrics.values(): metric.reset_state()\n",
    "    for metric in val_metrics.values(): metric.reset_state()\n",
    "\n",
    "    # Training loop\n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        train_step(x_batch, y_batch)\n",
    "\n",
    "    # Validation loop\n",
    "    for x_val_batch, y_val_batch in val_dataset:\n",
    "        val_step(x_val_batch, y_val_batch)\n",
    "\n",
    "    # Log results\n",
    "    train_results = {key: value.result().numpy() for key, value in train_metrics.items()}\n",
    "    val_results = {key: value.result().numpy() for key, value in val_metrics.items()}\n",
    "\n",
    "    # Store history\n",
    "    for key, value in train_results.items(): history[key].append(value)\n",
    "    for key, value in val_results.items(): history[f\"val_{key}\"].append(value)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "          f\"Loss: {train_results['loss']:.4f} - \"\n",
    "          f\"AUC: {train_results['auc']:.4f} - \"\n",
    "          f\"Val Loss: {val_results['loss']:.4f} - \"\n",
    "          f\"Val AUC: {val_results['auc']:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ],
   "id": "cf763fa9a1776af1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"pmbind_subtract_moe_autoencoder\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"pmbind_subtract_moe_autoencoder\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape     \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m   Param #\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mConnected to     \u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ pep_onehot          │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m21\u001B[0m)    │          \u001B[38;5;34m0\u001B[0m │ -                 │\n",
       "│ (\u001B[38;5;33mInputLayer\u001B[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pep_mask            │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m14\u001B[0m)        │          \u001B[38;5;34m0\u001B[0m │ -                 │\n",
       "│ (\u001B[38;5;33mInputLayer\u001B[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mhc_emb             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m312\u001B[0m, \u001B[38;5;34m1152\u001B[0m) │          \u001B[38;5;34m0\u001B[0m │ -                 │\n",
       "│ (\u001B[38;5;33mInputLayer\u001B[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mhc_mask            │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m312\u001B[0m)       │          \u001B[38;5;34m0\u001B[0m │ -                 │\n",
       "│ (\u001B[38;5;33mInputLayer\u001B[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pep_mask2           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m21\u001B[0m)    │          \u001B[38;5;34m0\u001B[0m │ pep_onehot[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m], │\n",
       "│ (\u001B[38;5;33mMaskedEmbedding\u001B[0m)   │                   │            │ pep_mask[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mhc_mask2           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m312\u001B[0m, \u001B[38;5;34m1152\u001B[0m) │          \u001B[38;5;34m0\u001B[0m │ mhc_emb[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],    │\n",
       "│ (\u001B[38;5;33mMaskedEmbedding\u001B[0m)   │                   │            │ mhc_mask[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pep_pos1            │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m21\u001B[0m)    │          \u001B[38;5;34m0\u001B[0m │ pep_mask2[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],  │\n",
       "│ (\u001B[38;5;33mPositionalEncodin…\u001B[0m │                   │            │ pep_mask[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mhc_pos1            │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m312\u001B[0m, \u001B[38;5;34m1152\u001B[0m) │          \u001B[38;5;34m0\u001B[0m │ mhc_mask2[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],  │\n",
       "│ (\u001B[38;5;33mPositionalEncodin…\u001B[0m │                   │            │ mhc_mask[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pep_Dense1 (\u001B[38;5;33mDense\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m21\u001B[0m)    │        \u001B[38;5;34m462\u001B[0m │ pep_pos1[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mhc_dense1 (\u001B[38;5;33mDense\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m312\u001B[0m, \u001B[38;5;34m21\u001B[0m)   │     \u001B[38;5;34m24,213\u001B[0m │ mhc_pos1[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pep_Dropout1        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m21\u001B[0m)    │          \u001B[38;5;34m0\u001B[0m │ pep_Dense1[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]  │\n",
       "│ (\u001B[38;5;33mDropout\u001B[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mhc_Dropout1        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m312\u001B[0m, \u001B[38;5;34m21\u001B[0m)   │          \u001B[38;5;34m0\u001B[0m │ mhc_dense1[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]  │\n",
       "│ (\u001B[38;5;33mDropout\u001B[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pmhc_subtract       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m312\u001B[0m, \u001B[38;5;34m294\u001B[0m)  │          \u001B[38;5;34m0\u001B[0m │ pep_Dropout1[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m…\u001B[0m │\n",
       "│ (\u001B[38;5;33mSubtractLayer\u001B[0m)     │                   │            │ pep_mask[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],   │\n",
       "│                     │                   │            │ mhc_Dropout1[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m…\u001B[0m │\n",
       "│                     │                   │            │ mhc_mask[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pmhc_gaussian_noise │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m312\u001B[0m, \u001B[38;5;34m294\u001B[0m)  │          \u001B[38;5;34m0\u001B[0m │ pmhc_subtract[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mAddGaussianNoise\u001B[0m)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mhc_subtracted_p_a… │ [(\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m312\u001B[0m,      │    \u001B[38;5;34m431,298\u001B[0m │ pmhc_gaussian_no… │\n",
       "│ (\u001B[38;5;33mAttentionLayer\u001B[0m)    │ \u001B[38;5;34m294\u001B[0m), (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m4\u001B[0m,   │            │ mhc_mask[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]    │\n",
       "│                     │ \u001B[38;5;34m312\u001B[0m, \u001B[38;5;34m312\u001B[0m)]        │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ peptide_cross_att   │ [(\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m21\u001B[0m),  │     \u001B[38;5;34m13,713\u001B[0m │ pep_Dropout1[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m…\u001B[0m │\n",
       "│ (\u001B[38;5;33mAttentionLayer\u001B[0m)    │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m4\u001B[0m, \u001B[38;5;34m14\u001B[0m,     │            │ pep_mask[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],   │\n",
       "│                     │ \u001B[38;5;34m312\u001B[0m)]             │            │ mhc_subtracted_p… │\n",
       "│                     │                   │            │ mhc_mask[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ avg_pooled          │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m21\u001B[0m)        │          \u001B[38;5;34m0\u001B[0m │ peptide_cross_at… │\n",
       "│ (\u001B[38;5;33mGlobalAveragePool…\u001B[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ std_pooled          │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m21\u001B[0m)        │          \u001B[38;5;34m0\u001B[0m │ peptide_cross_at… │\n",
       "│ (\u001B[38;5;33mGlobalSTDPooling1…\u001B[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ latent_vector_conc… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m42\u001B[0m)        │          \u001B[38;5;34m0\u001B[0m │ avg_pooled[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m], │\n",
       "│ (\u001B[38;5;33mConcatenate\u001B[0m)       │                   │            │ std_pooled[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ latent_mhc_dense1   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m312\u001B[0m, \u001B[38;5;34m588\u001B[0m)  │    \u001B[38;5;34m173,460\u001B[0m │ mhc_subtracted_p… │\n",
       "│ (\u001B[38;5;33mDense\u001B[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gating_network_den… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m)        │      \u001B[38;5;34m1,376\u001B[0m │ latent_vector_co… │\n",
       "│ (\u001B[38;5;33mDense\u001B[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ latent_mhc_dropout1 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m312\u001B[0m, \u001B[38;5;34m588\u001B[0m)  │          \u001B[38;5;34m0\u001B[0m │ latent_mhc_dense… │\n",
       "│ (\u001B[38;5;33mDropout\u001B[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gating_network_dro… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m)        │          \u001B[38;5;34m0\u001B[0m │ gating_network_d… │\n",
       "│ (\u001B[38;5;33mDropout\u001B[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cross_latent        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m312\u001B[0m, \u001B[38;5;34m21\u001B[0m)   │     \u001B[38;5;34m12,369\u001B[0m │ latent_mhc_dropo… │\n",
       "│ (\u001B[38;5;33mDense\u001B[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gating_network_sof… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m16\u001B[0m)        │        \u001B[38;5;34m528\u001B[0m │ gating_network_d… │\n",
       "│ (\u001B[38;5;33mDense\u001B[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ latent_mhc_dropout2 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m312\u001B[0m, \u001B[38;5;34m21\u001B[0m)   │          \u001B[38;5;34m0\u001B[0m │ cross_latent[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m…\u001B[0m │\n",
       "│ (\u001B[38;5;33mDropout\u001B[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pep_latent (\u001B[38;5;33mDense\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m21\u001B[0m)    │        \u001B[38;5;34m462\u001B[0m │ peptide_cross_at… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enhanced_mixture_o… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)         │      \u001B[38;5;34m7,856\u001B[0m │ latent_vector_co… │\n",
       "│ (\u001B[38;5;33mEnhancedMixtureOf…\u001B[0m │                   │            │ gating_network_s… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mhc_onehot          │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m312\u001B[0m, \u001B[38;5;34m21\u001B[0m)   │          \u001B[38;5;34m0\u001B[0m │ -                 │\n",
       "│ (\u001B[38;5;33mInputLayer\u001B[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mhc_reconstruction… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m312\u001B[0m, \u001B[38;5;34m21\u001B[0m)   │        \u001B[38;5;34m462\u001B[0m │ latent_mhc_dropo… │\n",
       "│ (\u001B[38;5;33mDense\u001B[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pep_reconstruction… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m21\u001B[0m)    │        \u001B[38;5;34m462\u001B[0m │ pep_latent[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]  │\n",
       "│ (\u001B[38;5;33mDense\u001B[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cls_ypred           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)         │          \u001B[38;5;34m0\u001B[0m │ enhanced_mixture… │\n",
       "│ (\u001B[38;5;33mActivation\u001B[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mhc_ytrue_ypred     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m312\u001B[0m, \u001B[38;5;34m42\u001B[0m)   │          \u001B[38;5;34m0\u001B[0m │ mhc_onehot[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m], │\n",
       "│ (\u001B[38;5;33mConcatenate\u001B[0m)       │                   │            │ mhc_reconstructi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pep_ytrue_ypred     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m42\u001B[0m)    │          \u001B[38;5;34m0\u001B[0m │ pep_onehot[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m], │\n",
       "│ (\u001B[38;5;33mConcatenate\u001B[0m)       │                   │            │ pep_reconstructi… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ pep_onehot          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pep_mask            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mhc_emb             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1152</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mhc_mask            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pep_mask2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pep_onehot[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaskedEmbedding</span>)   │                   │            │ pep_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mhc_mask2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1152</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ mhc_emb[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaskedEmbedding</span>)   │                   │            │ mhc_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pep_pos1            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pep_mask2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncodin…</span> │                   │            │ pep_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mhc_pos1            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1152</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ mhc_mask2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncodin…</span> │                   │            │ mhc_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pep_Dense1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">462</span> │ pep_pos1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mhc_dense1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,213</span> │ mhc_pos1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pep_Dropout1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pep_Dense1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mhc_Dropout1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ mhc_dense1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pmhc_subtract       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">294</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pep_Dropout1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SubtractLayer</span>)     │                   │            │ pep_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│                     │                   │            │ mhc_Dropout1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │                   │            │ mhc_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pmhc_gaussian_noise │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">294</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pmhc_subtract[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AddGaussianNoise</span>)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mhc_subtracted_p_a… │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">431,298</span> │ pmhc_gaussian_no… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionLayer</span>)    │ <span style=\"color: #00af00; text-decoration-color: #00af00\">294</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,   │            │ mhc_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>)]        │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ peptide_cross_att   │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>),  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">13,713</span> │ pep_Dropout1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,     │            │ pep_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>)]             │            │ mhc_subtracted_p… │\n",
       "│                     │                   │            │ mhc_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ avg_pooled          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ peptide_cross_at… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ std_pooled          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ peptide_cross_at… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalSTDPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ latent_vector_conc… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ avg_pooled[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ std_pooled[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ latent_mhc_dense1   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">588</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">173,460</span> │ mhc_subtracted_p… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gating_network_den… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,376</span> │ latent_vector_co… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ latent_mhc_dropout1 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">588</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ latent_mhc_dense… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gating_network_dro… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ gating_network_d… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cross_latent        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,369</span> │ latent_mhc_dropo… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gating_network_sof… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │ gating_network_d… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ latent_mhc_dropout2 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ cross_latent[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pep_latent (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">462</span> │ peptide_cross_at… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enhanced_mixture_o… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,856</span> │ latent_vector_co… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EnhancedMixtureOf…</span> │                   │            │ gating_network_s… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mhc_onehot          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mhc_reconstruction… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">462</span> │ latent_mhc_dropo… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pep_reconstruction… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">462</span> │ pep_latent[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cls_ypred           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ enhanced_mixture… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mhc_ytrue_ypred     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ mhc_onehot[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ mhc_reconstructi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pep_ytrue_ypred     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pep_onehot[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ pep_reconstructi… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m666,661\u001B[0m (2.54 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">666,661</span> (2.54 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m666,661\u001B[0m (2.54 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">666,661</span> (2.54 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"/var/folders/6m/k4qq5w395_99lg3s6fk1_8zm0000gn/T/ipykernel_2743/551561430.py\", line 62, in train_step  *\n        predictions = model(x_batch_list, training=True)\n    File \"/Users/amirreza/miniforge3/envs/py312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/6m/k4qq5w395_99lg3s6fk1_8zm0000gn/T/ipykernel_2743/92885885.py\", line 120, in call\n        stacked_kernel1 = tf.stack([e.fc1.kernel for e in self.experts], axis=0)\n\n    AttributeError: Exception encountered when calling EnhancedMixtureOfExperts.call().\n    \n    \u001B[1m'str' object has no attribute 'base_dtype'\u001B[0m\n    \n    Arguments received by EnhancedMixtureOfExperts.call():\n      • inputs=('tf.Tensor(shape=(128, 42), dtype=float32)', 'tf.Tensor(shape=(128, 16), dtype=float32)')\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[65]\u001B[39m\u001B[32m, line 116\u001B[39m\n\u001B[32m    114\u001B[39m \u001B[38;5;66;03m# Training loop\u001B[39;00m\n\u001B[32m    115\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m x_batch, y_batch \u001B[38;5;129;01min\u001B[39;00m train_dataset:\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m     \u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    118\u001B[39m \u001B[38;5;66;03m# Validation loop\u001B[39;00m\n\u001B[32m    119\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m x_val_batch, y_val_batch \u001B[38;5;129;01min\u001B[39;00m val_dataset:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/py312/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001B[39m, in \u001B[36mfilter_traceback.<locals>.error_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    151\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    152\u001B[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001B[32m--> \u001B[39m\u001B[32m153\u001B[39m   \u001B[38;5;28;01mraise\u001B[39;00m e.with_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    154\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    155\u001B[39m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/var/folders/6m/k4qq5w395_99lg3s6fk1_8zm0000gn/T/__autograph_generated_filepiwle2y3.py:10\u001B[39m, in \u001B[36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001B[39m\u001B[34m(x_batch, y_batch)\u001B[39m\n\u001B[32m      8\u001B[39m x_batch_list = [ag__.ld(x_batch)[\u001B[33m'\u001B[39m\u001B[33mpep_onehot\u001B[39m\u001B[33m'\u001B[39m], ag__.ld(x_batch)[\u001B[33m'\u001B[39m\u001B[33mpep_mask\u001B[39m\u001B[33m'\u001B[39m], ag__.ld(x_batch)[\u001B[33m'\u001B[39m\u001B[33mmhc_emb\u001B[39m\u001B[33m'\u001B[39m], ag__.ld(x_batch)[\u001B[33m'\u001B[39m\u001B[33mmhc_mask\u001B[39m\u001B[33m'\u001B[39m], ag__.ld(x_batch)[\u001B[33m'\u001B[39m\u001B[33mmhc_onehot\u001B[39m\u001B[33m'\u001B[39m]]\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m ag__.ld(tf).GradientTape() \u001B[38;5;28;01mas\u001B[39;00m tape:\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m     predictions = \u001B[43mag__\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconverted_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[43m.\u001B[49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[43m.\u001B[49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_batch_list\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mdict\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfscope\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     11\u001B[39m     pep_loss = ag__.converted_call(ag__.ld(masked_categorical_crossentropy), (ag__.ld(predictions)[\u001B[33m'\u001B[39m\u001B[33mpep_ytrue_ypred\u001B[39m\u001B[33m'\u001B[39m], ag__.ld(x_batch)[\u001B[33m'\u001B[39m\u001B[33mpep_mask\u001B[39m\u001B[33m'\u001B[39m], ag__.ld(PAD_TOKEN)), \u001B[38;5;28mdict\u001B[39m(sample_weight=ag__.ld(y_batch)[\u001B[33m'\u001B[39m\u001B[33mcls_ypred\u001B[39m\u001B[33m'\u001B[39m]), fscope)\n\u001B[32m     12\u001B[39m     mhc_loss = ag__.converted_call(ag__.ld(masked_categorical_crossentropy), (ag__.ld(predictions)[\u001B[33m'\u001B[39m\u001B[33mmhc_ytrue_ypred\u001B[39m\u001B[33m'\u001B[39m], ag__.ld(x_batch)[\u001B[33m'\u001B[39m\u001B[33mmhc_mask\u001B[39m\u001B[33m'\u001B[39m], ag__.ld(PAD_TOKEN)), \u001B[38;5;28mdict\u001B[39m(sample_weight=ag__.ld(y_batch)[\u001B[33m'\u001B[39m\u001B[33mcls_ypred\u001B[39m\u001B[33m'\u001B[39m]), fscope)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/py312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001B[39m, in \u001B[36mfilter_traceback.<locals>.error_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    119\u001B[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001B[32m    120\u001B[39m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[32m    121\u001B[39m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m122\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e.with_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    123\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    124\u001B[39m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[60]\u001B[39m\u001B[32m, line 120\u001B[39m, in \u001B[36mEnhancedMixtureOfExperts.call\u001B[39m\u001B[34m(self, inputs, training)\u001B[39m\n\u001B[32m    113\u001B[39m batch_dim = tf.shape(x)[\u001B[32m0\u001B[39m]\n\u001B[32m    115\u001B[39m \u001B[38;5;66;03m# Stack expert weights\u001B[39;00m\n\u001B[32m    116\u001B[39m \u001B[38;5;66;03m# Shapes:\u001B[39;00m\n\u001B[32m    117\u001B[39m \u001B[38;5;66;03m#  fc1: (E, I, H) / (E, H)\u001B[39;00m\n\u001B[32m    118\u001B[39m \u001B[38;5;66;03m#  fc2: (E, H, H2) / (E, H2)\u001B[39;00m\n\u001B[32m    119\u001B[39m \u001B[38;5;66;03m#  fc3: (E, H2, O) / (E, O)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m stacked_kernel1 = \u001B[43mtf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43me\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfc1\u001B[49m\u001B[43m.\u001B[49m\u001B[43mkernel\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43me\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mexperts\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    121\u001B[39m stacked_bias1 = tf.stack([e.fc1.bias \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.experts], axis=\u001B[32m0\u001B[39m)\n\u001B[32m    122\u001B[39m stacked_kernel2 = tf.stack([e.fc2.kernel \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.experts], axis=\u001B[32m0\u001B[39m)\n",
      "\u001B[31mAttributeError\u001B[39m: in user code:\n\n    File \"/var/folders/6m/k4qq5w395_99lg3s6fk1_8zm0000gn/T/ipykernel_2743/551561430.py\", line 62, in train_step  *\n        predictions = model(x_batch_list, training=True)\n    File \"/Users/amirreza/miniforge3/envs/py312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/6m/k4qq5w395_99lg3s6fk1_8zm0000gn/T/ipykernel_2743/92885885.py\", line 120, in call\n        stacked_kernel1 = tf.stack([e.fc1.kernel for e in self.experts], axis=0)\n\n    AttributeError: Exception encountered when calling EnhancedMixtureOfExperts.call().\n    \n    \u001B[1m'str' object has no attribute 'base_dtype'\u001B[0m\n    \n    Arguments received by EnhancedMixtureOfExperts.call():\n      • inputs=('tf.Tensor(shape=(128, 42), dtype=float32)', 'tf.Tensor(shape=(128, 16), dtype=float32)')\n      • training=True\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# import numpy as np\n",
    "#\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# # # load train, test data from the disk\n",
    "# df = pd.read_parquet(\"binding_affinity_dataset_with_swapped_negatives1.parquet\")\n",
    "# # df contains long_mer, mhc_embedding_keys, assigned_label\n",
    "# # mhc_latents_pooled_infer.mmap file contains the MHC embeddings and keys corresponding to mhc_embedding_keys in the df\n",
    "# print(len(df))\n",
    "#\n",
    "# print(mhc_latents_pooled.shape)\n",
    "# mhc_keys = df[\"mhc_embedding_key\"]\n",
    "#\n",
    "# # Create a mapping from MHC key to index\n",
    "# mhc_key_to_idx = {key: i for i, key in enumerate(mhc_keys)}\n",
    "#\n",
    "# # --- Prepare data ---\n",
    "# max_pep_len = 14\n",
    "# max_mhc_len = 312 # from dummy data, adjust if needed\n",
    "#\n",
    "# pep_OHE_list = []\n",
    "# pep_mask_list = []\n",
    "# mhc_emb_list = []\n",
    "# mhc_mask_list = []\n",
    "# y_true_list = []\n",
    "#\n",
    "# for _, row in df.iterrows():\n",
    "#     # Peptide processing\n",
    "#     pep_seq = row['long_mer']\n",
    "#     pep_ohe = seq_to_onehot(pep_seq, max_pep_len)\n",
    "#     pep_mask = np.full(max_pep_len, PAD_TOKEN, dtype=np.float32)\n",
    "#     pep_mask[:len(pep_seq)] = NORM_TOKEN\n",
    "#\n",
    "#     # MHC processing\n",
    "#     mhc_key = row['mhc_embedding_key']\n",
    "#     mhc_idx = mhc_key_to_idx[mhc_key]\n",
    "#     mhc_emb = mhc_latents_pooled[mhc_idx]\n",
    "#     # Assuming MHC embeddings are not padded/masked in the source file\n",
    "#     mhc_mask = np.full(max_mhc_len, NORM_TOKEN, dtype=np.float32)\n",
    "#\n",
    "#     pep_OHE_list.append(pep_ohe)\n",
    "#     pep_mask_list.append(pep_mask)\n",
    "#     mhc_emb_list.append(mhc_emb)\n",
    "#     mhc_mask_list.append(mhc_mask)\n",
    "#     y_true_list.append(row['assigned_label'])\n",
    "#\n",
    "# # Convert lists to numpy arrays\n",
    "# pep_OHE = np.array(pep_OHE_list, dtype=np.float32)\n",
    "# pep_mask = np.array(pep_mask_list, dtype=np.float32)\n",
    "# mhc_emb = np.array(mhc_emb_list, dtype=np.float32)\n",
    "# mhc_mask = np.array(mhc_mask_list, dtype=np.float32)\n",
    "# y_true = np.array(y_true_list, dtype=np.float32).reshape(-1, 1)\n",
    "#\n",
    "# # For autoencoder part, MHC OHE is needed. We don't have it, so creating dummy ones.\n",
    "# # In a real scenario, this should come from the data.\n",
    "# mhc_OHE = tf.one_hot(np.random.randint(0, 21, size=(len(df), max_mhc_len)), depth=21, dtype=tf.float32)\n",
    "#\n",
    "#\n",
    "# # --- Split data ---\n",
    "# indices = np.arange(len(df))\n",
    "# train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42, stratify=y_true)\n",
    "#\n",
    "# def create_dataset_dict(idx):\n",
    "#     return {\n",
    "#         \"pep_onehot\": pep_OHE[idx],\n",
    "#         \"pep_mask\": pep_mask[idx],\n",
    "#         \"mhc_emb\": mhc_emb[idx],\n",
    "#         \"mhc_mask\": mhc_mask[idx],\n",
    "#         \"mhc_onehot\": tf.gather(mhc_OHE, idx)\n",
    "#     }, {\n",
    "#         \"pep_ytrue_ypred\": pep_OHE[idx],\n",
    "#         \"mhc_ytrue_ypred\": tf.gather(mhc_OHE, idx),\n",
    "#         \"cls_ypred\": y_true[idx]\n",
    "#     }\n",
    "#\n",
    "# train_inputs, train_targets = create_dataset_dict(train_indices)\n",
    "# val_inputs, val_targets = create_dataset_dict(val_indices)\n",
    "#\n",
    "#\n"
   ],
   "id": "3bbe7bad86143c41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# 7. PLOT TRAINING HISTORY\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Plotting training history...\")\n",
    "\n",
    "# Create a dataframe from history for easier plotting\n",
    "history_df = pd.DataFrame(history)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Training and Validation Metrics', fontsize=20)\n",
    "\n",
    "# Plot Total Loss\n",
    "axes[0, 0].plot(history_df.index, history_df['loss'], label='Train Loss')\n",
    "axes[0, 0].plot(history_df.index, history_df['val_loss'], label='Validation Loss')\n",
    "axes[0, 0].set_title('Total Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Plot AUC\n",
    "axes[0, 1].plot(history_df.index, history_df['auc'], label='Train AUC')\n",
    "axes[0, 1].plot(history_df.index, history_df['val_auc'], label='Validation AUC')\n",
    "axes[0, 1].set_title('AUC')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('AUC')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Plot Reconstruction Losses\n",
    "axes[1, 0].plot(history_df.index, history_df['pep_recon_loss'], label='Train Peptide Recon Loss')\n",
    "axes[1, 0].plot(history_df.index, history_df['val_pep_recon_loss'], label='Val Peptide Recon Loss')\n",
    "axes[1, 0].plot(history_df.index, history_df['mhc_recon_loss'], label='Train MHC Recon Loss', linestyle='--')\n",
    "axes[1, 0].plot(history_df.index, history_df['val_mhc_recon_loss'], label='Val MHC Recon Loss', linestyle='--')\n",
    "axes[1, 0].set_title('Reconstruction Losses')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Plot Classification Loss\n",
    "axes[1, 1].plot(history_df.index, history_df['class_loss'], label='Train Classification Loss')\n",
    "axes[1, 1].plot(history_df.index, history_df['val_class_loss'], label='Validation Classification Loss')\n",
    "axes[1, 1].set_title('Classification Loss')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "print(\"Training history plots generated.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 8. UMAP VISUALIZATION OF LATENT SPACE\n",
    "# ==============================================================================\n",
    "# !pip install umap-learn seaborn\n",
    "import numpy as np\n",
    "import umap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Generating UMAP of latent space...\")\n",
    "\n",
    "# --- Get latent vectors and labels from the validation set ---\n",
    "latent_vectors = []\n",
    "true_labels = []\n",
    "\n",
    "for x_val_batch, y_val_batch in val_dataset:\n",
    "    x_batch_list = [\n",
    "        x_val_batch['pep_onehot'],\n",
    "        x_val_batch['pep_mask'],\n",
    "        x_val_batch['mhc_emb'],\n",
    "        x_val_batch['mhc_mask'],\n",
    "        x_val_batch['mhc_onehot']\n",
    "    ]\n",
    "    predictions = model(x_batch_list, training=False)\n",
    "    latent_vectors.append(predictions['latent_vector'].numpy())\n",
    "    true_labels.append(y_val_batch['cls_ypred'].numpy())\n",
    "\n",
    "# Concatenate all batches\n",
    "latent_vectors = np.concatenate(latent_vectors, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0).flatten()\n",
    "\n",
    "# --- Run UMAP ---\n",
    "# Note: umap-learn needs to be installed (`pip install umap-learn`)\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "embedding = reducer.fit_transform(latent_vectors)\n",
    "\n",
    "# --- Plot UMAP ---\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = sns.scatterplot(\n",
    "    x=embedding[:, 0],\n",
    "    y=embedding[:, 1],\n",
    "    hue=true_labels,\n",
    "    palette=sns.color_palette(\"viridis\", 2),\n",
    "    alpha=0.7,\n",
    "    s=50\n",
    ")\n",
    "plt.title('UMAP Projection of the Latent Space', fontsize=24)\n",
    "plt.xlabel(\"UMAP 1\", fontsize=14)\n",
    "plt.ylabel(\"UMAP 2\", fontsize=14)\n",
    "handles, _ = scatter.get_legend_handles_labels()\n",
    "plt.legend(handles, ['Negative', 'Positive'], title='Label')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"UMAP plot generated.\")"
   ],
   "id": "438ba30adabfe40e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# ==============================================================================\n",
   "id": "5ce0abd4dd09c242"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
