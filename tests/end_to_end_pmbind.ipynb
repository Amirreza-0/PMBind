{
 "cells": [
  {
   "cell_type": "code",
   "id": "11561aa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T06:41:54.169802Z",
     "start_time": "2025-08-19T06:41:54.165378Z"
    }
   },
   "source": [
    "# testing the subtract model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "outputs": [],
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "id": "d5d52c0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T06:41:56.561496Z",
     "start_time": "2025-08-19T06:41:54.719741Z"
    }
   },
   "source": [
    "# generate dummy data\n",
    "\n",
    "\n",
    "# generate a data that is pep one hot encoding with N=14, d=21. mhc_emb with N= 312 and d =1152. a contact map that is a soft weights over 312 mhc positions\n",
    "\n",
    "# Create dummy data\n",
    "num_samples = 1000\n",
    "\n",
    "# Peptide One-Hot Encoding (N=14, d=21)\n",
    "pep_len = 14\n",
    "pep_alphabet_size = 21\n",
    "pep_indices = np.random.randint(0, pep_alphabet_size, size=(num_samples, pep_len))\n",
    "pep_OHE = tf.one_hot(pep_indices, depth=pep_alphabet_size, dtype=tf.float32)\n",
    "\n",
    "# MHC Embedding (N=312, d=1152)\n",
    "mhc_len = 312\n",
    "mhc_embedding_dim = 1152\n",
    "mhc_emb = tf.random.normal(shape=(num_samples, mhc_len, mhc_embedding_dim))\n",
    "\n",
    "# Contact Map (soft weights over 312 MHC positions for each of the 14 peptide positions)\n",
    "contact_map_logits = tf.random.normal(shape=(num_samples, pep_len, mhc_len))\n",
    "contact_map = tf.nn.softmax(contact_map_logits, axis=-1)\n",
    "\n",
    "# Define pad and mask tokens\n",
    "pad_token = -2.0\n",
    "mask_token = -1.0\n",
    "\n",
    "# Generate peptide mask\n",
    "# Let's assume a variable number of padded/masked tokens per sample\n",
    "pep_mask = np.ones((num_samples, pep_len), dtype=np.float32)\n",
    "for i in range(num_samples):\n",
    "    # Add 0 to 3 padded tokens at the end\n",
    "    num_padded = np.random.randint(0, 4)\n",
    "    if num_padded > 0:\n",
    "        pep_mask[i, -num_padded:] = pad_token\n",
    "    # Add 0 to 2 masked tokens at random positions that are not padded\n",
    "    num_masked = np.random.randint(0, 3)\n",
    "    valid_indices = np.where(pep_mask[i] == 1.0)[0]\n",
    "    if num_masked > 0 and len(valid_indices) >= num_masked:\n",
    "        mask_indices = np.random.choice(valid_indices, num_masked, replace=False)\n",
    "        pep_mask[i, mask_indices] = mask_token\n",
    "\n",
    "pep_mask = tf.constant(pep_mask, dtype=tf.float32)\n",
    "\n",
    "# Generate MHC mask (similar logic)\n",
    "mhc_mask = np.ones((num_samples, mhc_len), dtype=np.float32)\n",
    "for i in range(num_samples):\n",
    "    # Add 0 to 10 padded tokens\n",
    "    num_padded = np.random.randint(0, 11)\n",
    "    if num_padded > 0:\n",
    "        mhc_mask[i, -num_padded:] = pad_token\n",
    "mhc_mask = tf.constant(mhc_mask, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# Zero out masked positions in the data\n",
    "# Create boolean masks where True means the position is valid (not padded or masked)\n",
    "pep_bool_mask = tf.cast(pep_mask > 0, dtype=tf.float32)\n",
    "mhc_bool_mask = tf.cast(mhc_mask > 0, dtype=tf.float32)\n",
    "\n",
    "# Apply masks to zero out data. Unsqueeze to allow broadcasting.\n",
    "pep_OHE = pep_OHE * pep_bool_mask[:, :, tf.newaxis]\n",
    "mhc_emb = mhc_emb * mhc_bool_mask[:, :, tf.newaxis]\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T06:41:56.612829Z",
     "start_time": "2025-08-19T06:41:56.598510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class MaskedEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, mask_token=-1., pad_token=-2., name='masked_embedding'):\n",
    "        super().__init__(name=name)\n",
    "        self.mask_token = mask_token\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (B, N, D)\n",
    "            mask: Tensor of shape (B, N)\n",
    "        Returns:\n",
    "            Tensor with masked positions set to zero.\n",
    "        \"\"\"\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        mask = tf.where((mask == self.pad_token) | (mask == self.mask_token), 0., 1.)\n",
    "        return x * mask[:, :, tf.newaxis]  # Apply mask to zero out positions\n",
    "\n",
    "\n",
    "class PositionalEncoding(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Sinusoidal Positional Encoding layer that applies encodings\n",
    "    only to non-masked tokens.\n",
    "\n",
    "    Args:\n",
    "        embed_dim (int): Dimension of embeddings (must match input last dim).\n",
    "        max_len (int): Maximum sequence length expected (used to precompute encodings).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, pos_range=100, mask_token=-1., pad_token=-2., name='positional_encoding'):\n",
    "        super().__init__(name=name)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.pos_range = pos_range\n",
    "        self.mask_token = mask_token\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "    def build(self, x):\n",
    "        # Create (1, pos_range, embed_dim) encoding matrix\n",
    "        pos = tf.range(self.pos_range, dtype=tf.float32)[:, tf.newaxis]  # (pos_range, 1)\n",
    "        i = tf.range(self.embed_dim, dtype=tf.float32)[tf.newaxis, :]  # (1, embed_dim)\n",
    "        #angle_rates = 1 / tf.pow(300.0, (2 * (i // 2)) / tf.cast(self.embed_dim, tf.float32))\n",
    "        angle_rates = tf.pow(300.0, -(2 * tf.floor(i / 2)) / tf.cast(self.embed_dim, tf.float32))\n",
    "        angle_rads = pos * angle_rates  # (pos_range, embed_dim)\n",
    "\n",
    "        # Apply sin to even indices, cos to odd indices\n",
    "        sines = tf.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)  # (max_len, embed_dim)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]  # (1, max_len, embed_dim)\n",
    "        self.pos_encoding = tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (B, N, D)\n",
    "            mask: Tensor of shape (B,N)\n",
    "        Returns:\n",
    "            Tensor with positional encodings added for masked and non padded tokens.\n",
    "        \"\"\"\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        pe = self.pos_encoding[:, :seq_len, :]  # (1, N, D)\n",
    "        mask = tf.cast(mask[:, :, tf.newaxis], tf.float32)  # (B, N, 1)\n",
    "        mask = tf.where(mask == self.pad_token, 0., 1.)\n",
    "        pe = pe * mask  # zero out positions where mask is 0\n",
    "\n",
    "        return x + pe\n",
    "\n",
    "class SubtractLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom layer to subtract a tensor from another tensor.\n",
    "    Tensor1: (B, P, D) -> (B, P*D) -> (B, M, P*D)\n",
    "    Tensor2: (B, M, D) -> (B, M, P*D)\n",
    "    Output: = Tensor2 - Tensor1\n",
    "    \"\"\"\n",
    "    def __init__(self, mask_token=-1., pad_token=-2., **kwargs):\n",
    "        \"\"\"Initialize the layer.\"\"\"\n",
    "        super(SubtractLayer, self).__init__(**kwargs)\n",
    "        self.mask_token = mask_token\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "    def call(self, peptide, pep_mask, mhc, mhc_mask):\n",
    "        B = tf.shape(peptide)[0]\n",
    "        P = tf.shape(peptide)[1]\n",
    "        D = tf.shape(peptide)[2]\n",
    "        M = tf.shape(mhc)[1]\n",
    "        P_D = P * D\n",
    "\n",
    "        pep_mask = tf.cast(pep_mask, tf.float32)\n",
    "        mhc_mask = tf.cast(mhc_mask, tf.float32)\n",
    "\n",
    "        pep_mask = tf.where(pep_mask == self.pad_token, x=0., y=1.)  # (B,P)\n",
    "        mhc_mask = tf.where(mhc_mask == self.pad_token, x=0., y=1.)\n",
    "\n",
    "        # peptide  (B,P,D) -> (B,P*D) -> (B,M,P*D)\n",
    "        peptide_flat = tf.reshape(peptide, (B, P_D))\n",
    "        peptide_exp = tf.repeat(peptide_flat[:, tf.newaxis, :], repeats=M, axis=1)\n",
    "        # mhc       (B,M,D) -> tile last axis P times -> (B,M,P*D)\n",
    "        mhc_exp = tf.tile(mhc, [1, 1, P])\n",
    "        result = mhc_exp - peptide_exp  # (B,M,P*D)\n",
    "        # peptide mask  (B,P) -> (B,P,D) -> flatten -> (B,P*D) -> (B,M,P*D)\n",
    "        pep_mask_PD = tf.tile(pep_mask[:, :, tf.newaxis], [1, 1, D])  # (B,P,D)\n",
    "        pep_mask_PD = tf.reshape(pep_mask_PD, (B, P_D))  # (B,P*D)\n",
    "        pep_mask_PD = tf.repeat(pep_mask_PD[:, tf.newaxis, :], repeats=M, axis=1)  # (B,M,P*D)\n",
    "        # mhc mask      (B,M) -> (B,M,1) -> repeat P*D along last axis\n",
    "        mhc_mask_PD = tf.repeat(mhc_mask[:, :, tf.newaxis], repeats=P_D, axis=2)  # (B,M,P*D)\n",
    "        combined_mask = tf.logical_and(tf.cast(pep_mask_PD, tf.bool), tf.cast(mhc_mask_PD, tf.bool))\n",
    "        masked_result = tf.where(combined_mask, result, tf.zeros_like(result))\n",
    "        return masked_result\n",
    "\n",
    "class AddGaussianNoise(layers.Layer):\n",
    "    def __init__(self, std=0.1, **kw): super().__init__(**kw); self.std = std\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if training: return x + tf.random.normal(tf.shape(x), stddev=self.std)\n",
    "        return x\n",
    "\n",
    "class AttentionLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom multi-head attention layer supporting self- and cross-attention.\n",
    "\n",
    "    Args:\n",
    "        query_dim (int): Input feature dimension for query.\n",
    "        context_dim (int): Input feature dimension for context (key and value).\n",
    "        output_dim (int): Output feature dimension.\n",
    "        type (str): 'self' or 'cross'.\n",
    "        heads (int): Number of attention heads.\n",
    "        resnet (bool): Whether to use residual connection.\n",
    "        return_att_weights (bool): Whether to return attention weights.\n",
    "        name (str): Layer name.\n",
    "        epsilon (float): Epsilon for layer normalization.\n",
    "        gate (bool): Whether to use gating mechanism.\n",
    "        mask_token (float): Value for masked tokens.\n",
    "        pad_token (float): Value for padded tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, query_dim, context_dim, output_dim, type, heads=4,\n",
    "                 resnet=True, return_att_weights=False, name='attention',\n",
    "                 epsilon=1e-6, gate=True, mask_token=-1., pad_token=-2.):\n",
    "        super().__init__(name=name)\n",
    "        assert isinstance(query_dim, int) and isinstance(context_dim, int) and isinstance(output_dim, int)\n",
    "        assert type in ['self', 'cross']\n",
    "        if resnet:\n",
    "            assert query_dim == output_dim\n",
    "        self.query_dim = query_dim\n",
    "        self.context_dim = context_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.type = type\n",
    "        self.heads = heads\n",
    "        self.resnet = resnet\n",
    "        self.return_att_weights = return_att_weights\n",
    "        self.epsilon = epsilon\n",
    "        self.gate = gate\n",
    "        self.mask_token = mask_token\n",
    "        self.pad_token = pad_token\n",
    "        self.att_dim = output_dim // heads  # Attention dimension per head\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Projection weights\n",
    "        self.q_proj = self.add_weight(shape=(self.heads, self.query_dim, self.att_dim),\n",
    "                                      initializer='random_normal', trainable=True, name=f'q_proj_{self.name}')\n",
    "        self.k_proj = self.add_weight(shape=(self.heads, self.context_dim, self.att_dim),\n",
    "                                      initializer='random_normal', trainable=True, name=f'k_proj_{self.name}')\n",
    "        self.v_proj = self.add_weight(shape=(self.heads, self.context_dim, self.att_dim),\n",
    "                                      initializer='random_normal', trainable=True, name=f'v_proj_{self.name}')\n",
    "        if self.gate:\n",
    "            self.g = self.add_weight(shape=(self.heads, self.query_dim, self.att_dim),\n",
    "                                     initializer='random_uniform', trainable=True, name=f'gate_{self.name}')\n",
    "        self.norm = layers.LayerNormalization(epsilon=self.epsilon, name=f'ln_{self.name}')\n",
    "        if self.type == 'cross':\n",
    "            self.norm_context = layers.LayerNormalization(epsilon=self.epsilon, name=f'ln_context_{self.name}')\n",
    "        self.norm_out = layers.LayerNormalization(epsilon=self.epsilon, name=f'ln_out_{self.name}')\n",
    "        if self.resnet:\n",
    "            self.norm_resnet = layers.LayerNormalization(epsilon=self.epsilon, name=f'ln_resnet_{self.name}')\n",
    "        self.out_w = self.add_weight(shape=(self.heads * self.att_dim, self.output_dim),\n",
    "                                     initializer='random_normal', trainable=True, name=f'outw_{self.name}')\n",
    "        self.out_b = self.add_weight(shape=(self.output_dim,), initializer='zeros',\n",
    "                                     trainable=True, name=f'outb_{self.name}')\n",
    "        self.scale = 1.0 / tf.math.sqrt(tf.cast(self.att_dim, tf.float32))\n",
    "\n",
    "    def call(self, x, mask, context=None, context_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (B, N, query_dim) for query.\n",
    "            mask: Tensor of shape (B, N).\n",
    "            context: Tensor of shape (B, M, context_dim) for key/value in cross-attention.\n",
    "            context_mask: Tensor of shape (B, M) for context.\n",
    "        \"\"\"\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        if self.type == 'self':\n",
    "            context = x\n",
    "            context_mask = mask\n",
    "            q_input = k_input = v_input = self.norm(x)\n",
    "            mask_q = mask_k = tf.where(mask == self.pad_token, 0., 1.)\n",
    "        else:\n",
    "            assert context is not None and context_mask is not None\n",
    "            q_input = self.norm(x)\n",
    "            k_input = v_input = self.norm_context(context)\n",
    "            mask_q = tf.where(mask == self.pad_token, 0., 1.)\n",
    "            mask_k = tf.where(context_mask == self.pad_token, 0., 1.)\n",
    "\n",
    "        # Project query, key, value\n",
    "        q = tf.einsum('bnd,hde->bhne', q_input, self.q_proj)\n",
    "        k = tf.einsum('bmd,hde->bhme', k_input, self.k_proj)\n",
    "        v = tf.einsum('bmd,hde->bhme', v_input, self.v_proj)\n",
    "\n",
    "        # Compute attention scores\n",
    "        att = tf.einsum('bhne,bhme->bhnm', q, k) * self.scale\n",
    "        mask_q_exp = tf.expand_dims(mask_q, axis=1)\n",
    "        mask_k_exp = tf.expand_dims(mask_k, axis=1)\n",
    "        attention_mask = tf.einsum('bqn,bkm->bqnm', mask_q_exp, mask_k_exp)\n",
    "        attention_mask = tf.broadcast_to(attention_mask, tf.shape(att))\n",
    "        att += (1.0 - attention_mask) * -1e9\n",
    "        att = tf.nn.softmax(att, axis=-1) * attention_mask\n",
    "\n",
    "        # Compute output\n",
    "        out = tf.einsum('bhnm,bhme->bhne', att, v)\n",
    "        if self.gate:\n",
    "            g = tf.einsum('bnd,hde->bhne', q_input, self.g)\n",
    "            g = tf.nn.sigmoid(g)\n",
    "            out *= g\n",
    "\n",
    "        out = tf.transpose(out, [0, 2, 1, 3])\n",
    "        out = tf.reshape(out, [tf.shape(x)[0], tf.shape(x)[1], self.heads * self.att_dim])\n",
    "        out = tf.matmul(out, self.out_w) + self.out_b\n",
    "\n",
    "        if self.resnet:\n",
    "            out += x\n",
    "            out = self.norm_resnet(out)\n",
    "        out = self.norm_out(out)\n",
    "        mask_exp = tf.expand_dims(mask_q, axis=-1)\n",
    "        out *= mask_exp\n",
    "\n",
    "        return (out, att) if self.return_att_weights else out"
   ],
   "id": "f9c778685d53eae2",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T10:59:29.830840Z",
     "start_time": "2025-08-19T10:59:29.818005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Expert(layers.Layer):\n",
    "    \"\"\"A binary prediction expert with added complexity.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.fc1 = layers.Dense(hidden_dim, activation='relu', input_shape=(input_dim,))\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.fc2 = layers.Dense(hidden_dim // 2, activation='relu')\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "        self.fc3 = layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class EnhancedMixtureOfExperts(layers.Layer):\n",
    "    \"\"\"\n",
    "    Enhanced Mixture of Experts layer that uses cluster assignments.\n",
    "\n",
    "    This implementation eliminates the need for a SparseDispatcher by:\n",
    "    - During training: Using hard clustering to train specific experts\n",
    "    - During inference: Using soft clustering to mix the experts' weights\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, num_experts, output_dim=1, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Create n experts\n",
    "        self.experts = [\n",
    "            Expert(input_dim, hidden_dim, output_dim, dropout_rate)\n",
    "            for _ in range(num_experts)\n",
    "        ]\n",
    "\n",
    "    def convert_to_hard_clustering(self, soft_clusters):\n",
    "        \"\"\"Convert soft clustering values to hard clustering (one-hot encoding)\"\"\"\n",
    "        # Get the index of the maximum value for each sample\n",
    "        hard_indices = tf.argmax(soft_clusters, axis=1)\n",
    "        # Convert to one-hot encoding\n",
    "        return tf.one_hot(hard_indices, depth=self.num_experts)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Unpack inputs\n",
    "        if isinstance(inputs, tuple) and len(inputs) == 2:\n",
    "            x, soft_cluster_probs = inputs\n",
    "        else:\n",
    "            raise ValueError(\"Inputs must include both features and clustering values\")\n",
    "\n",
    "        batch_size = tf.shape(x)[0]\n",
    "\n",
    "        # Convert to hard clustering during training if requested\n",
    "        if training:\n",
    "            clustering = self.convert_to_hard_clustering(soft_cluster_probs)\n",
    "        else:\n",
    "            clustering = soft_cluster_probs\n",
    "\n",
    "        # Initialize output tensor\n",
    "        combined_output = tf.zeros([batch_size, self.output_dim])\n",
    "\n",
    "        # Process each expert\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            # Get the weight for this expert for each sample in the batch\n",
    "            expert_weights = clustering[:, i:i + 1]  # Shape: [batch_size, 1]\n",
    "\n",
    "            # Only compute outputs for samples with non-zero weights\n",
    "            # to save computation during training with hard clustering\n",
    "            if training:\n",
    "                # Find samples assigned to this expert\n",
    "                assigned_indices = tf.where(expert_weights[:, 0] > 0)[:, 0]\n",
    "\n",
    "                def expert_computation():\n",
    "                    \"\"\"The computation to run if there are assigned samples.\"\"\"\n",
    "                    assigned_x = tf.gather(x, assigned_indices)\n",
    "                    expert_output = expert(assigned_x, training=training)\n",
    "                    indices = tf.expand_dims(assigned_indices, axis=1)\n",
    "                    return tf.scatter_nd(indices, expert_output, [batch_size, self.output_dim])\n",
    "\n",
    "                def no_computation():\n",
    "                    \"\"\"Return zeros if no samples are assigned.\"\"\"\n",
    "                    return tf.zeros([batch_size, self.output_dim])\n",
    "\n",
    "                # Use tf.cond to handle control flow in graph mode\n",
    "                update = tf.cond(\n",
    "                    tf.size(assigned_indices) > 0,\n",
    "                    expert_computation,\n",
    "                    no_computation\n",
    "                )\n",
    "                combined_output += update\n",
    "            else:\n",
    "                # During inference or when using soft clustering:\n",
    "                # Compute expert output for all samples\n",
    "                expert_output = expert(x, training=training)\n",
    "\n",
    "                # Weight the output by the clustering values\n",
    "                weighted_output = expert_output * expert_weights\n",
    "\n",
    "                # Add to combined output\n",
    "                combined_output += weighted_output\n",
    "\n",
    "        return combined_output"
   ],
   "id": "9f48b20ac5eb3ab3",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T11:26:16.654431Z",
     "start_time": "2025-08-19T11:26:16.646736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def masked_categorical_crossentropy(y_true_y_pred, mask, pad_token=-2.0):\n",
    "#     \"\"\"\n",
    "#     Compute masked categorical cross-entropy loss.\n",
    "#\n",
    "#     Args:\n",
    "#         y_true: True labels (tensor).\n",
    "#         y_pred: Predicted probabilities (tensor).\n",
    "#         mask: Mask tensor indicating positions to include in the loss.\n",
    "#\n",
    "#     Returns:\n",
    "#         Mean masked loss (tensor).\n",
    "#     \"\"\"\n",
    "#     y_true, y_pred = tf.split(y_true_y_pred, num_or_size_splits=2, axis=-1)\n",
    "#     # loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "#     loss = -tf.reduce_sum(y_true * tf.math.log(tf.clip_by_value(y_pred, 1e-7, 1.0)), axis=-1) #(B,N)\n",
    "#     mask = tf.cast(mask, tf.float32)  # Ensure mask is float\n",
    "#     mask = tf.where(mask == pad_token, 0.0, 1.0)  # Convert pad token to 0.0 and others to 1.0\n",
    "#     if tf.rank(mask) > tf.rank(loss):\n",
    "#         mask = tf.squeeze(mask, axis=-1)\n",
    "#     loss = loss * mask\n",
    "#     loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "#     return loss\n",
    "\n",
    "def masked_categorical_crossentropy(y_true_and_pred, mask, pad_token=-2.0):\n",
    "    \"\"\"\n",
    "    Compute masked categorical cross-entropy loss.\n",
    "\n",
    "    Args:\n",
    "        y_true_and_pred: Concatenated tensor of true labels and predictions.\n",
    "        mask: Mask tensor indicating positions to include in the loss.\n",
    "        pad_token: Value of the padding token in the mask.\n",
    "\n",
    "    Returns:\n",
    "        Mean masked loss (tensor).\n",
    "    \"\"\"\n",
    "    y_true, y_pred = tf.split(y_true_and_pred, num_or_size_splits=2, axis=-1)\n",
    "    loss = -tf.reduce_sum(y_true * tf.math.log(tf.clip_by_value(y_pred, 1e-9, 1.0)), axis=-1)\n",
    "\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    mask = tf.where(mask == pad_token, 0.0, 1.0)\n",
    "\n",
    "    # Squeeze the mask only if its rank is greater than the loss's rank\n",
    "    # and its last dimension is 1, which is a common scenario for masks.\n",
    "    if tf.rank(mask) > tf.rank(loss):\n",
    "        if mask.shape[-1] == 1:\n",
    "            mask = tf.squeeze(mask, axis=-1)\n",
    "\n",
    "    loss = loss * mask\n",
    "\n",
    "    # Avoid division by zero if mask is all zeros\n",
    "    total_loss = tf.reduce_sum(loss)\n",
    "    total_mask_elements = tf.reduce_sum(mask)\n",
    "    mean_loss = tf.math.divide_no_nan(total_loss, total_mask_elements)\n",
    "\n",
    "    return mean_loss\n"
   ],
   "id": "d21764a11124ea9c",
   "outputs": [],
   "execution_count": 104
  },
  {
   "cell_type": "code",
   "id": "2227699f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T11:26:17.369115Z",
     "start_time": "2025-08-19T11:26:17.354349Z"
    }
   },
   "source": [
    "# model\n",
    "from tensorflow.keras import layers\n",
    "MASK_TOKEN = -1.0\n",
    "PAD_TOKEN = -2.0\n",
    "\n",
    "\n",
    "def pmbind_subtract_moe_auto(max_pep_len: int,\n",
    "                               max_mhc_len: int,\n",
    "                               emb_dim: int = 96,\n",
    "                               heads: int = 4,\n",
    "                               noise_std: float = 0.1,\n",
    "                               num_experts: int = 30,\n",
    "                               mask_token: float = MASK_TOKEN,\n",
    "                               pad_token: float = PAD_TOKEN):\n",
    "    \"\"\"\n",
    "    Builds a pMHC autoencoder model with a Mixture-of-Experts (MoE) classifier head.\n",
    "\n",
    "    This model performs two tasks:\n",
    "    1. Autoencoding: Reconstructs peptide and MHC sequences from a latent representation.\n",
    "    2. Classification: Predicts a binary label using an MoE head, where experts are\n",
    "       selected based on an internally generated clustering of the latent space.\n",
    "    \"\"\"\n",
    "    # -------------------------------------------------------------------\n",
    "    # INPUTS\n",
    "    # -------------------------------------------------------------------\n",
    "    pep_OHE_in = keras.Input((max_pep_len, 21), name=\"pep_onehot\")\n",
    "    pep_mask_in = keras.Input((max_pep_len,), name=\"pep_mask\")\n",
    "    mhc_emb_in = keras.Input((max_mhc_len, 1152), name=\"mhc_emb\")\n",
    "    mhc_mask_in = keras.Input((max_mhc_len,), name=\"mhc_mask\")\n",
    "    mhc_OHE_in = keras.Input((max_mhc_len, 21), name=\"mhc_onehot\")\n",
    "    # -------------------------------------------------------------------\n",
    "    # MASKED  EMBEDDING  +  PE\n",
    "    # -------------------------------------------------------------------\n",
    "    pep = MaskedEmbedding(mask_token, pad_token, name=\"pep_mask2\")(pep_OHE_in, pep_mask_in)\n",
    "    pep = PositionalEncoding(21, int(max_pep_len * 3), name=\"pep_pos1\")(pep, pep_mask_in)\n",
    "    pep = layers.Dense(emb_dim, name=\"pep_Dense1\")(pep)\n",
    "    pep = layers.Dropout(0.1, name=\"pep_Dropout1\")(pep)\n",
    "\n",
    "    mhc = MaskedEmbedding(mask_token, pad_token, name=\"mhc_mask2\")(mhc_emb_in, mhc_mask_in)\n",
    "    mhc = PositionalEncoding(1152, int(max_mhc_len * 3), name=\"mhc_pos1\")(mhc, mhc_mask_in)\n",
    "    mhc = layers.Dense(emb_dim, name=\"mhc_dense1\")(mhc)\n",
    "    mhc = layers.Dropout(0.1, name=\"mhc_Dropout1\")(mhc)\n",
    "    # -------------------------------------------------------------------\n",
    "    # Subtract Layer\n",
    "    # -------------------------------------------------------------------\n",
    "    mhc_subtracted_p = SubtractLayer(name=\"pmhc_subtract\")(pep, pep_mask_in, mhc, mhc_mask_in) # (B, M, P*D) = mhc_expanded – peptide_expanded\n",
    "    #tf.print(\"mhc_subtracted_p shape:\", mhc_subtracted_p.shape)\n",
    "    # -------------------------------------------------------------------\n",
    "    # Add Gaussian Noise\n",
    "    # -------------------------------------------------------------------\n",
    "    mhc_subtracted_p = AddGaussianNoise(noise_std, name=\"pmhc_gaussian_noise\")(mhc_subtracted_p)\n",
    "    query_dim = int(emb_dim*max_pep_len)\n",
    "    # # -------------------------------------------------------------------\n",
    "    # Normal Self-Attention Layer\n",
    "    # # -------------------------------------------------------------------\n",
    "    mhc_subtracted_p_attn, mhc_subtracted_p_attn_scores = AttentionLayer(\n",
    "        query_dim=query_dim, context_dim=query_dim, output_dim=query_dim,\n",
    "        type=\"self\", heads=heads, resnet=True,\n",
    "        return_att_weights=True, name='mhc_subtracted_p_attn',\n",
    "        mask_token=mask_token,\n",
    "        pad_token=pad_token\n",
    "    )(mhc_subtracted_p, mhc_mask_in)\n",
    "    peptide_cross_att, peptide_cross_attn_scores = AttentionLayer(\n",
    "        query_dim=int(emb_dim), context_dim=query_dim, output_dim=int(emb_dim),\n",
    "        type=\"cross\", heads=heads, resnet=False,\n",
    "        return_att_weights=True, name='peptide_cross_att',\n",
    "        mask_token=mask_token,\n",
    "        pad_token=pad_token\n",
    "    )(pep, pep_mask_in, mhc_subtracted_p_attn, mhc_mask_in)\n",
    "\n",
    "    # --- Encoder ---\n",
    "    latent_sequence = layers.Dense(emb_dim*max_pep_len * 2, activation='relu', name='latent_mhc_dense1')(mhc_subtracted_p_attn)\n",
    "    latent_sequence = layers.Dropout(0.2, name='latent_mhc_dropout1')(latent_sequence)\n",
    "    latent_sequence = layers.Dense(emb_dim, activation='relu', name='cross_latent')(latent_sequence) # Shape: (B, M, D)\n",
    "\n",
    "    # --- Latent Vector for Clustering (pooled) ---\n",
    "    latent_vector = layers.GlobalAveragePooling1D(name=\"gap_latent\")(latent_sequence) # Shape: (B, D)\n",
    "    latent_vector = layers.Dense(emb_dim * 2, activation='relu', name='latent_dense2')(latent_vector)\n",
    "    latent_vector = layers.Dropout(0.2, name='latent_vector_dropout')(latent_vector)\n",
    "    latent_vector = layers.Dense(emb_dim, activation='relu', name='latent_vector_output')(latent_vector) # Shape: (B, D)\n",
    "\n",
    "    # --- Reconstruction Heads ---\n",
    "    mhc_recon_head = layers.Dropout(0.2, name='latent_mhc_dropout2')(latent_sequence)\n",
    "    mhc_recon = layers.Dense(21, activation='softmax', name='mhc_reconstruction_pred')(mhc_recon_head)\n",
    "    pep_recon = layers.Dense(emb_dim, activation='relu', name='pep_latent')(peptide_cross_att)\n",
    "    pep_recon = layers.Dense(21, activation='softmax', name='pep_reconstruction_pred')(pep_recon)\n",
    "\n",
    "    pep_out = layers.Concatenate(name='pep_ytrue_ypred', axis=-1)([pep_OHE_in, pep_recon]) #(B,P,42)\n",
    "    mhc_out = layers.Concatenate(name='mhc_ytrue_ypred', axis=-1)([mhc_OHE_in, mhc_recon]) #(B,M,42)\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # CLASSIFIER HEAD (MIXTURE OF EXPERTS)\n",
    "    # -------------------------------------------------------------------\n",
    "    # 1. Gating network: Generate soft cluster assignments from the latent vector\n",
    "    bigger_probs = layers.Dense(num_experts * 2, activation='relu', name='gating_network_dense1')(latent_vector)\n",
    "    bigger_probs = layers.Dropout(0.2, name='gating_network_dropout1')(bigger_probs)\n",
    "    soft_cluster_probs = layers.Dense(num_experts, activation='softmax', name='gating_network_softmax')(bigger_probs)\n",
    "\n",
    "    # 2. MoE layer: Get weighted prediction from experts\n",
    "    moe_layer = EnhancedMixtureOfExperts(\n",
    "        input_dim=emb_dim,\n",
    "        hidden_dim=emb_dim // 2,\n",
    "        num_experts=num_experts,\n",
    "        output_dim=1,\n",
    "        dropout_rate=0.2\n",
    "    )\n",
    "    y_pred = moe_layer((latent_vector, soft_cluster_probs))\n",
    "    y_pred = layers.Activation('sigmoid', name='cls_ypred')(y_pred)\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # MODEL DEFINITION\n",
    "    # -------------------------------------------------------------------\n",
    "    model = keras.Model(\n",
    "        inputs=[pep_OHE_in, pep_mask_in, mhc_emb_in, mhc_mask_in, mhc_OHE_in],\n",
    "        outputs={\n",
    "            \"pep_ytrue_ypred\": pep_out,\n",
    "            \"mhc_ytrue_ypred\": mhc_out,\n",
    "            \"cls_ypred\": y_pred,\n",
    "        },\n",
    "        name=\"pmbind_subtract_moe_autoencoder\"\n",
    "    )\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T11:26:19.666583Z",
     "start_time": "2025-08-19T11:26:17.840819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# 3. DUMMY DATA GENERATION (for a runnable example)\n",
    "# ==============================================================================\n",
    "num_samples = 1000\n",
    "pep_len = 14\n",
    "mhc_len = 312\n",
    "pep_alphabet_size = 21\n",
    "mhc_embedding_dim = 1152\n",
    "\n",
    "# Peptide One-Hot Encoding\n",
    "pep_indices = np.random.randint(0, pep_alphabet_size, size=(num_samples, pep_len))\n",
    "pep_OHE = tf.one_hot(pep_indices, depth=pep_alphabet_size, dtype=tf.float32)\n",
    "\n",
    "# MHC Embedding\n",
    "mhc_emb = tf.random.normal(shape=(num_samples, mhc_len, mhc_embedding_dim))\n",
    "mhc_indices = np.random.randint(0, pep_alphabet_size, size=(num_samples, mhc_len))\n",
    "mhc_OHE = tf.one_hot(mhc_indices, depth=pep_alphabet_size, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# Peptide mask\n",
    "# Define pad and mask tokens\n",
    "pad_token = -2.0\n",
    "mask_token = -1.0\n",
    "\n",
    "# Generate peptide mask\n",
    "# Let's assume a variable number of padded/masked tokens per sample\n",
    "pep_mask = np.ones((num_samples, pep_len), dtype=np.float32)\n",
    "for i in range(num_samples):\n",
    "    # Add 0 to 3 padded tokens at the end\n",
    "    num_padded = np.random.randint(0, 4)\n",
    "    if num_padded > 0:\n",
    "        pep_mask[i, -num_padded:] = pad_token\n",
    "    # Add 0 to 2 masked tokens at random positions that are not padded\n",
    "    num_masked = np.random.randint(0, 3)\n",
    "    valid_indices = np.where(pep_mask[i] == 1.0)[0]\n",
    "    if num_masked > 0 and len(valid_indices) >= num_masked:\n",
    "        mask_indices = np.random.choice(valid_indices, num_masked, replace=False)\n",
    "        pep_mask[i, mask_indices] = mask_token\n",
    "\n",
    "pep_mask = tf.constant(pep_mask, dtype=tf.float32)\n",
    "\n",
    "# Generate MHC mask (similar logic)\n",
    "mhc_mask = np.ones((num_samples, mhc_len), dtype=np.float32)\n",
    "for i in range(num_samples):\n",
    "    # Add 0 to 10 padded tokens\n",
    "    num_padded = np.random.randint(0, 11)\n",
    "    if num_padded > 0:\n",
    "        mhc_mask[i, -num_padded:] = pad_token\n",
    "mhc_mask = tf.constant(mhc_mask, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# Zero out masked positions in the data\n",
    "# Create boolean masks where True means the position is valid (not padded or masked)\n",
    "pep_bool_mask = tf.cast(pep_mask > 0, dtype=tf.float32)\n",
    "mhc_bool_mask = tf.cast(mhc_mask > 0, dtype=tf.float32)\n",
    "\n",
    "# Apply masks to zero out data. Unsqueeze to allow broadcasting.\n",
    "pep_OHE = pep_OHE * pep_bool_mask[:, :, tf.newaxis]\n",
    "mhc_emb = mhc_emb * mhc_bool_mask[:, :, tf.newaxis]\n",
    "\n",
    "# Classification labels\n",
    "y_true = tf.cast(tf.random.uniform((num_samples, 1)) > 0.5, tf.float32)\n",
    "\n",
    "# Group inputs and outputs into dictionaries for tf.data.Dataset\n",
    "inputs = {\n",
    "    \"pep_onehot\": pep_OHE,\n",
    "    \"pep_mask\": pep_mask,\n",
    "    \"mhc_emb\": mhc_emb,\n",
    "    \"mhc_mask\": mhc_mask,\n",
    "    \"mhc_onehot\": mhc_OHE\n",
    "}\n",
    "targets = {\n",
    "    \"pep_ytrue_ypred\": pep_OHE,\n",
    "    \"mhc_ytrue_ypred\": mhc_OHE,\n",
    "    \"cls_ypred\": y_true\n",
    "}"
   ],
   "id": "fca0b2997a2dd315",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T11:30:14.257413Z",
     "start_time": "2025-08-19T11:27:09.924474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 4. CUSTOM TRAINING LOOP SETUP\n",
    "# ==============================================================================\n",
    "# --- Hyperparameters and Setup ---\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "emb_dim = 96\n",
    "heads = 4\n",
    "noise_std = 0.1\n",
    "num_experts = 8\n",
    "\n",
    "# --- Instantiate Model, Optimizer, and Loss ---\n",
    "model = pmbind_subtract_moe_auto(\n",
    "    max_pep_len=pep_len,\n",
    "    max_mhc_len=mhc_len,\n",
    "    emb_dim=emb_dim,\n",
    "    heads=heads,\n",
    "    noise_std=noise_std,\n",
    "    num_experts=num_experts,\n",
    "    mask_token=MASK_TOKEN,\n",
    "    pad_token=PAD_TOKEN\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "binary_loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# compile the model\n",
    "# model.compile(\n",
    "#     optimizer=optimizer,\n",
    "#     loss={\n",
    "#         \"pep_ytrue_ypred\": masked_categorical_crossentropy,\n",
    "#         \"mhc_ytrue_ypred\": masked_categorical_crossentropy,\n",
    "#         \"cls_ypred\": binary_loss_fn\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# --- Metrics for Tracking ---\n",
    "metrics_names = ['loss', 'pep_recon_loss', 'mhc_recon_loss', 'class_loss', 'auc']\n",
    "train_metrics = {name: tf.keras.metrics.Mean(name=f\"train_{name}\") for name in metrics_names}\n",
    "val_metrics = {name: tf.keras.metrics.Mean(name=f\"val_{name}\") for name in metrics_names}\n",
    "train_metrics['auc'] = tf.keras.metrics.AUC(name='train_auc')\n",
    "val_metrics['auc'] = tf.keras.metrics.AUC(name='val_auc')\n",
    "\n",
    "# --- Prepare tf.data.Dataset for efficient training ---\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(num_samples)\n",
    "val_size = int(0.2 * num_samples)\n",
    "train_dataset = dataset.skip(val_size).batch(batch_size, drop_remainder=True)\n",
    "val_dataset = dataset.take(val_size).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. TRAIN AND VALIDATION STEPS (using tf.function for performance)\n",
    "# ==============================================================================\n",
    "@tf.function\n",
    "def train_step(x_batch, y_batch):\n",
    "    x_batch_list = [x_batch['pep_onehot'], x_batch['pep_mask'], x_batch['mhc_emb'], x_batch['mhc_mask'], x_batch['mhc_onehot']]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x_batch_list, training=True)\n",
    "        # Calculate individual losses\n",
    "        pep_loss = masked_categorical_crossentropy(predictions['pep_ytrue_ypred'], x_batch['pep_mask'], PAD_TOKEN)\n",
    "        mhc_loss = masked_categorical_crossentropy(predictions['mhc_ytrue_ypred'], x_batch['mhc_mask'], PAD_TOKEN)\n",
    "        class_loss = binary_loss_fn(y_batch['cls_ypred'], predictions['cls_ypred'])\n",
    "\n",
    "        # Combine losses (you can apply weights here, e.g., total_loss = 0.5*pep_loss + ...)\n",
    "        total_loss = pep_loss + mhc_loss + class_loss\n",
    "\n",
    "    # Apply gradients\n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Update metrics\n",
    "    train_metrics['loss'](total_loss)\n",
    "    train_metrics['pep_recon_loss'](pep_loss)\n",
    "    train_metrics['mhc_recon_loss'](mhc_loss)\n",
    "    train_metrics['class_loss'](class_loss)\n",
    "    train_metrics['auc'](y_batch['cls_ypred'], predictions['cls_ypred'])\n",
    "\n",
    "@tf.function\n",
    "def val_step(x_batch, y_batch):\n",
    "    x_batch_list = [x_batch['pep_onehot'], x_batch['pep_mask'], x_batch['mhc_emb'], x_batch['mhc_mask'], x_batch['mhc_onehot']]\n",
    "\n",
    "    predictions = model(x_batch_list, training=False)\n",
    "    # Calculate losses\n",
    "    pep_loss = masked_categorical_crossentropy(predictions['pep_ytrue_ypred'], x_batch['pep_mask'], PAD_TOKEN)\n",
    "    mhc_loss = masked_categorical_crossentropy(predictions['mhc_ytrue_ypred'], x_batch['mhc_mask'], PAD_TOKEN)\n",
    "    class_loss = binary_loss_fn(y_batch['cls_ypred'], predictions['cls_ypred'])\n",
    "    total_loss = pep_loss + mhc_loss + class_loss\n",
    "\n",
    "    # Update metrics\n",
    "    val_metrics['loss'](total_loss)\n",
    "    val_metrics['pep_recon_loss'](pep_loss)\n",
    "    val_metrics['mhc_recon_loss'](mhc_loss)\n",
    "    val_metrics['class_loss'](class_loss)\n",
    "    val_metrics['auc'](y_batch['cls_ypred'], predictions['cls_ypred'])\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. THE MAIN TRAINING LOOP\n",
    "# ==============================================================================\n",
    "history = {f\"{key}\": [] for key in train_metrics.keys()}\n",
    "history.update({f\"val_{key}\": [] for key in val_metrics.keys()})\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(epochs):\n",
    "    # Reset metrics at the start of each epoch\n",
    "    for metric in train_metrics.values(): metric.reset_state()\n",
    "    for metric in val_metrics.values(): metric.reset_state()\n",
    "\n",
    "    # Training loop\n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        train_step(x_batch, y_batch)\n",
    "\n",
    "    # Validation loop\n",
    "    for x_val_batch, y_val_batch in val_dataset:\n",
    "        val_step(x_val_batch, y_val_batch)\n",
    "\n",
    "    # Log results\n",
    "    train_results = {key: value.result().numpy() for key, value in train_metrics.items()}\n",
    "    val_results = {key: value.result().numpy() for key, value in val_metrics.items()}\n",
    "\n",
    "    # Store history\n",
    "    for key, value in train_results.items(): history[key].append(value)\n",
    "    for key, value in val_results.items(): history[f\"val_{key}\"].append(value)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "          f\"Loss: {train_results['loss']:.4f} - \"\n",
    "          f\"AUC: {train_results['auc']:.4f} - \"\n",
    "          f\"Val Loss: {val_results['loss']:.4f} - \"\n",
    "          f\"Val AUC: {val_results['auc']:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. VISUALIZATION\n",
    "# ==============================================================================\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Plot total loss\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.plot(history['loss'], label='Training Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.title('Total Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot peptide reconstruction loss\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.plot(history['pep_recon_loss'], label='Peptide Recon Loss')\n",
    "plt.plot(history['val_pep_recon_loss'], label='Val Peptide Recon Loss')\n",
    "plt.title('Peptide Reconstruction Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot MHC reconstruction loss\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.plot(history['mhc_recon_loss'], label='MHC Recon Loss')\n",
    "plt.plot(history['val_mhc_recon_loss'], label='Val MHC Recon Loss')\n",
    "plt.title('MHC Reconstruction Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot AUC\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.plot(history['auc'], label='Training AUC')\n",
    "plt.plot(history['val_auc'], label='Validation AUC')\n",
    "plt.title('Classification AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "89e8bbba42f1005c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 13:28:14.233361: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-08-19 13:28:19.190200: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 6.5618 - AUC: 0.5175 - Val Loss: 6.2262 - Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 13:29:21.681634: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Loss: 5.3287 - AUC: 0.5093 - Val Loss: 4.1891 - Val AUC: 0.5000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[109], line 110\u001B[0m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;66;03m# Training loop\u001B[39;00m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m x_batch, y_batch \u001B[38;5;129;01min\u001B[39;00m train_dataset:\n\u001B[0;32m--> 110\u001B[0m     \u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;66;03m# Validation loop\u001B[39;00m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m x_val_batch, y_val_batch \u001B[38;5;129;01min\u001B[39;00m val_dataset:\n",
      "File \u001B[0;32m~/miniforge3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/miniforge3/envs/py310/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    830\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    832\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 833\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    835\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    836\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m~/miniforge3/envs/py310/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:869\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    866\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    867\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[1;32m    868\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[0;32m--> 869\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtracing_compilation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    870\u001B[0m \u001B[43m      \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_no_variable_creation_config\u001B[49m\n\u001B[1;32m    871\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    872\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_config \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    873\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[1;32m    874\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[1;32m    875\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[0;32m~/miniforge3/envs/py310/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001B[0m, in \u001B[0;36mcall_function\u001B[0;34m(args, kwargs, tracing_options)\u001B[0m\n\u001B[1;32m    137\u001B[0m bound_args \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mbind(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    138\u001B[0m flat_inputs \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39munpack_inputs(bound_args)\n\u001B[0;32m--> 139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# pylint: disable=protected-access\u001B[39;49;00m\n\u001B[1;32m    140\u001B[0m \u001B[43m    \u001B[49m\u001B[43mflat_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\n\u001B[1;32m    141\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/py310/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[0;34m(self, tensor_inputs, captured_inputs)\u001B[0m\n\u001B[1;32m   1318\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[1;32m   1319\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[1;32m   1320\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[1;32m   1321\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[0;32m-> 1322\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_preflattened\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1323\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[1;32m   1324\u001B[0m     args,\n\u001B[1;32m   1325\u001B[0m     possible_gradient_type,\n\u001B[1;32m   1326\u001B[0m     executing_eagerly)\n\u001B[1;32m   1327\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[0;32m~/miniforge3/envs/py310/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001B[0m, in \u001B[0;36mAtomicFunction.call_preflattened\u001B[0;34m(self, args)\u001B[0m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcall_preflattened\u001B[39m(\u001B[38;5;28mself\u001B[39m, args: Sequence[core\u001B[38;5;241m.\u001B[39mTensor]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    215\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 216\u001B[0m   flat_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    217\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mpack_output(flat_outputs)\n",
      "File \u001B[0;32m~/miniforge3/envs/py310/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001B[0m, in \u001B[0;36mAtomicFunction.call_flat\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m record\u001B[38;5;241m.\u001B[39mstop_recording():\n\u001B[1;32m    250\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mexecuting_eagerly():\n\u001B[0;32m--> 251\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_bound_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    252\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction_type\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflat_outputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    256\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    257\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m make_call_op_in_graph(\n\u001B[1;32m    258\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    259\u001B[0m         \u001B[38;5;28mlist\u001B[39m(args),\n\u001B[1;32m    260\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mfunction_call_options\u001B[38;5;241m.\u001B[39mas_attrs(),\n\u001B[1;32m    261\u001B[0m     )\n",
      "File \u001B[0;32m~/miniforge3/envs/py310/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1683\u001B[0m, in \u001B[0;36mContext.call_function\u001B[0;34m(self, name, tensor_inputs, num_outputs)\u001B[0m\n\u001B[1;32m   1681\u001B[0m cancellation_context \u001B[38;5;241m=\u001B[39m cancellation\u001B[38;5;241m.\u001B[39mcontext()\n\u001B[1;32m   1682\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cancellation_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1683\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1684\u001B[0m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1685\u001B[0m \u001B[43m      \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1686\u001B[0m \u001B[43m      \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1687\u001B[0m \u001B[43m      \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1688\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1689\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1690\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1691\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[1;32m   1692\u001B[0m       name\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m   1693\u001B[0m       num_outputs\u001B[38;5;241m=\u001B[39mnum_outputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1697\u001B[0m       cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_context,\n\u001B[1;32m   1698\u001B[0m   )\n",
      "File \u001B[0;32m~/miniforge3/envs/py310/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     52\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 53\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     56\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c1648bf04b0ea49b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fd327a4fb8b30525"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
